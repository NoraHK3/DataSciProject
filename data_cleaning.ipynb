{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoraHK3/DataSciProject/blob/main/data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **1- CSV Translation**\n",
        "\n",
        "The original dataset contained a mix of **Arabic and English** text â€” some dish names and ingredients were already in English, while others were still in Arabic.\n",
        "To make the data consistent and ready for analysis, this step translates **all Arabic parts** into **English**, while keeping the existing English text as it is.\n",
        "\n",
        "It uses **Google Translate** through the `deep-translator` library, with extra tools to keep translations accurate:\n",
        "\n",
        "* **Overrides (`overrides_expanded.json`)** â€“ custom manual translations that replace Googleâ€™s output for specific words (e.g., â€œÙ„ÙŠÙ…ÙˆÙ† Ø£Ø³ÙˆØ¯â€ â†’ â€œblack limeâ€).\n",
        "* **Cache (`translation_cache.csv`)** â€“ stores previous translations to keep results consistent and faster.\n",
        "* **Post-fixes** â€“ fixes common translation mistakes automatically (e.g., â€œblack lemonâ€ â†’ â€œblack limeâ€).\n",
        "* **Classification handling** â€“ splits multi-part text like â€œØ±Ø² | Ø¯Ø¬Ø§Ø¬â€ and translates each piece separately (â€œrice | chickenâ€).\n",
        "\n",
        "**Why we did this:**\n",
        "Mixing two languages made the data inconsistent. Translating everything to English first ensures that the later cleaning and ingredient normalization steps work correctly and uniformly.\n",
        "\n",
        "**Output:**\n",
        " `SaudiFoodFile_english_FIXED.csv` â€” fully English, consistent version of the dataset\n"
      ],
      "metadata": {
        "id": "noVdL6e1tkOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OVERRIDES_JSON = \"overrides_expanded.json\""
      ],
      "metadata": {
        "id": "pBK46XVf0WB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Translator (Deep) with Expanded Overrides + Cache Purge + Debug\n",
        "# ============================================\n",
        "\n",
        "!pip install -q pandas deep-translator\n",
        "\n",
        "import os, re, json, pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "INPUT_CSV  = \"SaudiFoodFile.csv\"\n",
        "OUTPUT_CSV = \"SaudiFoodFile_english_FIXED.csv\"\n",
        "CACHE_CSV  = \"translation_cache.csv\"\n",
        "\n",
        "# Prefer expanded overrides if present\n",
        "OVR_EXP   = \"overrides_expanded.json\"\n",
        "OVR_BASE  = \"overrides.json\"\n",
        "OVERRIDES_JSON = OVR_EXP if os.path.exists(OVR_EXP) else OVR_BASE\n",
        "\n",
        "HANDLE_CLASSIFICATIONS = True  # split 'classifications' by '|'\n",
        "TRANSLATE_COLS = None          # None -> all object columns\n",
        "\n",
        "# ---------- helpers ----------\n",
        "AR_DIAC = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u06D6-\\u06ED]\")\n",
        "def norm_ar(s: str) -> str:\n",
        "    s = AR_DIAC.sub(\"\", s)\n",
        "    s = s.replace(\"\\u0640\",\"\")\n",
        "    s = s.replace(\"Ø£\",\"Ø§\").replace(\"Ø¥\",\"Ø§\").replace(\"Ø¢\",\"Ø§\")\n",
        "    s = s.replace(\"Ù‰\",\"ÙŠ\").replace(\"Ø¦\",\"ÙŠ\").replace(\"Ø¤\",\"Ùˆ\").replace(\"Ù±\",\"Ø§\")\n",
        "    return s\n",
        "\n",
        "def key_norm(x: str) -> str:\n",
        "    return norm_ar(str(x).strip().lower())\n",
        "\n",
        "POST_FIX = {\n",
        "    \"black lemon\": \"black lime\",\n",
        "    \"nail\": \"cloves\",\n",
        "    \"cardamon\": \"cardamom\",\n",
        "    \"yougurt\": \"yogurt\",\n",
        "    \"youghurt\": \"yogurt\",\n",
        "}\n",
        "\n",
        "def apply_postfix(en: str) -> str:\n",
        "    return POST_FIX.get(str(en).strip().lower(), str(en).strip())\n",
        "\n",
        "# ---------- load data ----------\n",
        "# CSV\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
        "except UnicodeDecodeError:\n",
        "    df = pd.read_csv(INPUT_CSV, encoding=\"cp1256\")\n",
        "\n",
        "# Overrides\n",
        "if os.path.exists(OVERRIDES_JSON):\n",
        "    with open(OVERRIDES_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "        OV = json.load(f)\n",
        "else:\n",
        "    OV = {}\n",
        "\n",
        "# Normalized override view (for Arabic variant matching)\n",
        "OV_NORM = {key_norm(k): v for k, v in OV.items() if re.search(r\"[\\u0600-\\u06FF]\", k)}\n",
        "print(f\"ðŸ”§ Using overrides file: {OVERRIDES_JSON}\")\n",
        "print(f\"   Loaded overrides: {len(OV)} (normalized Arabic keys: {len(OV_NORM)})\")\n",
        "# show a few samples for sanity\n",
        "for i,(k,v) in enumerate(list(OV.items())[:8]):\n",
        "    print(f\"   â€¢ {k}  ->  {v}\")\n",
        "    if i>=7: break\n",
        "\n",
        "# Cache (load then purge entries that now have overrides)\n",
        "if os.path.exists(CACHE_CSV):\n",
        "    cache_df = pd.read_csv(CACHE_CSV)\n",
        "    CACHE = dict(cache_df.values)  # {raw: english}\n",
        "else:\n",
        "    CACHE = {}\n",
        "\n",
        "def override_lookup(text: str):\n",
        "    if text in OV:\n",
        "        return OV[text]\n",
        "    kn = key_norm(text)\n",
        "    if kn in OV_NORM:\n",
        "        return OV_NORM[kn]\n",
        "    return None\n",
        "\n",
        "# Purge cache entries that should now be overridden\n",
        "purged = 0\n",
        "to_del = []\n",
        "for raw in list(CACHE.keys()):\n",
        "    if override_lookup(raw):\n",
        "        to_del.append(raw)\n",
        "for raw in to_del:\n",
        "    CACHE.pop(raw, None)\n",
        "    purged += 1\n",
        "print(f\"ðŸ§¹ Purged {purged} cache entries that now have overrides\")\n",
        "\n",
        "translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
        "\n",
        "def translate_text(text: str) -> str:\n",
        "    if pd.isna(text) or str(text).strip() == \"\":\n",
        "        return text\n",
        "    s = str(text).strip()\n",
        "\n",
        "    # 1) override wins (exact or normalized)\n",
        "    ov = override_lookup(s)\n",
        "    if ov:\n",
        "        return ov\n",
        "\n",
        "    # 2) cache\n",
        "    if s in CACHE:\n",
        "        return CACHE[s]\n",
        "\n",
        "    # 3) machine translation\n",
        "    try:\n",
        "        en = translator.translate(s) or s\n",
        "        en = apply_postfix(en)\n",
        "    except Exception:\n",
        "        en = s  # keep original on error\n",
        "\n",
        "    CACHE[s] = en\n",
        "    return en\n",
        "\n",
        "def translate_classifications_cell(cell: str) -> str:\n",
        "    parts = [p.strip() for p in str(cell).split(\"|\")]\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        if not p:\n",
        "            continue\n",
        "        ov = override_lookup(p)\n",
        "        en = ov if ov else translate_text(p)\n",
        "        out.append(str(en).lower())\n",
        "    return \" | \".join(out)\n",
        "\n",
        "# ---------- choose columns ----------\n",
        "obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
        "cols = obj_cols if TRANSLATE_COLS is None else [c for c in TRANSLATE_COLS if c in df.columns]\n",
        "print(f\"ðŸ“ Translating columns: {cols}\")\n",
        "\n",
        "# ---------- translate ----------\n",
        "for col in cols:\n",
        "    print(f\"âž¡ï¸  Translating: {col}\")\n",
        "    if HANDLE_CLASSIFICATIONS and col.lower() == \"classifications\":\n",
        "        df[col] = df[col].astype(str).apply(translate_classifications_cell)\n",
        "    else:\n",
        "        df[col] = df[col].apply(translate_text)\n",
        "\n",
        "# ---------- save ----------\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "pd.DataFrame(list(CACHE.items()), columns=[\"raw\",\"english\"]).to_csv(CACHE_CSV, index=False)\n",
        "\n",
        "print(f\"âœ… Done: {OUTPUT_CSV}\")\n",
        "print(f\"ðŸ’¾ Cache: {CACHE_CSV}\")\n",
        "print(f\"âœï¸ Overrides file in use: {OVERRIDES_JSON}\")"
      ],
      "metadata": {
        "id": "eWhNseTgpFHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2- Data Cleaning**"
      ],
      "metadata": {
        "id": "9X_ZnNB8yqMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning - Changing dish name**\n",
        "\n",
        "**clean dish name, removing irrelavent extra discription**\n",
        "\n",
        "This code cleans and standardizes dish names in the Saudi food dataset.\n",
        " 1. It removes unnecessary words and descriptions (like \"for Saudi National Day\",\n",
        "   \"how to make\", \"traditional\", etc.) from the dish names.\n",
        "2. It then standardizes different spellings or variations of the same dish\n",
        "  (e.g., \"kabsah\", \"kbsa\" â†’ \"Kabsa\", \"shaksoka\" â†’ \"Shakshuka\").\n",
        "  \n",
        " 3. Finally, it shows before/after examples, reports the most common dish names,\n",
        " and saves the cleaned dataset as 'SaudiFoodFile_cleaned.csv' for later use."
      ],
      "metadata": {
        "id": "bOCGSgSOObVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('SaudiFoodFile_english_FIXED.csv')\n",
        "\n",
        "# Display initial data info\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Task 1: Clean dish names - remove extra descriptions\n",
        "def clean_dish_name(name):\n",
        "    \"\"\"\n",
        "    Remove extra descriptions from dish names like 'for Saudi National Day',\n",
        "    'how to make', 'Saudi style', etc.\n",
        "    \"\"\"\n",
        "    # Common patterns to remove\n",
        "    patterns_to_remove = [\n",
        "        r'for saudi national day',\n",
        "        r'how to make',\n",
        "        r'saudi style',\n",
        "        r'saudi',\n",
        "        r'traditional',\n",
        "        r'the saudi',\n",
        "        r'method for',\n",
        "        r'according to',\n",
        "        r'with.*',\n",
        "        r'for.*',\n",
        "        r'the hijazi way',\n",
        "        r'hijazi',\n",
        "        r'recipe',\n",
        "        r'easy',\n",
        "        r'authentic',\n",
        "        r'copycat',\n",
        "        r'slow-?roast',\n",
        "        r'no bake',\n",
        "        r'healthy',\n",
        "        r'vegetarian',\n",
        "        r'stuffed',\n",
        "        r'baked',\n",
        "        r'grilled',\n",
        "        r'roasted',\n",
        "        r'creamy',\n",
        "        r'spiced',\n",
        "        r'middle eastern'\n",
        "    ]\n",
        "\n",
        "    cleaned_name = name.lower().strip()\n",
        "\n",
        "    # Remove patterns\n",
        "    for pattern in patterns_to_remove:\n",
        "        cleaned_name = re.sub(pattern, '', cleaned_name, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove extra spaces and punctuation\n",
        "    cleaned_name = re.sub(r'[^\\w\\s]', ' ', cleaned_name)  # Remove punctuation\n",
        "    cleaned_name = re.sub(r'\\s+', ' ', cleaned_name)  # Remove extra spaces\n",
        "    cleaned_name = cleaned_name.strip()\n",
        "\n",
        "    # Remove common measurement/portion descriptions\n",
        "    portion_patterns = [\n",
        "        r'\\([^)]*\\)',  # Remove anything in parentheses\n",
        "        r'\\bwhole grain\\b',\n",
        "        r'\\bhalf a piece\\b',\n",
        "        r'\\bhalf piece\\b',\n",
        "        r'\\bquarter\\b',\n",
        "        r'\\bone person\\b',\n",
        "        r'\\bperson\\b',\n",
        "        r'\\bplain\\b',\n",
        "        r'\\bwith rice\\b',\n",
        "        r'\\bwithout rice\\b'\n",
        "    ]\n",
        "\n",
        "    for pattern in portion_patterns:\n",
        "        cleaned_name = re.sub(pattern, '', cleaned_name, flags=re.IGNORECASE)\n",
        "\n",
        "    # Final cleanup\n",
        "    cleaned_name = re.sub(r'\\s+', ' ', cleaned_name).strip()\n",
        "\n",
        "    # Title case for consistency\n",
        "    cleaned_name = cleaned_name.title()\n",
        "\n",
        "    return cleaned_name\n",
        "\n",
        "# Task 2: Standardize dish name variations\n",
        "def standardize_dish_name(name):\n",
        "    \"\"\"\n",
        "    Standardize variations of dish names (kabsa/kabsah/kbsa -> kabsa)\n",
        "    \"\"\"\n",
        "    standardization_map = {\n",
        "        r'\\bkabsah?\\b': 'Kabsa',\n",
        "        r'\\bkbsa\\b': 'Kabsa',\n",
        "        r'\\bkleija\\b': 'Kleja',\n",
        "        r'\\bkulaija\\b': 'Kleja',\n",
        "        r'\\bklija\\b': 'Kleja',\n",
        "        r'\\bshaksoka\\b': 'Shakshuka',\n",
        "        r'\\bshakshuka\\b': 'Shakshuka',\n",
        "        r'\\bshaksuka\\b': 'Shakshuka',\n",
        "        r'\\bbasbousa\\b': 'Basbousa',\n",
        "        r'\\bbasbosa\\b': 'Basbousa',\n",
        "        r'\\bjareesh\\b': 'Jareesh',\n",
        "        r'\\bjarish\\b': 'Jareesh',\n",
        "        r'\\bgreesh\\b': 'Jareesh',\n",
        "        r'\\bgroats\\b': 'Jareesh',\n",
        "        r'\\bmaqshoosh\\b': 'Maqshush',\n",
        "        r'\\bmaqshush\\b': 'Maqshush',\n",
        "        r'\\bmutabbaq\\b': 'Mutabak',\n",
        "        r'\\bmutabak\\b': 'Mutabak',\n",
        "        r'\\bsaleeq\\b': 'Saleek',\n",
        "        r'\\bsaliq\\b': 'Saleek',\n",
        "        r'\\bsaleek\\b': 'Saleek',\n",
        "        r'\\bsulait?\\b': 'Saleek',\n",
        "        r'\\bmaamoul\\b': 'Mamoul',\n",
        "        r'\\bmamoul\\b': 'Mamoul',\n",
        "        r'\\bmadhbi\\b': 'Madhbi',\n",
        "        r'\\bmadghog\\b': 'Madhghut',\n",
        "        r'\\bmadjou?h\\b': 'Madhghut',\n",
        "        r'\\bmadfoon\\b': 'Madfun',\n",
        "        r'\\bmadfoun\\b': 'Madfun',\n",
        "        r'\\bmandi\\b': 'Mandi',\n",
        "        r'\\bzurbian\\b': 'Zurbian',\n",
        "        r'\\bzerbian\\b': 'Zurbian',\n",
        "        r'\\bshrimp\\b': 'Shrimp',\n",
        "        r'\\bshurbian\\b': 'Shrimp',\n",
        "        r'\\bsambosa\\b': 'Sambusa',\n",
        "        r'\\bsambousek\\b': 'Sambusa',\n",
        "        r'\\bsamosa\\b': 'Sambusa',\n",
        "        r'\\bmagloba\\b': 'Maqluba',\n",
        "        r'\\bmaqluba\\b': 'Maqluba',\n",
        "        r'\\bmakloubeh\\b': 'Maqluba',\n",
        "        r'\\bmoussaka\\b': 'Musaqa',\n",
        "        r'\\bmoussaqa\\b': 'Musaqa',\n",
        "        r'\\bmusakaa\\b': 'Musaqa',\n",
        "        r'\\bmolokhia\\b': 'Mulukhiyah',\n",
        "        r'\\bmolokhiya\\b': 'Mulukhiyah',\n",
        "        r'\\bmulukhiyah\\b': 'Mulukhiyah',\n",
        "        r'\\bmargog\\b': 'Marqouq',\n",
        "        r'\\bmarqouk\\b': 'Marqouq',\n",
        "        r'\\bmarqooq\\b': 'Marqouq',\n",
        "        r'\\bmatazeez\\b': 'Mataziz',\n",
        "        r'\\bmogalgal\\b': 'Muqalqal',\n",
        "        r'\\bmqalqal\\b': 'Muqalqal',\n",
        "        r'\\bhemees\\b': 'Hamees',\n",
        "        r'\\bhemen\\b': 'Hamees',\n",
        "        r'\\bmohalabiya\\b': 'Muhalabiya',\n",
        "        r'\\bmohala\\b': 'Muhalabiya',\n",
        "        r'\\bkunafa\\b': 'Kunafa',\n",
        "        r'\\bknafeh\\b': 'Kunafa',\n",
        "        r'\\bsabeeb\\b': 'Sabeeb',\n",
        "        r'\\bsabib\\b': 'Sabeeb',\n",
        "        r'\\btaheena\\b': 'Tahini',\n",
        "        r'\\btainna\\b': 'Tahini',\n",
        "        r'\\btahini\\b': 'Tahini',\n",
        "        r'\\bfatteh\\b': 'Fatteh',\n",
        "        r'\\bfateh\\b': 'Fatteh',\n",
        "        r'\\bfreekeh\\b': 'Freekeh',\n",
        "        r'\\bfreekey\\b': 'Freekeh',\n",
        "        r'\\bhashweh\\b': 'Hashu',\n",
        "        r'\\bhashu\\b': 'Hashu',\n",
        "        r'\\bmujadara\\b': 'Mujaddara',\n",
        "        r'\\bmujaddara\\b': 'Mujaddara',\n",
        "        r'\\bzaatar\\b': 'Zaatar',\n",
        "        r'\\bza\\'atar\\b': 'Zaatar'\n",
        "    }\n",
        "\n",
        "    standardized_name = name\n",
        "    for pattern, replacement in standardization_map.items():\n",
        "        standardized_name = re.sub(pattern, replacement, standardized_name, flags=re.IGNORECASE)\n",
        "\n",
        "    return standardized_name\n",
        "\n",
        "# Apply cleaning and standardization\n",
        "print(\"\\nApplying data cleaning...\")\n",
        "\n",
        "# Create cleaned dish names\n",
        "df['cleaned_dish_name'] = df['dish_name'].apply(clean_dish_name)\n",
        "df['standardized_dish_name'] = df['cleaned_dish_name'].apply(standardize_dish_name)\n",
        "\n",
        "# Show before and after examples\n",
        "print(\"\\nName cleaning examples:\")\n",
        "sample_size = min(10, len(df))\n",
        "for i in range(sample_size):\n",
        "    print(f\"Original: {df['dish_name'].iloc[i]}\")\n",
        "    print(f\"Cleaned: {df['cleaned_dish_name'].iloc[i]}\")\n",
        "    print(f\"Standardized: {df['standardized_dish_name'].iloc[i]}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Show most common dish names after standardization\n",
        "print(\"\\nMost common standardized dish names:\")\n",
        "print(df['standardized_dish_name'].value_counts().head(20))\n",
        "\n",
        "# Check for remaining variations\n",
        "print(\"\\nChecking for remaining variations (sample):\")\n",
        "unique_names = df['standardized_dish_name'].unique()\n",
        "for name in sorted(unique_names)[:30]:  # Show first 30\n",
        "    print(f\"  - {name}\")\n",
        "\n",
        "# Save the cleaned data\n",
        "df_cleaned = df.copy()\n",
        "# You can choose to replace the original dish_name or keep both\n",
        "df_cleaned['dish_name_original'] = df['dish_name']\n",
        "df_cleaned['dish_name'] = df['standardized_dish_name']\n",
        "\n",
        "# Drop temporary columns\n",
        "df_cleaned = df_cleaned.drop(['cleaned_dish_name', 'standardized_dish_name'], axis=1)\n",
        "\n",
        "print(f\"\\nFinal data shape: {df_cleaned.shape}\")\n",
        "print(\"\\nFirst few rows of cleaned data:\")\n",
        "print(df_cleaned[['dish_name_original', 'dish_name']].head(15))\n",
        "\n",
        "# Save to new CSV file\n",
        "output_filename = 'SaudiFoodFile_cleaned.csv'\n",
        "df_cleaned.to_csv(output_filename, index=False)\n",
        "print(f\"\\nCleaned data saved to: {output_filename}\")\n",
        "\n",
        "# Additional analysis: Show name standardization results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"NAME STANDARDIZATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Group similar names to show standardization effect\n",
        "name_groups = {}\n",
        "for orig, new in zip(df['dish_name'], df_cleaned['dish_name']):\n",
        "    if new not in name_groups:\n",
        "        name_groups[new] = []\n",
        "    if orig not in name_groups[new]:\n",
        "        name_groups[new] = sorted(name_groups[new] + [orig])\n",
        "\n",
        "print(\"\\nStandardization groups (showing first 15 groups):\")\n",
        "count = 0\n",
        "for standardized_name, original_names in name_groups.items():\n",
        "    if len(original_names) > 1:  # Only show names that had variations\n",
        "        print(f\"\\n{standardized_name}:\")\n",
        "        for orig_name in original_names:\n",
        "            print(f\"  - {orig_name}\")\n",
        "        count += 1\n",
        "        if count >= 15:\n",
        "            break"
      ],
      "metadata": {
        "id": "DG15BuSYpJs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning - Changing image name**\n",
        "\n",
        "**Changing image name (make it like the dish name )**\n",
        "\n",
        "\n",
        "\n",
        " Purpose: Standardize image file names in the CSV based on dish names, ensure uniqueness,\n",
        "          and save the result for downstream use.\n",
        "\n",
        "What it does:\n",
        " 1) Loads 'SaudiFoodFile_cleaned.csv' and inspects dish_name quality (missing/non-string).\n",
        "2) Builds clean image file names from dish_name:\n",
        "    - lowercase, remove special chars, replace spaces/dashes with underscores,\n",
        "    - keep the original file extension (e.g., .jpg, .png),\n",
        "    - fallback to original image base name if dish_name is missing.\n",
        " 3) Ensures uniqueness by appending _2, _3, ... for duplicates.\n",
        " 4) Reports examples and a summary (duplicate groups, most common dish names, short names).\n",
        "5) Writes a new CSV 'SaudiFoodFile_final_cleaned.csv' with:\n",
        "   - image_file_original (old),\n",
        "   - image_file (new standardized).\n",
        " Note: This updates names in the CSV only. It does NOT rename files on disk."
      ],
      "metadata": {
        "id": "DXsPjLVdPI-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Load the cleaned data\n",
        "df = pd.read_csv('SaudiFoodFile_cleaned.csv')\n",
        "\n",
        "# Display initial data info\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df[['dish_name', 'image_file']].head())\n",
        "\n",
        "# Check for missing or non-string values in dish_name\n",
        "print(f\"\\nData types: {df['dish_name'].dtype}\")\n",
        "print(f\"Missing values in dish_name: {df['dish_name'].isna().sum()}\")\n",
        "print(f\"Non-string values sample: {df[df['dish_name'].apply(lambda x: not isinstance(x, str))].head()}\")\n",
        "\n",
        "# Function to create clean image filename from dish name\n",
        "def create_image_filename(dish_name, original_image_file):\n",
        "    \"\"\"\n",
        "    Create clean image filename based on dish name and handle duplicates\n",
        "    \"\"\"\n",
        "    # Handle NaN or non-string values\n",
        "    if not isinstance(dish_name, str) or pd.isna(dish_name):\n",
        "        # Use original image file name as fallback\n",
        "        base_name = os.path.splitext(os.path.basename(original_image_file))[0]\n",
        "        clean_name = base_name.lower()\n",
        "    else:\n",
        "        # Clean the dish name for filename\n",
        "        clean_name = dish_name.lower()\n",
        "\n",
        "    # Remove special characters and replace spaces with underscores\n",
        "    clean_name = re.sub(r'[^\\w\\s-]', '', clean_name)\n",
        "    clean_name = re.sub(r'[-\\s]+', '_', clean_name)\n",
        "\n",
        "    # Keep the file extension from original\n",
        "    file_extension = os.path.splitext(original_image_file)[1]\n",
        "\n",
        "    # Create base filename\n",
        "    base_filename = f\"{clean_name}{file_extension}\"\n",
        "\n",
        "    return base_filename\n",
        "\n",
        "# Apply image filename creation\n",
        "print(\"\\nCreating standardized image filenames...\")\n",
        "\n",
        "# Create base image filenames\n",
        "df['base_image_file'] = df.apply(\n",
        "    lambda row: create_image_filename(row['dish_name'], row['image_file']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Handle duplicates by adding incremental IDs\n",
        "print(\"\\nHandling duplicate image filenames...\")\n",
        "\n",
        "# Count occurrences and add IDs to duplicates\n",
        "duplicate_count = {}\n",
        "df['new_image_file'] = \"\"\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    base_name = row['base_image_file']\n",
        "\n",
        "    if base_name in duplicate_count:\n",
        "        duplicate_count[base_name] += 1\n",
        "        # Add ID to duplicate (before extension)\n",
        "        name_without_ext, ext = os.path.splitext(base_name)\n",
        "        final_name = f\"{name_without_ext}_{duplicate_count[base_name]}{ext}\"\n",
        "    else:\n",
        "        duplicate_count[base_name] = 1\n",
        "        final_name = base_name\n",
        "\n",
        "    df.at[idx, 'new_image_file'] = final_name\n",
        "\n",
        "# Show before and after examples\n",
        "print(\"\\nImage filename standardization examples:\")\n",
        "sample_size = min(20, len(df))\n",
        "for i in range(sample_size):\n",
        "    print(f\"Dish: {df['dish_name'].iloc[i]}\")\n",
        "    print(f\"Original image: {df['image_file'].iloc[i]}\")\n",
        "    print(f\"New image: {df['new_image_file'].iloc[i]}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Show duplicates that were handled\n",
        "duplicates = {name: count for name, count in duplicate_count.items() if count > 1}\n",
        "if duplicates:\n",
        "    print(f\"\\nFound {len(duplicates)} image names with duplicates:\")\n",
        "    for name, count in list(duplicates.items())[:15]:\n",
        "        print(f\"  - {name}: {count} occurrences\")\n",
        "\n",
        "    # Show specific examples of duplicate resolution\n",
        "    print(\"\\nExamples of duplicate resolution:\")\n",
        "    for duplicate_name in list(duplicates.keys())[:10]:\n",
        "        matching_rows = df[df['base_image_file'] == duplicate_name]\n",
        "        print(f\"\\n{duplicate_name}:\")\n",
        "        for _, row in matching_rows.iterrows():\n",
        "            print(f\"  - {row['new_image_file']} (from: {row['dish_name']})\")\n",
        "else:\n",
        "    print(\"\\nNo duplicate image names found!\")\n",
        "\n",
        "# Create the final dataframe\n",
        "df_final = df.copy()\n",
        "df_final['image_file_original'] = df['image_file']\n",
        "df_final['image_file'] = df['new_image_file']\n",
        "\n",
        "# Drop temporary columns\n",
        "df_final = df_final.drop(['base_image_file', 'new_image_file'], axis=1)\n",
        "\n",
        "print(f\"\\nFinal data shape: {df_final.shape}\")\n",
        "\n",
        "# Save to new CSV\n",
        "output_filename = 'SaudiFoodFile_final_cleaned.csv'\n",
        "df_final.to_csv(output_filename, index=False)\n",
        "print(f\"\\nFinal cleaned data saved to: {output_filename}\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IMAGE FILENAME STANDARDIZATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total dishes: {len(df_final)}\")\n",
        "print(f\"Unique original image names: {df['image_file'].nunique()}\")\n",
        "print(f\"Unique new image names: {df_final['image_file'].nunique()}\")\n",
        "print(f\"Duplicates handled: {len(duplicates)}\")\n",
        "\n",
        "# Show most common dish names and their image files\n",
        "print(\"\\nMost common dish names and their new image files:\")\n",
        "common_dishes = df_final['dish_name'].value_counts().head(15)\n",
        "for dish, count in common_dishes.items():\n",
        "    matching_images = df_final[df_final['dish_name'] == dish]['image_file'].tolist()\n",
        "    print(f\"\\n{dish} (appears {count} times):\")\n",
        "    for img in matching_images:\n",
        "        print(f\"  - {img}\")\n",
        "\n",
        "# Show problematic cases (very short names or empty names)\n",
        "print(\"\\nChecking for problematic dish names:\")\n",
        "short_names = df_final[df_final['dish_name'].str.len() < 3] if 'dish_name' in df_final.columns else pd.DataFrame()\n",
        "if len(short_names) > 0:\n",
        "    print(\"Very short dish names found:\")\n",
        "    for _, row in short_names.iterrows():\n",
        "        print(f\"  - '{row['dish_name']}' -> {row['image_file']}\")\n",
        "\n",
        "# Show the complete mapping for verification\n",
        "print(\"\\nComplete filename mapping (first 30 entries):\")\n",
        "print(\"Dish Name -> Original Image -> New Image\")\n",
        "for i in range(min(30, len(df_final))):\n",
        "    dish_name = df_final['dish_name'].iloc[i] if isinstance(df_final['dish_name'].iloc[i], str) else \"MISSING_NAME\"\n",
        "    print(f\"{dish_name} -> {df_final['image_file_original'].iloc[i]} -> {df_final['image_file'].iloc[i]}\")\n",
        "\n",
        "# Additional: Show any rows with missing dish names\n",
        "missing_dish_names = df_final[df_final['dish_name'].isna()]\n",
        "if len(missing_dish_names) > 0:\n",
        "    print(f\"\\nWARNING: Found {len(missing_dish_names)} rows with missing dish names:\")\n",
        "    for idx, row in missing_dish_names.iterrows():\n",
        "        print(f\"  - Row {idx}: Original image: {row['image_file_original']}, New image: {row['image_file']}\")"
      ],
      "metadata": {
        "id": "zlwloKdsp0Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning - Image File Renaming**\n",
        "\n",
        " **Image File Renaming (Done in a Separate Colab) with the name (renaming images file)**\n",
        "\n",
        " In this step, which was performed in a separate Colab notebook,\n",
        " we renamed all the image files on disk to match their corresponding\n",
        " standardized names in the CSV file\n",
        "  \n",
        "  \n",
        "  what this step do: Rename image files on disk to match the standardized image names\n",
        "          listed in the CSV file.\n",
        " What it does:\n",
        " 1) Reads the CSV (which contains the mapping between old and new image names).\n",
        "2) Finds each original image file in your folder.\n",
        " 3) Renames it to the corresponding new standardized name.\n",
        "4) Creates a backup (optional) before renaming, to keep the original files safe.\n",
        " 5) Reports missing or renamed files for verification.\n",
        "#\n",
        " Notes:\n",
        " - This step actually changes filenames in your images folder, unlike the earlier\n",
        "   CSV-only step that just updated name references in the file.\n",
        " - Make sure to set the correct folder path for your images before running."
      ],
      "metadata": {
        "id": "bWsQ0YKZVzKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning - Ingredient Cleaning**\n",
        "\n",
        "\n",
        "\n",
        "### **Ingredient Cleaning -Step 1 â€” Removing Extra Columns**\n",
        "\n",
        "Before starting ingredient cleaning, the dataset contained two unnecessary columns at the end.\n",
        "This step removes them to keep the file clean and consistent.\n",
        "\n",
        "\n",
        "\n",
        "**Actions performed:**\n",
        "\n",
        "1.    Checked that the file exists.\n",
        "\n",
        "1.    Dropped the last two columns using positional indexing.\n",
        "\n",
        "1.    Saved the updated version for the next steps.\n",
        "\n",
        "**Outputs:**\n",
        "'Standerlized_file_cleaned.csv'"
      ],
      "metadata": {
        "id": "osp0dulOw1jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# ðŸ§¹ Remove Last Two Columns from a CSV (by path)\n",
        "# ===========================================\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 1ï¸âƒ£ Set your file path\n",
        "input_path = \"SaudiFoodFile_final_cleaned.csv\"   # ðŸ”¹ Change this to your actual file path\n",
        "output_path = \"/content/Standerlized_file_cleaned.csv\"\n",
        "\n",
        "# 2ï¸âƒ£ Make sure the file exists\n",
        "if not os.path.exists(input_path):\n",
        "    raise FileNotFoundError(f\"âŒ File not found at: {input_path}\")\n",
        "\n",
        "# 3ï¸âƒ£ Load the CSV\n",
        "df = pd.read_csv(input_path)\n",
        "print(\"âœ… Original shape:\", df.shape)\n",
        "\n",
        "# 4ï¸âƒ£ Drop the last two columns\n",
        "df_dropped = df.iloc[:, :-2]\n",
        "print(\"âœ… New shape after removing last two columns:\", df_dropped.shape)\n",
        "\n",
        "# 5ï¸âƒ£ Show which columns were deleted\n",
        "removed_cols = df.columns[-2:].tolist()\n",
        "print(\"ðŸ—‘ï¸ Removed columns:\", removed_cols)\n",
        "\n",
        "# 6ï¸âƒ£ Save the cleaned CSV\n",
        "df_dropped.to_csv(output_path, index=False)\n",
        "print(f\"âœ… Cleaned file saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "1gra2lcmp40u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Ingredient Cleaning - Step 2 â€” Ingredient Cleaning**\n",
        "\n",
        "This step cleans and standardizes all ingredient information before the consistency check.\n",
        "Some dishes originally had messy or incomplete ingredient lists â€” for example:\n",
        "\n",
        "> `\"olive oil / tomatoes - onion, chilli\"` or sometimes just `\"unknown\"`\n",
        "\n",
        "The goal here is to convert everything into clear, structured ingredient lists such as:\n",
        "\n",
        "> `[\"olive oil\", \"tomato\", \"onion\", \"chili\"]`\n",
        "> and to replace unclear or missing entries like `[\"unknown\"]` with the clear label `[\"unknown_ingredients\"]`.\n",
        "\n",
        "**What this step does:**\n",
        "\n",
        "1. **Splits ingredients correctly:**\n",
        "   Separates text using real separators (`|`, `/`, `,`, `;`, Arabic commas, or dashes) without breaking multi-word names.\n",
        "\n",
        "2. **Protects multi-word ingredients:**\n",
        "   Keeps terms like *olive oil* or *tomato paste* together as one ingredient.\n",
        "\n",
        "3. **Removes non-ingredient words:**\n",
        "   Drops extra words such as *add*, *garnish*, *with*, or *hot* that arenâ€™t actual ingredients.\n",
        "\n",
        "4. **Normalizes plurals and spellings:**\n",
        "   Converts plurals (*tomatoes â†’ tomato*) and unifies spellings (*chilli*, *chilies* â†’ *chili*).\n",
        "\n",
        "5. **Handles unknown or missing entries:**\n",
        "   Any ingredient cell that is empty or simply says `\"not found\"` or `\"unknown\"` becomes `[\"unknown\"]` to keep the format consistent.\n",
        "\n",
        "6. **Embedding + Clustering (semantic cleaning):**\n",
        "   group similar ingredient names (like *cardamon* and *cardamom*) into one canonical form.\n",
        "\n",
        "7. **Creates standardized ingredient lists:**\n",
        "   Every dish ends up with a clean list of consistent, machine-readable ingredients.\n",
        "\n",
        "**Outputs:**\n",
        "\n",
        "* `SaudiFoodFile_standardized.csv` â†’ the final cleaned ingredient lists per dish. will be used in the next step.\n",
        "* `ingredient_clusters_report.csv` â†’ groups of similar ingredients and their canonical names (helper)\n",
        "* `ingredient_canonical_map.json` â†’ mapping of each ingredient to its canonical form (helper)\n"
      ],
      "metadata": {
        "id": "-M3Nf4XyxB-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# Student 2 â€” Ingredient Cleaning (phrase-aware, no space-splitting mistakes)\n",
        "# - Split on real separators only (| / \\ , ; ØŒ - with spaces, bullets)\n",
        "# - Protect multi-word ingredient phrases (KEEP_PHRASES)\n",
        "# - Extract ingredients from stray sentences; drop non-ingredient words\n",
        "# - Canonicalize with ALIASES + singularization + fuzzy nudge\n",
        "# - Output only: dish_name | classifications_std_list | image_file | scrape_date\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q pandas sentence-transformers scikit-learn rapidfuzz inflect\n",
        "\n",
        "import re, json, pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from rapidfuzz import process, fuzz\n",
        "import inflect\n",
        "\n",
        "# --- Unknown/placeholder detection ---\n",
        "UNKNOWN_PATTERNS = [\n",
        "    r\"^\\s*not\\s*found\\s*$\",\n",
        "    r\"^\\s*unknown\\s*$\",\n",
        "    r\"^\\s*n/?a\\s*$\",\n",
        "    r\"^\\s*none\\s*$\",\n",
        "    r\"^\\s*null\\s*$\",\n",
        "    r\"^\\s*missing\\s*$\",\n",
        "    r\"^\\s*Ø¨Ø¯ÙˆÙ†\\s*$\",           # Arabic: without\n",
        "    r\"^\\s*ØºÙŠØ±\\s*Ù…ØªÙˆÙØ±\\s*$\",   # Arabic: unavailable\n",
        "]\n",
        "\n",
        "UNKNOWN_REGEXES = [re.compile(p, re.IGNORECASE) for p in UNKNOWN_PATTERNS]\n",
        "\n",
        "def is_unknown_text(s: str) -> bool:\n",
        "    return any(rx.match(s) for rx in UNKNOWN_REGEXES)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "INPUT_CSV  = \"Standerlized_file_cleaned.csv\"\n",
        "OUTPUT_CSV = \"SaudiFoodFile_standardized.csv\"\n",
        "REPORT_CSV = \"ingredient_clusters_report.csv\"\n",
        "MAP_JSON   = \"ingredient_canonical_map.json\"\n",
        "\n",
        "CLASS_COL_CANDIDATES = [\"classifications\", \"classifications_en\", \"classfications\"]\n",
        "MODEL_NAME = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
        "DISTANCE_THRESHOLD = 0.35\n",
        "FUZZY_SCORE_CUTOFF = 92\n",
        "\n",
        "# ----------------------------\n",
        "# Phrase protection & vocabulary\n",
        "# ----------------------------\n",
        "# Multi-word ingredients to KEEP as single tokens\n",
        "KEEP_PHRASES = {\n",
        "    # oils / dairy\n",
        "    \"olive oil\", \"vegetable oil\", \"clarified butter\", \"milk powder\", \"butter milk\", \"heavy cream\",\n",
        "    # stocks / sauces / pastes\n",
        "    \"tomato paste\", \"tomato sauce\", \"soy sauce\", \"pomegranate molasses\", \"date molasses\", \"rose water\", \"orange blossom water\",\n",
        "    # veg & herbs\n",
        "    \"bell pepper\", \"green onion\", \"spring onion\", \"bay leaves\", \"mint leaves\", \"parsley leaves\", \"coriander leaves\",\n",
        "    # spices\n",
        "    \"black lime\", \"mixed spices\", \"spice mix\", \"allspice\", \"black pepper\", \"white pepper\",\n",
        "    # proteins / grains\n",
        "    \"basmati rice\", \"chicken stock\", \"beef stock\", \"vegetable stock\",\n",
        "}\n",
        "\n",
        "# Synonyms/variants â†’ canonical (lowercase)\n",
        "ALIASES = {\n",
        "    # souring / lime\n",
        "    \"dried lime\": \"black lime\", \"dried limes\": \"black lime\", \"omani lime\": \"black lime\",\n",
        "    \"omani limes\": \"black lime\", \"loomi\": \"black lime\", \"black lemon\": \"black lime\",\n",
        "\n",
        "    # oils/fats/dairy\n",
        "    \"veg oil\": \"vegetable oil\", \"olive oils\": \"olive oil\", \"butter milk\": \"buttermilk\",\n",
        "    \"yoghurt\": \"yogurt\", \"labnah\": \"labneh\",\n",
        "\n",
        "    # herbs & veg\n",
        "    \"cilantro\": \"coriander\", \"coriander leaves\": \"coriander\", \"green coriander\": \"coriander\",\n",
        "    \"parsley leaves\": \"parsley\", \"mint leaves\": \"mint\", \"spring onion\": \"green onion\",\n",
        "\n",
        "    # peppers\n",
        "    \"capsicum\": \"bell pepper\", \"green pepper\": \"bell pepper\", \"sweet pepper\": \"bell pepper\",\n",
        "    \"chilli\": \"chili\", \"chilies\": \"chili\", \"chillies\": \"chili\", \"chili pepper\": \"chili\", \"chili peppers\": \"chili\",\n",
        "\n",
        "    # powders & sticks -> base spice\n",
        "    \"turmeric powder\": \"turmeric\", \"ginger powder\": \"ginger\",\n",
        "    \"garlic powder\": \"garlic\", \"onion powder\": \"onion\",\n",
        "    \"cinnamon stick\": \"cinnamon\", \"cinnamon sticks\": \"cinnamon\",\n",
        "    \"cardamon\": \"cardamom\",\n",
        "\n",
        "    # sauces/pastes/syrups\n",
        "    \"tomato purÃ©e\": \"tomato paste\", \"tomato puree\": \"tomato paste\",\n",
        "    \"simple syrup\": \"sugar syrup\",\n",
        "\n",
        "    # pulses/grains\n",
        "    \"garbanzo beans\": \"chickpeas\", \"garbanzo bean\": \"chickpeas\",\n",
        "    \"chick peas\": \"chickpeas\", \"chick pea\": \"chickpeas\",\n",
        "    \"black eyed beans\": \"black-eyed beans\", \"black eyed pea\": \"black-eyed beans\",\n",
        "\n",
        "    # spice mixes / generic\n",
        "    \"spice mix\": \"mixed spices\", \"mix spices\": \"mixed spices\", \"spices mix\": \"mixed spices\"\n",
        "}\n",
        "\n",
        "# Words that are NOT ingredients (filler, verbs, instructions)\n",
        "NON_ING_WORDS = {\n",
        "    \"after\",\"decorate\",\"decoration\",\"decorations\",\"add\",\"with\",\"such\",\"touch\",\"patriotic\",\"cream\",\n",
        "    \"or\",\"and\",\"the\",\"a\",\"an\",\"then\",\"until\",\"when\",\"like\",\"as\",\"to\",\"for\",\"of\",\"into\",\"over\",\n",
        "    \"warm\",\"hot\",\"cold\",\"slice\",\"sliced\",\"diced\",\"chopped\",\"minced\",\"ground\",\"crushed\",\"whole\",\n",
        "    \"fresh\",\"optional\",\"needed\",\"garnish\",\"make\",\"prepare\",\"preparation\",\"cook\",\"cooked\",\"baked\",\n",
        "    \"boiled\",\"fried\",\"seauted\",\"sauteed\",\"browned\",\"mix\",\"mixed\",\"topping\",\"kitchen\",\"precise\",\"instant\"\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "SEP_NORMALIZER = re.compile(r\"[|/\\\\ØŒ;,]+\")      # | / \\ , ; Arabic comma\n",
        "AROUND_HYPHEN  = re.compile(r\"\\s*[-â€“â€”]\\s*\")     # spaced hyphens/dashes as separators\n",
        "BULLETS        = re.compile(r\"[â€¢Â·]+\")\n",
        "\n",
        "# After splitting, strip these inside tokens\n",
        "PUNCT_DROP_INSIDE = re.compile(r\"[-_/\\\\|]+\")\n",
        "NONWORD           = re.compile(r\"[^\\w\\s\\(\\)]\")\n",
        "\n",
        "IRREGULAR = {\"tomatoes\":\"tomato\",\"potatoes\":\"potato\",\"limes\":\"lime\",\"chillies\":\"chili\",\"chilies\":\"chili\",\"cloves\":\"cloves\"}\n",
        "p = inflect.engine()\n",
        "\n",
        "def safe_str(x):\n",
        "    if x is None: return \"\"\n",
        "    s = str(x)\n",
        "    return \"\" if s.strip().lower() in {\"\", \"nan\", \"none\", \"null\"} else s\n",
        "\n",
        "def protect_phrases(text: str) -> str:\n",
        "    \"\"\"Replace spaces inside KEEP_PHRASES with underscores to protect them.\"\"\"\n",
        "    s = text\n",
        "    # longer phrases first to avoid partial overlaps\n",
        "    for ph in sorted(KEEP_PHRASES, key=lambda x: -len(x)):\n",
        "        pattern = r\"\\b\" + re.escape(ph) + r\"\\b\"\n",
        "        s = re.sub(pattern, ph.replace(\" \", \"_\"), s, flags=re.IGNORECASE)\n",
        "    return s\n",
        "\n",
        "def normalize_separators(s: str) -> str:\n",
        "    s = BULLETS.sub(\"|\", s)\n",
        "    s = AROUND_HYPHEN.sub(\"|\", s)           # ' - ' â†’ '|'\n",
        "    s = SEP_NORMALIZER.sub(\"|\", s)          # unify to '|'\n",
        "    s = re.sub(r\"\\|{2,}\", \"|\", s).strip(\"| \")\n",
        "    return s\n",
        "\n",
        "def to_singular(word: str) -> str:\n",
        "    w = word.strip().lower()\n",
        "    if w in IRREGULAR: return IRREGULAR[w]\n",
        "    s = p.singular_noun(w)\n",
        "    return s if isinstance(s, str) and s else w\n",
        "\n",
        "def clean_phrase(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = PUNCT_DROP_INSIDE.sub(\" \", s)\n",
        "    s = NONWORD.sub(\" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def drop_non_ingredients(words):\n",
        "    return [w for w in words if w not in NON_ING_WORDS and len(w) > 1]\n",
        "\n",
        "def split_to_tokens(cell) -> list[str]:\n",
        "    \"\"\"\n",
        "    - If the whole cell is a placeholder (not found/unknown/etc.), return []\n",
        "      so it becomes [\"unknown_ingredients\"] later.\n",
        "    - Normalize separators to '|', protect phrases, split on '|'.\n",
        "    - Extract protected phrases, then ingredient-like words only.\n",
        "    \"\"\"\n",
        "    raw = safe_str(cell)\n",
        "    if not raw:\n",
        "        return []\n",
        "    # whole-cell unknowns â†’ empty list (will become [\"unknown_ingredients\"])\n",
        "    if is_unknown_text(raw):\n",
        "        return []\n",
        "\n",
        "    # normalize separators and protect phrases\n",
        "    s = normalize_separators(raw)\n",
        "    s = protect_phrases(s)\n",
        "    parts = [p.strip() for p in s.split(\"|\") if p.strip()]\n",
        "\n",
        "    tokens = []\n",
        "    for p in parts:\n",
        "        # per-part unknowns too\n",
        "        if is_unknown_text(p):\n",
        "            # skip this part entirely\n",
        "            continue\n",
        "\n",
        "        # restore underscores for already isolated protected phrases\n",
        "        if \"_\" in p and p in KEEP_PHRASES or p.replace(\"_\",\" \") in KEEP_PHRASES:\n",
        "            tokens.append(p.replace(\"_\",\" \"))\n",
        "            continue\n",
        "\n",
        "        # 1) collect any protected phrases still inside\n",
        "        found = []\n",
        "        rest  = p\n",
        "        for ph in sorted(KEEP_PHRASES, key=lambda x: -len(x)):\n",
        "            ph_prot = ph.replace(\" \", \"_\")\n",
        "            if ph_prot.lower() in rest.lower():\n",
        "                found.append(ph)\n",
        "                rest = re.sub(re.escape(ph_prot), \" \", rest, flags=re.IGNORECASE)\n",
        "\n",
        "        # 2) remaining words (filtered)\n",
        "        rest = clean_phrase(rest.replace(\"_\",\" \"))\n",
        "        words = drop_non_ingredients(rest.split())\n",
        "        words = [to_singular(w) for w in words]\n",
        "\n",
        "        for ph in found:\n",
        "            tokens.append(ph)\n",
        "        for w in words:\n",
        "            tokens.append(w)\n",
        "\n",
        "    # alias + dedupe\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        t0 = ALIASES.get(t.strip().lower().replace(\"_\",\" \"), t.strip().lower().replace(\"_\",\" \"))\n",
        "        if t0:\n",
        "            out.append(t0)\n",
        "\n",
        "    seen, dedup = set(), []\n",
        "    for x in out:\n",
        "        if x not in seen:\n",
        "            dedup.append(x); seen.add(x)\n",
        "    return dedup\n",
        "\n",
        "    # normalize separators and protect phrases\n",
        "    s = normalize_separators(raw)\n",
        "    s = protect_phrases(s)\n",
        "    parts = [p.strip() for p in s.split(\"|\") if p.strip()]\n",
        "\n",
        "    tokens = []\n",
        "    for p in parts:\n",
        "        # restore underscores for already isolated protected phrases\n",
        "        if \"_\" in p and p in KEEP_PHRASES or p.replace(\"_\",\" \") in KEEP_PHRASES:\n",
        "            tokens.append(p.replace(\"_\",\" \"))\n",
        "            continue\n",
        "\n",
        "        # If it's a sentence: extract any protected phrases inside, then words\n",
        "        # 1) pull out protected phrases inside the piece\n",
        "        found = []\n",
        "        rest  = p\n",
        "        for ph in sorted(KEEP_PHRASES, key=lambda x: -len(x)):\n",
        "            ph_prot = ph.replace(\" \", \"_\")\n",
        "            if ph_prot.lower() in rest.lower():\n",
        "                # collect and remove\n",
        "                found.append(ph)\n",
        "                rest = re.sub(re.escape(ph_prot), \" \", rest, flags=re.IGNORECASE)\n",
        "\n",
        "        # 2) remaining words\n",
        "        rest = clean_phrase(rest.replace(\"_\",\" \"))\n",
        "        words = drop_non_ingredients(rest.split())\n",
        "        # singularize last token of any 1-2 word units (light touch)\n",
        "        words = [to_singular(w) for w in words]\n",
        "\n",
        "        # combine: protected phrases + remaining words\n",
        "        for ph in found:\n",
        "            tokens.append(ph)\n",
        "        for w in words:\n",
        "            tokens.append(w)\n",
        "\n",
        "    # apply aliases & final cleanup\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        t0 = t.strip().lower()\n",
        "        t0 = t0.replace(\"_\",\" \")\n",
        "        if not t0:\n",
        "            continue\n",
        "        t0 = ALIASES.get(t0, t0)\n",
        "        out.append(t0)\n",
        "\n",
        "    # de-dup preserve order\n",
        "    seen, dedup = set(), []\n",
        "    for x in out:\n",
        "        if x not in seen:\n",
        "            dedup.append(x); seen.add(x)\n",
        "\n",
        "    return dedup\n",
        "\n",
        "def normalize_for_clustering(token: str) -> str:\n",
        "    \"\"\"Secondary normalization for clustering stage.\"\"\"\n",
        "    t = token.strip().lower()\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t\n",
        "\n",
        "# ----------------------------\n",
        "# Load data (robust to \"NA\")\n",
        "# ----------------------------\n",
        "df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\", keep_default_na=False, na_filter=False)\n",
        "n_rows = len(df)\n",
        "CLASS_COL = next((c for c in CLASS_COL_CANDIDATES if c in df.columns), None)\n",
        "if not CLASS_COL:\n",
        "    raise ValueError(f\"Could not find classifications column. Found: {list(df.columns)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Tokenize all rows with phrase-aware extractor\n",
        "# ----------------------------\n",
        "raw_lists = df[CLASS_COL].apply(split_to_tokens)\n",
        "\n",
        "# Clean tokens for clustering\n",
        "cleaned_all, bag = [], []\n",
        "for tokens in raw_lists:\n",
        "    cleaned = []\n",
        "    for t in tokens:\n",
        "        nt = normalize_for_clustering(t)\n",
        "        if nt:\n",
        "            cleaned.append(nt)\n",
        "            bag.append(nt)\n",
        "    cleaned_all.append(cleaned)\n",
        "\n",
        "freq = Counter(bag)\n",
        "unique_tokens = list(freq.keys())\n",
        "\n",
        "# If no tokens â†’ fill unknowns but still emit rows\n",
        "if not unique_tokens:\n",
        "    df[\"classifications_std_list\"] = [[\"unknown_ingredients\"] for _ in range(n_rows)]\n",
        "    out = df[[\"dish_name\",\"classifications_std_list\",\"image_file\",\"scrape_date\"]]\n",
        "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "    print(f\"âœ… Done (all unknown). Rows: {len(out)}\")\n",
        "    raise SystemExit\n",
        "\n",
        "# ----------------------------\n",
        "# Embed & cluster\n",
        "# ----------------------------\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "emb = model.encode(unique_tokens, show_progress_bar=True, normalize_embeddings=True)\n",
        "\n",
        "try:\n",
        "    clust = AgglomerativeClustering(\n",
        "        n_clusters=None, linkage=\"average\",\n",
        "        metric=\"cosine\", distance_threshold=DISTANCE_THRESHOLD\n",
        "    )\n",
        "except TypeError:\n",
        "    clust = AgglomerativeClustering(\n",
        "        n_clusters=None, linkage=\"average\",\n",
        "        affinity=\"cosine\", distance_threshold=DISTANCE_THRESHOLD\n",
        "    )\n",
        "labels = clust.fit_predict(emb)\n",
        "\n",
        "clusters = defaultdict(list)\n",
        "for tok, lab in zip(unique_tokens, labels):\n",
        "    clusters[lab].append(tok)\n",
        "\n",
        "# canonical per cluster: most frequent (tie -> shortest)\n",
        "cluster_canonical = {lab: sorted(toks, key=lambda t: (-freq[t], len(t)))[0] for lab, toks in clusters.items()}\n",
        "token2canon = {tok: cluster_canonical[lab] for tok, lab in zip(unique_tokens, labels)}\n",
        "\n",
        "# optional fuzzy nudge\n",
        "canonical_vocab = list(set(token2canon.values()))\n",
        "def fuzzy_canon(token: str, cutoff=FUZZY_SCORE_CUTOFF):\n",
        "    best = process.extractOne(token, canonical_vocab, scorer=fuzz.WRatio, score_cutoff=cutoff)\n",
        "    return best[0] if best else token\n",
        "for tok in list(token2canon.keys()):\n",
        "    cand = fuzzy_canon(tok)\n",
        "    if cand != token2canon[tok] and freq.get(cand, 0) >= freq.get(token2canon[tok], 0):\n",
        "        token2canon[tok] = cand\n",
        "\n",
        "# ----------------------------\n",
        "# Apply mapping to EVERY row\n",
        "# ----------------------------\n",
        "std_lists = []\n",
        "for cleaned in cleaned_all:\n",
        "    mapped = [token2canon.get(t, t) for t in cleaned]\n",
        "    # de-dup preserve order\n",
        "    seen, dedup = set(), []\n",
        "    for x in mapped:\n",
        "        if x not in seen:\n",
        "            dedup.append(x); seen.add(x)\n",
        "    if not dedup:\n",
        "        dedup = [\"unknown_ingredients\"]\n",
        "    std_lists.append(dedup)\n",
        "\n",
        "assert len(std_lists) == n_rows\n",
        "df[\"classifications_std_list\"] = std_lists\n",
        "\n",
        "# --- Force unknown_ingredients for placeholder fragments like [\"not\",\"found\"] ---\n",
        "def coalesce_unknown(lst):\n",
        "    # If list is empty, we'll handle later; if it exactly equals [\"not\",\"found\"], force unknown\n",
        "    if not lst:\n",
        "        return [\"unknown_ingredients\"]\n",
        "    lf = [x.strip().lower() for x in lst]\n",
        "    if lf == [\"not\",\"found\"] or lf == [\"unknown\"]:\n",
        "        return [\"unknown_ingredients\"]\n",
        "    # If list contains only non-ingredient placeholders, collapse too\n",
        "    joined = \" \".join(lf)\n",
        "    if is_unknown_text(joined):\n",
        "        return [\"unknown_ingredients\"]\n",
        "    return lst\n",
        "\n",
        "df[\"classifications_std_list\"] = df[\"classifications_std_list\"].apply(coalesce_unknown)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Post-processing consistency fix: collapse generic + specific variants\n",
        "# ---------------------------------------------------------\n",
        "canonical_collapse = {\n",
        "    # generic â†’ preferred\n",
        "    \"oil\": \"olive oil\",\n",
        "    \"vegetable oil\": \"olive oil\",\n",
        "    \"ghee\": \"butter\",\n",
        "    \"yogurt\": \"labneh\",       # example, if you prefer labneh\n",
        "    # add any others you notice\n",
        "}\n",
        "\n",
        "def collapse_variants(lst):\n",
        "    \"\"\"Replace generic tokens with canonical equivalents and de-duplicate.\"\"\"\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for x in lst:\n",
        "        y = canonical_collapse.get(x, x)\n",
        "        if y not in seen:\n",
        "            out.append(y)\n",
        "            seen.add(y)\n",
        "    return out\n",
        "\n",
        "df[\"classifications_std_list\"] = df[\"classifications_std_list\"].apply(collapse_variants)\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Reports & mapping\n",
        "# ----------------------------\n",
        "rows = []\n",
        "for lab, toks in clusters.items():\n",
        "    can = cluster_canonical[lab]\n",
        "    for t in sorted(toks):\n",
        "        rows.append({\"cluster_id\": lab, \"canonical\": can, \"member\": t, \"member_freq\": freq[t]})\n",
        "pd.DataFrame(rows).sort_values([\"canonical\",\"member\"]).to_csv(REPORT_CSV, index=False, encoding=\"utf-8\")\n",
        "with open(MAP_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(token2canon, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ----------------------------\n",
        "# Save ONLY the requested 4 columns\n",
        "# ----------------------------\n",
        "required_cols = [\"dish_name\", \"image_file\", \"scrape_date\"]\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
        "\n",
        "out = df[[\"dish_name\",\"classifications_std_list\",\"image_file\",\"scrape_date\"]]\n",
        "out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Done.\")\n",
        "print(f\"Rows in/out: {n_rows} / {len(out)}\")\n",
        "print(f\"Example rows:\\n{out.head(6).to_string(index=False)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9zdufmpAp8hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ingredient Cleaning - Step 3 â€” Final Consistency Pass**\n",
        "\n",
        "After cleaning and standardizing the ingredients, some names were still not completely consistent.\n",
        "For example, a few dishes still used slightly different spellings or duplicate ingredient terms such as:\n",
        "\n",
        "> [\"paper\", \"chilli\", \"olive oils\"] instead of [\"chili\", \"olive oil\"].\n",
        "\n",
        "This step was added to double-check and correct any remaining inconsistencies in spelling, wording, or duplicates.\n",
        "It reviews every ingredient list and makes final adjustments to ensure that all rows follow the same standard format."
      ],
      "metadata": {
        "id": "h3BOTp3SxRES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Consistency pass over the produced CSV\n",
        "# - Enforce canonical spellings & synonyms (e.g., chilli â†’ chili)\n",
        "# - Your requests: paper â†’ chili, allspices â†’ mixed spices\n",
        "# - Remove non-ingredient words, dedupe, keep only 4 columns\n",
        "# Outputs:\n",
        "#   â€¢ SaudiFoodFile_standardized_consistent.csv\n",
        "#   â€¢ consistency_changes_report.csv (what changed)\n",
        "# ============================================================\n",
        "\n",
        "import ast, re, json, pandas as pd\n",
        "\n",
        "INPUT_CSV  = \"SaudiFoodFile_standardized.csv\"   # <â€” your produced file\n",
        "OUTPUT_CSV = \"SaudiFoodFile_standardized_consistent.csv\"\n",
        "REPORT_CSV = \"consistency_changes_report.csv\"\n",
        "\n",
        "# If your column name differs, adjust here:\n",
        "CLASS_COL = \"classifications_std_list\"\n",
        "REQ_COLS  = [\"dish_name\", CLASS_COL, \"image_file\", \"scrape_date\"]\n",
        "\n",
        "# ---------- canonicalization config ----------\n",
        "# âœ… Add/adjust anything you want here:\n",
        "ALIASES = {\n",
        "    # your explicit requests\n",
        "    \"paper\": \"chili\",              # e.g., OCR/typo -> chili\n",
        "    \"allspices\": \"mixed spices\",\n",
        "    \"all-spices\": \"mixed spices\",\n",
        "    \"all spice\": \"mixed spices\",\n",
        "    \"all-spice\": \"mixed spices\",\n",
        "    \"allspice\": \"mixed spices\",    # if you prefer \"mixed spices\" as canonical\n",
        "\n",
        "    # common spellings / plurals / variants\n",
        "    \"chilli\": \"chili\",\n",
        "    \"chilies\": \"chili\",\n",
        "    \"chillies\": \"chili\",\n",
        "    \"green chilli\": \"chili\",\n",
        "    \"green chili\": \"chili\",\n",
        "    \"red chili\": \"chili\",\n",
        "    \"red chilli\": \"chili\",\n",
        "    \"chili pepper\": \"chili\",\n",
        "    \"chili peppers\": \"chili\",\n",
        "\n",
        "    \"black lemon\": \"black lime\",\n",
        "    \"dried lime\": \"black lime\",\n",
        "    \"dried limes\": \"black lime\",\n",
        "    \"omani lime\": \"black lime\",\n",
        "    \"omani limes\": \"black lime\",\n",
        "    \"loomi\": \"black lime\",\n",
        "\n",
        "    \"veg oil\": \"vegetable oil\",\n",
        "    \"olive oils\": \"olive oil\",\n",
        "\n",
        "    \"cilantro\": \"coriander\",\n",
        "    \"coriander leaves\": \"coriander\",\n",
        "    \"green coriander\": \"coriander\",\n",
        "    \"parsley leaves\": \"parsley\",\n",
        "    \"mint leaves\": \"mint\",\n",
        "    \"spring onion\": \"green onion\",\n",
        "    \"capsicum\": \"bell pepper\",\n",
        "    \"green pepper\": \"bell pepper\",\n",
        "    \"sweet pepper\": \"bell pepper\",\n",
        "\n",
        "    \"turmeric powder\": \"turmeric\",\n",
        "    \"ginger powder\": \"ginger\",\n",
        "    \"garlic powder\": \"garlic\",\n",
        "    \"onion powder\": \"onion\",\n",
        "    \"cinnamon stick\": \"cinnamon\",\n",
        "    \"cinnamon sticks\": \"cinnamon\",\n",
        "    \"cardamon\": \"cardamom\",\n",
        "\n",
        "    \"tomato puree\": \"tomato paste\",\n",
        "    \"tomato purÃ©e\": \"tomato paste\",\n",
        "    \"simple syrup\": \"sugar syrup\",\n",
        "\n",
        "    \"garbanzo bean\": \"chickpeas\",\n",
        "    \"garbanzo beans\": \"chickpeas\",\n",
        "    \"chick pea\": \"chickpeas\",\n",
        "    \"chick peas\": \"chickpeas\",\n",
        "    \"black eyed bean\": \"black-eyed beans\",\n",
        "    \"black eyed beans\": \"black-eyed beans\",\n",
        "}\n",
        "\n",
        "# If BOTH appear, drop the generic in favor of the preferred.\n",
        "# (You can add more pairs here.)\n",
        "COLLAPSE_IF_PRESENT = [\n",
        "    (\"oil\", \"olive oil\"),          # keep \"olive oil\", drop \"oil\"\n",
        "    (\"vegetable oil\", \"olive oil\"),# keep \"olive oil\"\n",
        "    (\"pepper\", \"chili\"),           # if chili is present, drop generic \"pepper\"\n",
        "]\n",
        "\n",
        "# Words to drop if they sneak in (not ingredients)\n",
        "NON_ING_WORDS = {\n",
        "    \"after\",\"decorate\",\"decoration\",\"decorations\",\"add\",\"with\",\"such\",\"touch\",\"patriotic\",\n",
        "    \"or\",\"and\",\"the\",\"a\",\"an\",\"then\",\"until\",\"when\",\"like\",\"as\",\"to\",\"for\",\"of\",\"into\",\"over\",\n",
        "    \"warm\",\"hot\",\"cold\",\"slice\",\"sliced\",\"diced\",\"chopped\",\"minced\",\"ground\",\"crushed\",\"whole\",\n",
        "    \"fresh\",\"optional\",\"needed\",\"garnish\",\"make\",\"prepare\",\"preparation\",\"cook\",\"cooked\",\"baked\",\n",
        "    \"boiled\",\"fried\",\"seauted\",\"sauteed\",\"browned\",\"mix\",\"mixed\",\"topping\",\"kitchen\",\"precise\",\"instant\"\n",
        "}\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def parse_list_cell(x):\n",
        "    \"\"\"Parse list stored as Python-list string, or already-list, or fallback.\"\"\"\n",
        "    if isinstance(x, list):\n",
        "        return [str(t).strip().lower() for t in x if str(t).strip()]\n",
        "    s = str(x).strip()\n",
        "    if not s:\n",
        "        return []\n",
        "    try:\n",
        "        val = ast.literal_eval(s)\n",
        "        if isinstance(val, list):\n",
        "            return [str(t).strip().lower() for t in val if str(t).strip()]\n",
        "    except Exception:\n",
        "        pass\n",
        "    # fallback: split by | or comma\n",
        "    parts = re.split(r\"\\s*\\|\\s*|,+\", s)\n",
        "    return [p.strip().lower() for p in parts if p.strip()]\n",
        "\n",
        "def apply_aliases(tokens):\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        t0 = t.strip().lower()\n",
        "        if not t0 or t0 in NON_ING_WORDS:\n",
        "            continue\n",
        "        t0 = ALIASES.get(t0, t0)\n",
        "        out.append(t0)\n",
        "    # de-dup preserve order\n",
        "    seen, dedup = set(), []\n",
        "    for x in out:\n",
        "        if x not in seen:\n",
        "            dedup.append(x); seen.add(x)\n",
        "    return dedup\n",
        "\n",
        "def collapse_generics(tokens):\n",
        "    s = set(tokens)\n",
        "    # if preferred present, drop generic\n",
        "    for generic, preferred in COLLAPSE_IF_PRESENT:\n",
        "        if preferred in s and generic in s:\n",
        "            s.discard(generic)\n",
        "    # rebuild original order\n",
        "    out, seen = [], set()\n",
        "    for x in tokens:\n",
        "        if x in s and x not in seen:\n",
        "            out.append(x); seen.add(x)\n",
        "    return out\n",
        "\n",
        "# ---------- load ----------\n",
        "df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
        "missing = [c for c in REQ_COLS if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# ---------- process ----------\n",
        "changes = []\n",
        "new_lists = []\n",
        "for idx, row in df.iterrows():\n",
        "    original = parse_list_cell(row[CLASS_COL])\n",
        "    aliased  = apply_aliases(original)\n",
        "    collapsed = collapse_generics(aliased)\n",
        "\n",
        "    # track changes (only if different)\n",
        "    if original != collapsed:\n",
        "        changes.append({\n",
        "            \"row\": idx,\n",
        "            \"before\": json.dumps(original, ensure_ascii=False),\n",
        "            \"after\":  json.dumps(collapsed, ensure_ascii=False),\n",
        "        })\n",
        "    new_lists.append(collapsed if collapsed else [\"unknown_ingredients\"])\n",
        "\n",
        "df[CLASS_COL] = new_lists\n",
        "\n",
        "# keep only requested 4 columns, same order\n",
        "out = df[[\"dish_name\", CLASS_COL, \"image_file\", \"scrape_date\"]]\n",
        "out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "pd.DataFrame(changes).to_csv(REPORT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Consistency pass complete.\")\n",
        "print(f\"â€¢ Output CSV: {OUTPUT_CSV}\")\n",
        "print(f\"â€¢ Changes report: {REPORT_CSV}\")\n",
        "print(f\"Rows changed: {len(changes)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Axs63FW1qDSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}