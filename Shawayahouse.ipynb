{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7NmApITxznL+0QB0C+1+L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoraHK3/DataSciProject/blob/main/Shawayahouse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install playwright==1.46.0 requests nest_asyncio pillow\n",
        "!playwright install chromium\n",
        "\n",
        "import os, re, csv, json, time, unicodedata, asyncio, requests\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from urllib import robotparser\n",
        "from playwright.async_api import async_playwright\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# ---------- Config ----------\n",
        "BASE_URL = \"https://shawayahouse.my.taker.io\"\n",
        "MENU_ROOT = f\"{BASE_URL}/menu?language=ar\"\n",
        "UA = (\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "      \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\")\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "# ----------------------------\n",
        "\n",
        "# ---------- Utils ----------\n",
        "AR_DIACRITICS = re.compile(r\"[\\u064B-\\u0652\\u0670\\u0640]\")  # تنوين/تشكيل + تطويل\n",
        "def ar_clean(text):\n",
        "    if not text: return \"\"\n",
        "    t = unicodedata.normalize(\"NFKC\", text)\n",
        "    t = AR_DIACRITICS.sub(\"\", t)          # شيل التشكيل والتمطيط\n",
        "    t = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \" \", t)  # شيل رموز غريبة\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def slugify(text, maxlen=80):\n",
        "    if not text: return \"image\"\n",
        "    t = unicodedata.normalize(\"NFKD\", text)\n",
        "    t = \"\".join(ch for ch in t if not unicodedata.combining(ch))\n",
        "    t = re.sub(r\"[^0-9A-Za-z\\u0600-\\u06FF _.-]+\", \"\", t)\n",
        "    t = re.sub(r\"\\s+\", \"_\", t.strip())\n",
        "    return (t[:maxlen] or \"image\")\n",
        "\n",
        "def robots_allows(url, user_agent=\"*\"):\n",
        "    rp = robotparser.RobotFileParser()\n",
        "    rp.set_url(urljoin(BASE_URL, \"/robots.txt\"))\n",
        "    try: rp.read()\n",
        "    except: return True\n",
        "    return rp.can_fetch(user_agent, url)\n",
        "\n",
        "def ensure_abs(url):\n",
        "    return url if bool(urlparse(url).netloc) else urljoin(BASE_URL, url)\n",
        "\n",
        "def download_image(img_url, name_hint, referer):\n",
        "    if not img_url: return \"\"\n",
        "    img_url = ensure_abs(img_url)\n",
        "    headers = {\"User-Agent\": UA, \"Referer\": referer}\n",
        "    r = requests.get(img_url, headers=headers, timeout=60, stream=True)\n",
        "    r.raise_for_status()\n",
        "    data = r.content\n",
        "    # حدد الامتداد من المحتوى\n",
        "    try:\n",
        "        im = Image.open(BytesIO(data))\n",
        "        ext = (im.format or \"JPEG\").lower()\n",
        "        if ext == \"jpeg\": ext = \"jpg\"\n",
        "        if ext not in (\"jpg\",\"png\",\"webp\"): ext = \"jpg\"\n",
        "    except Exception:\n",
        "        ext = \"jpg\"\n",
        "    fn = f\"{slugify(name_hint)}.{ext}\"\n",
        "    out_path = os.path.join(\"images\", fn)\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        f.write(data)\n",
        "    return out_path\n",
        "\n",
        "# ---------- Classification (مُحدّث مع تنظيف عربي + اعتماد على اسم القسم والوصف) ----------\n",
        "CLASS_MAP = [\n",
        "    (r\"(رز|ارز|كبسه|كبسة|برياني|مندي|mandi|kabsa|rice|biryani|جريش|قرصان|هريس)\", \"Rice\"),\n",
        "    (r\"(سلطه|سلطة|تبوله|تبولة|فتوش|salad|tabbouleh|fattoush|كولسلو)\", \"Salad\"),\n",
        "    (r\"(دجاج|مسحب|شوايه|شواية|chicken|tawook|shawaya|broast)\", \"Chicken\"),\n",
        "    (r\"(لحم|بقري|كباب|برجر|burger|beef)\", \"Meat\"),\n",
        "    (r\"(غنم|ضأن|lamb|mutton|حاشي|camel)\", \"Lamb\"),\n",
        "    (r\"(سمك|هامور|fish|روبيان|shrimp|جمبري)\", \"Fish\"),\n",
        "    (r\"(خبز|صامولي|عربي|pita|tamees|tortilla|saj|bread)\", \"Bread\"),\n",
        "    (r\"(بطاطس|بطاطا|فرايز|fries)\", \"Fries\"),\n",
        "    (r\"(صلصه|صلصة|صوص|ثوم|طحينه|طحينة|مايونيز|كاتشب|sauce|garlic|tahini|ketchup)\", \"Sauce\"),\n",
        "    # الحلويات (Dessert)\n",
        "    (r\"(حلى|حلويات|كنافه|كنافة|لقيمات|بسبوسه|بسبوسة|معمول|dessert|kunafa|basbousa|maamoul|ام_علي|أم_علي|مهلبيه|مهلبية)\", \"Dessert\"),\n",
        "    (r\"(مكرونه|مكرونة|باستا|pasta)\", \"Pasta\"),\n",
        "    (r\"(شاورما|shawarma|راب|wrap|سندوتش|سندويتش|ساندوتش|ساندويتش)\", \"Sandwich\"),\n",
        "    # إضافاتك: sides/vegetables\n",
        "    (r\"(بشاميل|ملوخيه|ملوخية|خضار مشكل|خضار_مشكل|مصقعه|مصقعة)\", \"Sides\"),\n",
        "     # المشروبات\n",
        "    (r\"(مويه|موية|ماء|مياه|معدنيه|معدنية|مياه معدنية|مياه_معدنيه|لبن|لَبَن|laban|milk|مشلبي|مشلبيه|مشروبات غازيه|مشروبات غازية|بيبسي|pepsi|كوكاكولا|coca[- ]?cola|سفن اب|7up|sprite|سبرايت|fanta|فانتا|cola|coke)\", \"Drinks\"),\n",
        "\n",
        "]\n",
        "\n",
        "SECTION_HINTS = [  # لو اسم القسم فيه كلمة تدل، نضيفها للمساعدة\n",
        "    (r\"(السلطات|سلطات|سلطه|سلطة)\", \"Salad\"),\n",
        "    (r\"(الحلى|حلويات)\", \"Dessert\"),\n",
        "    (r\"(السندوتشات|السندويتشات|الشاورما|ساندويتش|رابس?)\", \"Sandwich\"),\n",
        "    (r\"(الاطباق الرئيسية|الرئيسية|الرز|الارز|المنسف|الكبسه)\", \"Rice\"),\n",
        "    (r\"(المشويات|الشوي|grill|مشوي)\", \"Meat\"),\n",
        "    (r\"(البطاطس|الفرايز)\", \"Fries\"),\n",
        "    (r\"(الصوص|الصلصات)\", \"Sauce\"),\n",
        "]\n",
        "\n",
        "def classify_food(name=\"\", desc=\"\", section_name=\"\"):\n",
        "    text = ar_clean(f\"{name} {desc}\")\n",
        "    sec  = ar_clean(section_name)\n",
        "    labels = set()\n",
        "    # من اسم القسم أولاً\n",
        "    for pat, lab in SECTION_HINTS:\n",
        "        if re.search(pat, sec, flags=re.IGNORECASE):\n",
        "            labels.add(lab)\n",
        "    # من الاسم + الوصف\n",
        "    for pat, lab in CLASS_MAP:\n",
        "        if re.search(pat, text, flags=re.IGNORECASE):\n",
        "            labels.add(lab)\n",
        "    return \" - \".join(labels) if labels else \"Unclassified\"\n",
        "\n",
        "# ---------- Scraping ----------\n",
        "async def scroll_to_bottom(page, max_rounds=20, pause=800):\n",
        "    last = 0\n",
        "    for _ in range(max_rounds):\n",
        "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "        await page.wait_for_timeout(pause)\n",
        "        h = await page.evaluate(\"document.body.scrollHeight\")\n",
        "        if h == last: break\n",
        "        last = h\n",
        "\n",
        "async def get_all_category_links(context):\n",
        "    page = await context.new_page()\n",
        "    await page.goto(MENU_ROOT, wait_until=\"domcontentloaded\", timeout=60000)\n",
        "    await scroll_to_bottom(page, max_rounds=10)\n",
        "    anchors = await page.query_selector_all('a[href*=\"/menu/category/\"]')\n",
        "    links = set()\n",
        "    for a in anchors:\n",
        "        href = await a.get_attribute(\"href\")\n",
        "        if href and \"/menu/category/\" in href:\n",
        "            links.add(ensure_abs(href.split(\"?\")[0] + \"?language=ar\"))\n",
        "    await page.close()\n",
        "    return sorted(links)\n",
        "\n",
        "async def scrape_category(context, category_url):\n",
        "    rows = []\n",
        "    page = await context.new_page()\n",
        "    await page.goto(category_url, wait_until=\"domcontentloaded\", timeout=60000)\n",
        "    await scroll_to_bottom(page)\n",
        "    # اسم القسم (يساعد في التصنيف)\n",
        "    section_name = (await page.title()) or \"\"\n",
        "    # كروت المنتجات\n",
        "    cards = await page.query_selector_all('a[href*=\"/menu/product\"]')\n",
        "    # لو صفر، جرب تحديد أوسع\n",
        "    if not cards:\n",
        "        cards = await page.query_selector_all(\"a.menu-product, a.card, a[href*='product']\")\n",
        "    # fallback: صور ببديل alt\n",
        "    imgs = await page.query_selector_all(\"img\")\n",
        "\n",
        "    # من روابط المنتجات\n",
        "    for a in cards:\n",
        "        name = (await a.get_attribute(\"title\")) or (await a.inner_text() or \"\")\n",
        "        name = ar_clean(name)\n",
        "        img_url = \"\"\n",
        "        img = await a.query_selector(\"img\")\n",
        "        if not img:\n",
        "            # جرب أقرب كارد\n",
        "            img = await a.evaluate_handle(\n",
        "                \"el => el.closest('article,div,li,section') && el.closest('article,div,li,section').querySelector('img')\"\n",
        "            )\n",
        "        if img:\n",
        "            try:\n",
        "                if hasattr(img, \"get_attribute\"):\n",
        "                    for attr in [\"src\",\"data-src\",\"data-original\",\"data-lazy\",\"data-srcset\",\"srcset\"]:\n",
        "                        v = await img.get_attribute(attr)\n",
        "                        if v and v.strip():\n",
        "                            img_url = v.split()[0].strip(); break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if name and img_url:\n",
        "            rows.append({\n",
        "                \"name\": name,\n",
        "                \"img_url\": img_url,\n",
        "                \"section\": section_name,\n",
        "                \"classification\": classify_food(name=name, section_name=section_name)\n",
        "            })\n",
        "\n",
        "    # fallback: أي صورة alt فيها اسم أكلة\n",
        "    if not rows and imgs:\n",
        "        for img in imgs:\n",
        "            alt = ar_clean((await img.get_attribute(\"alt\") or \"\"))\n",
        "            if not alt: continue\n",
        "            src = \"\"\n",
        "            for attr in [\"src\",\"data-src\",\"data-original\",\"data-lazy\",\"data-srcset\",\"srcset\"]:\n",
        "                v = await img.get_attribute(attr)\n",
        "                if v and v.strip():\n",
        "                    src = v.split()[0].strip(); break\n",
        "            if alt and src:\n",
        "                rows.append({\n",
        "                    \"name\": alt,\n",
        "                    \"img_url\": src,\n",
        "                    \"section\": section_name,\n",
        "                    \"classification\": classify_food(name=alt, section_name=section_name)\n",
        "                })\n",
        "\n",
        "    await page.close()\n",
        "    return rows\n",
        "\n",
        "async def run_all():\n",
        "    # robots.txt check\n",
        "    if not robots_allows(MENU_ROOT, user_agent=\"*\"):\n",
        "        print(\"[robots] Not allowed. Stop.\")\n",
        "        return []\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True, args=[\"--no-sandbox\"])\n",
        "        context = await browser.new_context(locale=\"ar-SA\", user_agent=UA)\n",
        "\n",
        "        # 1) اجمع كل روابط الأقسام\n",
        "        cats = await get_all_category_links(context)\n",
        "        print(f\"[info] Categories found: {len(cats)}\")\n",
        "        for c in cats: print(\" -\", c)\n",
        "\n",
        "        # 2) اكشط كل قسم\n",
        "        all_rows = []\n",
        "        for c in cats:\n",
        "            try:\n",
        "                rows = await scrape_category(context, c)\n",
        "                print(f\"[info] {c} -> {len(rows)} items\")\n",
        "                all_rows.extend(rows)\n",
        "            except Exception as e:\n",
        "                print(\"[warn] category failed:\", c, e)\n",
        "\n",
        "        await context.close()\n",
        "        await browser.close()\n",
        "    return all_rows\n",
        "\n",
        "# ---------- Run ----------\n",
        "rows = asyncio.get_event_loop().run_until_complete(run_all())\n",
        "print(\"Total parsed rows:\", len(rows))\n",
        "print(rows[:5])\n",
        "\n",
        "# ---------- Save (download images + CSV/JSON) ----------\n",
        "final = []\n",
        "scrape_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "for r in rows:\n",
        "    img_path = download_image(r[\"img_url\"], r[\"name\"] or \"item\", MENU_ROOT)\n",
        "    final.append({\n",
        "        \"name\": r[\"name\"],\n",
        "        \"image_file\": img_path,\n",
        "        \"classification\": r[\"classification\"],\n",
        "        \"scrape_date\": scrape_date\n",
        "    })\n",
        "\n",
        "with open(\"dishes.csv\",\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=[\"name\",\"image_file\",\"classification\",\"scrape_date\"])\n",
        "    w.writeheader()\n",
        "    w.writerows(final)\n",
        "\n",
        "with open(\"dishes.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump(final, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved {len(final)} items → dishes.csv & dishes.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bocp1r63k1Wc",
        "outputId": "8f427683-f191-43cc-8600-e87c0b9da9d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] Categories found: 10\n",
            " - https://shawayahouse.my.taker.io/menu/category/10?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/11?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/12?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/18?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/20?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/25?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/6?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/7?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/8?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/9?language=ar\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/10?language=ar -> 5 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/11?language=ar -> 3 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/12?language=ar -> 7 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/18?language=ar -> 1 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/20?language=ar -> 1 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/25?language=ar -> 2 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/6?language=ar -> 4 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/7?language=ar -> 4 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/8?language=ar -> 4 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/9?language=ar -> 6 items\n",
            "Total parsed rows: 37\n",
            "[{'name': 'كنافة بالقشطة', 'img_url': 'https://images.taker.io/shawaya_house/1689255609_XcV5vl.jpg', 'section': 'بيت الشواية - الحلا', 'classification': 'Dessert'}, {'name': 'أم علي', 'img_url': 'https://images.taker.io/shawaya_house/1689256976_5ZSOom.jpg', 'section': 'بيت الشواية - الحلا', 'classification': 'Unclassified'}, {'name': 'سلطة فواكة', 'img_url': 'https://images.taker.io/shawaya_house/1689255665_0RUWxp.jpg', 'section': 'بيت الشواية - الحلا', 'classification': 'Salad'}, {'name': 'أرز بالحليب بالفرن', 'img_url': 'https://images.taker.io/shawaya_house/1689256946_AoJ1Tr.jpg', 'section': 'بيت الشواية - الحلا', 'classification': 'Rice'}, {'name': 'مهلبية', 'img_url': 'https://images.taker.io/shawaya_house/1689256463_pvpocB.jpg', 'section': 'بيت الشواية - الحلا', 'classification': 'Dessert'}]\n",
            "Saved 37 items → dishes.csv & dishes.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# اضغطي مجلد الصور إلى ملف ZIP\n",
        "import shutil, os, zipfile\n",
        "from google.colab import files\n",
        "\n",
        "IMAGES_DIR = \"images\"  # غيّريه إذا غيرتي مسار الصور\n",
        "ZIP_NAME = \"images_backup.zip\"\n",
        "\n",
        "# يصنع ملف ZIP من مجلد الصور\n",
        "shutil.make_archive(\"images_backup\", \"zip\", IMAGES_DIR)\n",
        "\n",
        "# نزّلي ملف الـ ZIP + الملفات المرافقة\n",
        "files.download(ZIP_NAME)            # ينزّل مجلد الصور مضغوط\n",
        "if os.path.exists(\"dishes.csv\"): files.download(\"dishes.csv\")\n",
        "if os.path.exists(\"dishes.json\"): files.download(\"dishes.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dBvXx31yrC9b",
        "outputId": "0090ce8f-c190-45ce-aa8a-5897fb11c00c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_673fce08-fb9c-4de9-8a01-32cd6488572c\", \"images_backup.zip\", 2471969)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_86eb6b4e-a226-44a4-a89a-0211f01880c1\", \"dishes.csv\", 3054)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a06ab90d-0a2f-4387-a346-34d28ebe5b7b\", \"dishes.json\", 6342)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}