{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoraHK3/DataSciProject/blob/main/Data_cleaning_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **1- CSV Translation**\n",
        "\n",
        "The original dataset contained a mix of **Arabic and English** text â€” some dish names and ingredients were already in English, while others were still in Arabic.\n",
        "To make the data consistent and ready for analysis, this step translates **all Arabic parts** into **English**, while keeping the existing English text as it is.\n",
        "\n",
        "It uses **Google Translate** through the `deep-translator` library, with extra tools to keep translations accurate:\n",
        "\n",
        "* **Overrides (`overrides_expanded.json`)** â€“ custom manual translations that replace Googleâ€™s output for specific words (e.g., â€œÙ„ÙŠÙ…ÙˆÙ† Ø£Ø³ÙˆØ¯â€ â†’ â€œblack limeâ€).\n",
        "* **Cache (`translation_cache.csv`)** â€“ stores previous translations to keep results consistent and faster.\n",
        "* **Post-fixes** â€“ fixes common translation mistakes automatically (e.g., â€œblack lemonâ€ â†’ â€œblack limeâ€).\n",
        "* **Classification handling** â€“ splits multi-part text like â€œØ±Ø² | Ø¯Ø¬Ø§Ø¬â€ and translates each piece separately (â€œrice | chickenâ€).\n",
        "\n",
        "**Why we did this:**\n",
        "Mixing two languages made the data inconsistent. Translating everything to English first ensures that the later cleaning and ingredient normalization steps work correctly and uniformly.\n",
        "\n",
        "**Output:**\n",
        " `SaudiFoodFile_english_FIXED.csv` â€” fully English, consistent version of the dataset\n"
      ],
      "metadata": {
        "id": "ZN-CFHSAQroS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OVERRIDES_JSON = \"overrides_expanded.json\""
      ],
      "metadata": {
        "id": "NVYcEN5yQtDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Translator (Deep) with Expanded Overrides + Cache Purge + Debug\n",
        "# ============================================\n",
        "\n",
        "!pip install -q pandas deep-translator\n",
        "\n",
        "import os, re, json, pandas as pd\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "INPUT_CSV  = \"SaudiFoodFile.csv\"\n",
        "OUTPUT_CSV = \"SaudiFoodFile_english_FIXED.csv\"\n",
        "CACHE_CSV  = \"translation_cache.csv\"\n",
        "\n",
        "# Prefer expanded overrides if present\n",
        "OVR_EXP   = \"overrides_expanded.json\"\n",
        "OVR_BASE  = \"overrides.json\"\n",
        "OVERRIDES_JSON = OVR_EXP if os.path.exists(OVR_EXP) else OVR_BASE\n",
        "\n",
        "HANDLE_CLASSIFICATIONS = True  # split 'classifications' by '|'\n",
        "TRANSLATE_COLS = None          # None -> all object columns\n",
        "\n",
        "# ---------- helpers ----------\n",
        "AR_DIAC = re.compile(r\"[\\u0610-\\u061A\\u064B-\\u065F\\u06D6-\\u06ED]\")\n",
        "def norm_ar(s: str) -> str:\n",
        "    s = AR_DIAC.sub(\"\", s)\n",
        "    s = s.replace(\"\\u0640\",\"\")\n",
        "    s = s.replace(\"Ø£\",\"Ø§\").replace(\"Ø¥\",\"Ø§\").replace(\"Ø¢\",\"Ø§\")\n",
        "    s = s.replace(\"Ù‰\",\"ÙŠ\").replace(\"Ø¦\",\"ÙŠ\").replace(\"Ø¤\",\"Ùˆ\").replace(\"Ù±\",\"Ø§\")\n",
        "    return s\n",
        "\n",
        "def key_norm(x: str) -> str:\n",
        "    return norm_ar(str(x).strip().lower())\n",
        "\n",
        "POST_FIX = {\n",
        "    \"black lemon\": \"black lime\",\n",
        "    \"nail\": \"cloves\",\n",
        "    \"cardamon\": \"cardamom\",\n",
        "    \"yougurt\": \"yogurt\",\n",
        "    \"youghurt\": \"yogurt\",\n",
        "}\n",
        "\n",
        "def apply_postfix(en: str) -> str:\n",
        "    return POST_FIX.get(str(en).strip().lower(), str(en).strip())\n",
        "\n",
        "# ---------- load data ----------\n",
        "# CSV\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
        "except UnicodeDecodeError:\n",
        "    df = pd.read_csv(INPUT_CSV, encoding=\"cp1256\")\n",
        "\n",
        "# Overrides\n",
        "if os.path.exists(OVERRIDES_JSON):\n",
        "    with open(OVERRIDES_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "        OV = json.load(f)\n",
        "else:\n",
        "    OV = {}\n",
        "\n",
        "# Normalized override view (for Arabic variant matching)\n",
        "OV_NORM = {key_norm(k): v for k, v in OV.items() if re.search(r\"[\\u0600-\\u06FF]\", k)}\n",
        "print(f\"ðŸ”§ Using overrides file: {OVERRIDES_JSON}\")\n",
        "print(f\"   Loaded overrides: {len(OV)} (normalized Arabic keys: {len(OV_NORM)})\")\n",
        "# show a few samples for sanity\n",
        "for i,(k,v) in enumerate(list(OV.items())[:8]):\n",
        "    print(f\"   â€¢ {k}  ->  {v}\")\n",
        "    if i>=7: break\n",
        "\n",
        "# Cache (load then purge entries that now have overrides)\n",
        "if os.path.exists(CACHE_CSV):\n",
        "    cache_df = pd.read_csv(CACHE_CSV)\n",
        "    CACHE = dict(cache_df.values)  # {raw: english}\n",
        "else:\n",
        "    CACHE = {}\n",
        "\n",
        "def override_lookup(text: str):\n",
        "    if text in OV:\n",
        "        return OV[text]\n",
        "    kn = key_norm(text)\n",
        "    if kn in OV_NORM:\n",
        "        return OV_NORM[kn]\n",
        "    return None\n",
        "\n",
        "# Purge cache entries that should now be overridden\n",
        "purged = 0\n",
        "to_del = []\n",
        "for raw in list(CACHE.keys()):\n",
        "    if override_lookup(raw):\n",
        "        to_del.append(raw)\n",
        "for raw in to_del:\n",
        "    CACHE.pop(raw, None)\n",
        "    purged += 1\n",
        "print(f\"ðŸ§¹ Purged {purged} cache entries that now have overrides\")\n",
        "\n",
        "translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
        "\n",
        "def translate_text(text: str) -> str:\n",
        "    if pd.isna(text) or str(text).strip() == \"\":\n",
        "        return text\n",
        "    s = str(text).strip()\n",
        "\n",
        "    # 1) override wins (exact or normalized)\n",
        "    ov = override_lookup(s)\n",
        "    if ov:\n",
        "        return ov\n",
        "\n",
        "    # 2) cache\n",
        "    if s in CACHE:\n",
        "        return CACHE[s]\n",
        "\n",
        "    # 3) machine translation\n",
        "    try:\n",
        "        en = translator.translate(s) or s\n",
        "        en = apply_postfix(en)\n",
        "    except Exception:\n",
        "        en = s  # keep original on error\n",
        "\n",
        "    CACHE[s] = en\n",
        "    return en\n",
        "\n",
        "def translate_classifications_cell(cell: str) -> str:\n",
        "    parts = [p.strip() for p in str(cell).split(\"|\")]\n",
        "    out = []\n",
        "    for p in parts:\n",
        "        if not p:\n",
        "            continue\n",
        "        ov = override_lookup(p)\n",
        "        en = ov if ov else translate_text(p)\n",
        "        out.append(str(en).lower())\n",
        "    return \" | \".join(out)\n",
        "\n",
        "# ---------- choose columns ----------\n",
        "obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
        "cols = obj_cols if TRANSLATE_COLS is None else [c for c in TRANSLATE_COLS if c in df.columns]\n",
        "print(f\"ðŸ“ Translating columns: {cols}\")\n",
        "\n",
        "# ---------- translate ----------\n",
        "for col in cols:\n",
        "    print(f\"âž¡ï¸  Translating: {col}\")\n",
        "    if HANDLE_CLASSIFICATIONS and col.lower() == \"classifications\":\n",
        "        df[col] = df[col].astype(str).apply(translate_classifications_cell)\n",
        "    else:\n",
        "        df[col] = df[col].apply(translate_text)\n",
        "\n",
        "# ---------- save ----------\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "pd.DataFrame(list(CACHE.items()), columns=[\"raw\",\"english\"]).to_csv(CACHE_CSV, index=False)\n",
        "\n",
        "print(f\"âœ… Done: {OUTPUT_CSV}\")\n",
        "print(f\"ðŸ’¾ Cache: {CACHE_CSV}\")\n",
        "print(f\"âœï¸ Overrides file in use: {OVERRIDES_JSON}\")"
      ],
      "metadata": {
        "id": "utbW2izPUx3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2- Data Cleaning**\n",
        "\n",
        "#**Step 1: Merge Image File names and Assign Unique IDs**\n",
        "\n",
        "**what is done**\n",
        "\n",
        "\n",
        "Weâ€™re combining the Arabic and English datasets so that each English row keeps its original image file name and a new unique ID.\n",
        "\n",
        "**Why:**\n",
        "\n",
        "The Arabic file has the real image filenames (used later for matching or renaming images).\n",
        "\n",
        "The English file has the cleaned and translated dish information.\n",
        "\n",
        "Merging them lets us link each translated dish to its actual image and give every dish a clear numeric ID for easy reference in preprocessing and modeling.\n",
        "\n",
        "**steps:**\n",
        "\n",
        "Read both CSVs using pandas.\n",
        "\n",
        "Find the correct â€œimage fileâ€ column from the Arabic dataset.\n",
        "\n",
        "Add that column into the English dataset as original names.\n",
        "\n",
        "Create a new column image_id that numbers each row sequentially.\n",
        "\n",
        "Save the final combined dataset as a new CSV for later steps.\n",
        "\n",
        "\n",
        "**Input & Output for Step 1**\n",
        "\n",
        "\n",
        "\n",
        "/content/SaudiFoodFile.csv â†’ (Arabic dataset with original image filenames)\n",
        "\n",
        "/content/SaudiFoodFile_english_FIXED.csv â†’ (English translated dataset)\n",
        "\n",
        "Output file:\n",
        "\n",
        "/content/SaudiFoodFile_english_WITH_original_names_and_ID.csv â†’ (English dataset with added original names and image_id columns)"
      ],
      "metadata": {
        "id": "yVtixQM_kFow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Extract 'image file' -> add as 'original names' -> add image_id ===\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load both files (put both CSVs in /content first)\n",
        "ar_path = \"/content/SaudiFoodFile.csv\"\n",
        "en_path = \"/content/SaudiFoodFile_english_FIXED.csv\"\n",
        "\n",
        "df_ar = pd.read_csv(ar_path, encoding=\"utf-8-sig\")\n",
        "df_en = pd.read_csv(en_path, encoding=\"utf-8-sig\")\n",
        "\n",
        "# 2) Find the 'image file' column in the Arabic CSV (robust to spaces/underscores/case)\n",
        "def find_col(cols, target=\"image file\"):\n",
        "    t = target.lower().replace(\"_\", \" \").strip()\n",
        "    for c in cols:\n",
        "        if c.lower().replace(\"_\", \" \").strip() == t:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "image_col = find_col(df_ar.columns, \"image file\")\n",
        "if image_col is None:\n",
        "    raise KeyError(\n",
        "        \"Could not find a column named 'image file' in SaudiFoodFile.csv. \"\n",
        "        f\"Available columns: {list(df_ar.columns)}\"\n",
        "    )\n",
        "\n",
        "# 3) Basic sanity check (same length)\n",
        "if len(df_ar) != len(df_en):\n",
        "    raise ValueError(f\"Row count mismatch! Arabic={len(df_ar)}, English={len(df_en)}\")\n",
        "\n",
        "# 4) Add as 'original names'\n",
        "df_en[\"original names\"] = df_ar[image_col]\n",
        "\n",
        "# 5) Add an ID for each 'original names' row (1..N)\n",
        "df_en[\"image_id\"] = range(1, len(df_en) + 1)\n",
        "\n",
        "# 6) Save\n",
        "out_path = \"/content/SaudiFoodFile_english_WITH_original_names_and_ID.csv\"\n",
        "df_en.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"âœ… Done! Saved to:\", out_path)\n",
        "df_en.head()\n"
      ],
      "metadata": {
        "id": "ZW3Um14PUziv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Remove Duplicate Rows by 'original names'**\n",
        "\n",
        "\n",
        "**what is done**\n",
        "\n",
        "\n",
        "Weâ€™re cleaning the dataset by removing any duplicate rows that have the same **original image name**.\n",
        "\n",
        "**Why:**\n",
        "\n",
        "Some images might appear more than once (for example, the same dish uploaded multiple times). Duplicates can **distort analysis and model training**, so we keep only the first occurrence of each unique image.\n",
        "\n",
        "**steps:**\n",
        "\n",
        "1. Load the previous cleaned CSV file.\n",
        "2. Detect the **\"original names\"** column (even if spacing or capitalization differs).\n",
        "3. Use `drop_duplicates()` to remove repeated image entries.\n",
        "4. Save the new deduplicated file as **`imagesID_no_duplicates.csv`**.\n",
        "\n",
        "\n",
        "Input & Output for This Step\n",
        "\n",
        "Input file:\n",
        "/content/SaudiFoodFile_english_WITH_original_names_and_ID.csv\n",
        "(the file created in Step 1)\n",
        "\n",
        "Output file:\n",
        "/content/imagesID_no_duplicates.csv\n",
        "(same data, but with duplicate image names removed)\n"
      ],
      "metadata": {
        "id": "0j0qJZD3vtEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Remove Duplicate Rows by 'original names' ===\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load your CSV\n",
        "df = pd.read_csv(\"/content/SaudiFoodFile_english_WITH_original_names_and_ID.csv\", encoding=\"utf-8-sig\")\n",
        "\n",
        "# 2) Identify the correct column name (handles variations)\n",
        "cols = [c.lower().replace(\"_\", \" \").strip() for c in df.columns]\n",
        "if \"original names\" in cols:\n",
        "    col_name = df.columns[cols.index(\"original names\")]\n",
        "else:\n",
        "    raise KeyError(f\"âŒ Column 'original names' not found. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# 3) Drop duplicate rows based on that column (keep first)\n",
        "df_clean = df.drop_duplicates(subset=[col_name], keep=\"first\")\n",
        "\n",
        "# 4) Save cleaned file\n",
        "output_path = \"/content/imagesID_no_duplicates.csv\"\n",
        "df_clean.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"âœ… Duplicates removed successfully!\")\n",
        "print(f\"Original rows: {len(df)}\")\n",
        "print(f\"Cleaned rows:  {len(df_clean)}\")\n",
        "print(f\"ðŸ“ Saved as: {output_path}\")\n"
      ],
      "metadata": {
        "id": "EXhzGifkU1Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Rename ZIP Images Using CSV Mapping (Arabic-safe)**\n",
        "\n",
        "**what is done**\n",
        "\n",
        "\n",
        "Rename images inside a ZIP to the format img(ID).ext using the CSVâ€™s original names â†’ image_id mapping.\n",
        "\n",
        "**Why:**\n",
        "\n",
        "\n",
        "Ensures consistent, model-friendly filenames and reliable linkage between each dish row and its image.\n",
        "\n",
        "\n",
        "**steps:**\n",
        "\n",
        "(Optional) Upload CSV and ZIP via Colab widgets (or use fixed paths).\n",
        "\n",
        "Read CSV and build a robust, Unicode-safe lookup from original names â†’ image_id.\n",
        "\n",
        "Iterate ZIP files; for each matched filename, write it out as img(ID).ext to the output folder.\n",
        "\n",
        "Zip the results and write a rename report (renamed vs. skipped).\n",
        "\n",
        "\n",
        "**Inputs:**\n",
        "\n",
        "CSV: /content/imagesID_no_duplicates.csv (must contain original names and image_id)\n",
        "\n",
        "ZIP: /content/images.zip (original images)\n",
        "\n",
        "\n",
        "**Outputs:**\n",
        "\n",
        "Folder: /content/renamed_images/ (renamed files as img(ID).ext)\n",
        "\n",
        "ZIP: /content/renamed_images.zip (packaged renamed images)\n",
        "\n",
        "Report CSV: /content/rename_report.csv (status for each file)"
      ],
      "metadata": {
        "id": "uxdctCB3v5Q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Rename images from ZIP using CSV (Arabic-safe)\n",
        "# ===========================\n",
        "\n",
        "# --- SETTINGS ---\n",
        "USE_UPLOAD_WIDGETS = False  # True â†’ show upload widgets for CSV + ZIP. False â†’ use paths below.\n",
        "\n",
        "CSV_PATH  = \"/content/imagesID_no_duplicates.csv\"  # used when USE_UPLOAD_WIDGETS=False\n",
        "ZIP_PATH  = \"/content/images.zip\"                  # used when USE_UPLOAD_WIDGETS=False\n",
        "OUT_DIR   = \"/content/renamed_images\"              # output folder\n",
        "OUT_ZIP   = \"/content/renamed_images.zip\"          # zipped output\n",
        "NAME_PREF = \"img\"                                  # final name format: img(ID).ext\n",
        "\n",
        "import os, csv, zipfile, shutil, unicodedata, io\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# ---- (A) Upload files interactively in Colab (optional) ----\n",
        "if USE_UPLOAD_WIDGETS:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"ðŸ“¤ Please upload your CSV (with 'original names' + 'image_id'):\")\n",
        "        up1 = files.upload()\n",
        "        CSV_PATH = \"/content/\" + list(up1.keys())[0]\n",
        "        print(\"ðŸ“¤ Please upload your IMAGES ZIP:\")\n",
        "        up2 = files.upload()\n",
        "        ZIP_PATH = \"/content/\" + list(up2.keys())[0]\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Colab file upload failed. Set USE_UPLOAD_WIDGETS=False to use paths.\") from e\n",
        "\n",
        "print(\"CSV_PATH:\", CSV_PATH)\n",
        "print(\"ZIP_PATH:\", ZIP_PATH)\n",
        "\n",
        "# ---- Helpers ----\n",
        "def norm_header(s: str) -> str:\n",
        "    return s.lower().replace(\"_\", \" \").strip()\n",
        "\n",
        "def norm_key(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize a filename for robust matching:\n",
        "    - keep only basename (drop any 'images/...').\n",
        "    - NFC Unicode normalization for Arabic.\n",
        "    - casefold() for case-insensitive match.\n",
        "    - strip spaces.\n",
        "    \"\"\"\n",
        "    base = Path(str(name)).name\n",
        "    return unicodedata.normalize(\"NFC\", base).casefold().strip()\n",
        "\n",
        "# ---- 1) Load mapping CSV (Arabic-safe) ----\n",
        "df = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\")\n",
        "col_map = {norm_header(c): c for c in df.columns}\n",
        "\n",
        "# find columns\n",
        "if \"original names\" not in col_map:\n",
        "    # try common variants\n",
        "    for alt in [\"original name\", \"image file\", \"image\", \"images\", \"original_images\"]:\n",
        "        if alt in col_map:\n",
        "            col_map[\"original names\"] = col_map[alt]\n",
        "            break\n",
        "\n",
        "id_col_key = \"image id\" if \"image id\" in col_map else (\"image_id\" if \"image_id\" in col_map else None)\n",
        "if \"original names\" not in col_map or id_col_key is None:\n",
        "    raise KeyError(f\"CSV must have 'original names' and 'image_id' columns. Found: {list(df.columns)}\")\n",
        "\n",
        "orig_col = col_map[\"original names\"]\n",
        "id_col   = col_map[id_col_key]\n",
        "\n",
        "# Build lookup: normalized basename â†’ image_id (int)\n",
        "lookup = {}\n",
        "dups = df[orig_col].duplicated(keep=False)\n",
        "if dups.any():\n",
        "    print(\"âš ï¸ CSV contains duplicate values in 'original names'. First occurrence will be used.\")\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    key = norm_key(row[orig_col])\n",
        "    # keep first occurrence\n",
        "    if key not in lookup:\n",
        "        lookup[key] = int(row[id_col])\n",
        "\n",
        "print(f\"âœ… Mapping loaded: {len(lookup)} names â†’ IDs\")\n",
        "\n",
        "# ---- 2) Prepare output dir ----\n",
        "shutil.rmtree(OUT_DIR, ignore_errors=True)\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- 3) Read ZIP and rename files ----\n",
        "renamed, skipped = [], []\n",
        "\n",
        "def write_member_to(path_out, zin, zinfo):\n",
        "    with zin.open(zinfo) as src, open(path_out, \"wb\") as dst:\n",
        "        dst.write(src.read())\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"r\") as zin:\n",
        "    for zinfo in zin.infolist():\n",
        "        if zinfo.is_dir():\n",
        "            continue\n",
        "\n",
        "        # Get the basename (ignore any folders inside the zip)\n",
        "        orig_zip_path = zinfo.filename\n",
        "        base = Path(orig_zip_path).name\n",
        "        key  = norm_key(base)\n",
        "\n",
        "        if key in lookup:\n",
        "            img_id = lookup[key]\n",
        "            ext = Path(base).suffix  # keep original extension\n",
        "            new_name = f\"{NAME_PREF}({img_id}){ext}\"\n",
        "            out_path = os.path.join(OUT_DIR, new_name)\n",
        "            write_member_to(out_path, zin, zinfo)\n",
        "            renamed.append((orig_zip_path, new_name, img_id))\n",
        "        else:\n",
        "            skipped.append((orig_zip_path, \"no match for 'original names'\"))\n",
        "\n",
        "# ---- 4) Zip the output folder ----\n",
        "shutil.make_archive(OUT_ZIP.replace(\".zip\", \"\"), \"zip\", OUT_DIR)\n",
        "\n",
        "# ---- 5) Report CSV ----\n",
        "report_path = \"/content/rename_report.csv\"\n",
        "with open(report_path, \"w\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"status\", \"zip_original_path\", \"new_name_or_reason\", \"image_id\"])\n",
        "    for o, n, i in renamed:\n",
        "        w.writerow([\"renamed\", o, n, i])\n",
        "    for o, reason in skipped:\n",
        "        w.writerow([\"skipped\", o, reason, \"\"])\n",
        "\n",
        "print(\"ðŸŽ‰ Done!\")\n",
        "print(f\"â€¢ Renamed: {len(renamed)}\")\n",
        "print(f\"â€¢ Skipped (no match): {len(skipped)}\")\n",
        "print(f\"ðŸ“‚ Output folder: {OUT_DIR}\")\n",
        "print(f\"ðŸ—œï¸ Output zip:    {OUT_ZIP}\")\n",
        "print(f\"ðŸ§¾ Report CSV:    {report_path}\")"
      ],
      "metadata": {
        "id": "pfDxDk4-U5Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Update Image Filenames Inside the CSV**\n",
        "\n",
        "**what is done**\n",
        "\n",
        "\n",
        "Rename the image file entries inside the CSV so they match the renamed image format img(ID).ext.\n",
        "\n",
        "\n",
        "**Why:**\n",
        "To keep the dataset consistent with the renamed images in your folder/ZIP â€” ensuring every rowâ€™s filename matches its actual image file.\n",
        "\n",
        "\n",
        "**steps:**\n",
        "\n",
        "Load the latest CSV containing image_id.\n",
        "\n",
        "Automatically find the column that holds image filenames.\n",
        "\n",
        "Replace each filename with the new standardized format img(ID).ext while keeping the same extension.\n",
        "\n",
        "Save the updated CSV for use in later steps.\n",
        "\n",
        "\n",
        "\n",
        "**Input file:**\n",
        "/content/imagesID_no_duplicates.csv (before renaming inside CSV)\n",
        "\n",
        "**Output file:**\n",
        "/content/imagesID_renamed_in_csv.csv (filenames now follow the img(ID).ext format)"
      ],
      "metadata": {
        "id": "0H7g0ZZSNKF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Rename filenames in CSV to match img(ID).ext format ===\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Load your CSV (make sure it contains 'image_id' and the column with the image file paths)\n",
        "CSV_PATH = \"/content/imagesID_no_duplicates.csv\"  # or your latest version\n",
        "df = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\")\n",
        "\n",
        "# 2) Identify the image path column (commonly 'image_file' or 'images')\n",
        "def find_col(cols, target=\"image\"):\n",
        "    t = target.lower()\n",
        "    for c in cols:\n",
        "        if t in c.lower():\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "img_col = find_col(df.columns, \"image\")\n",
        "if not img_col:\n",
        "    raise KeyError(f\"No column found containing 'image'. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# 3) Rename each image according to its ID, keeping the same file extension\n",
        "def make_new_name(old_path, image_id):\n",
        "    ext = Path(str(old_path)).suffix or \".jpg\"  # default to .jpg if missing\n",
        "    return f\"img({image_id}){ext}\"\n",
        "\n",
        "df[img_col] = [make_new_name(df.loc[i, img_col], df.loc[i, \"image_id\"]) for i in range(len(df))]\n",
        "\n",
        "# 4) Save the updated CSV\n",
        "OUTPUT_PATH = \"/content/imagesID_renamed_in_csv.csv\"\n",
        "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"âœ… All image filenames updated inside the CSV.\")\n",
        "print(f\"ðŸ“ Saved as: {OUTPUT_PATH}\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "v8Aean4WU63i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 5: Remove the â€œoriginal namesâ€ Column**\n",
        "\n",
        "\n",
        "Delete the **`original names`** column from the dataset.\n",
        "\n",
        "**Why:**\n",
        "After all images have been renamed and matched using IDs, the original filenames are no longer needed.\n",
        "Removing this column keeps the dataset **clean and ready for modeling or analysis**.\n",
        "\n",
        "**steps**\n",
        "\n",
        "1. Load the latest CSV file.\n",
        "2. Find and drop the **`original names`** column (handles different spellings or spacing).\n",
        "3. Save the final cleaned version.\n",
        "\n",
        "\n",
        "\n",
        "**Input file:**\n",
        "`/content/imagesID_renamed_in_csv.csv`\n",
        "\n",
        "**Output file:**\n",
        "`/content/remove_original_names.csv` *(final cleaned dataset without the old image name column)*\n"
      ],
      "metadata": {
        "id": "Sr7P_4-UNyQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Remove \"original names\" column ===\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load your CSV file\n",
        "df = pd.read_csv(\"/content/imagesID_renamed_in_csv.csv\", encoding=\"utf-8-sig\")\n",
        "\n",
        "# 2) Remove the column safely (handles naming variations)\n",
        "cols = [c.lower().replace(\"_\", \" \").strip() for c in df.columns]\n",
        "if \"original names\" in cols:\n",
        "    col_name = df.columns[cols.index(\"original names\")]\n",
        "    df = df.drop(columns=[col_name])\n",
        "else:\n",
        "    raise KeyError(f\"Column 'original names' not found. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# 3) Save the new file\n",
        "output_path = \"/content/remove_original_names.csv\"\n",
        "df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"âœ… 'original names' column removed.\")\n",
        "print(f\"ðŸ“ Clean file saved as: {output_path}\")\n"
      ],
      "metadata": {
        "id": "DK-tKIvrU8ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 6: Clean and Standardize Dish Names**\n",
        "\n",
        "**what is done**\n",
        "\n",
        "This step cleans and unifies all dish names â€” keeping authentic Arabic and Middle Eastern dishes intact while removing unnecessary English words, event mentions, and quantity descriptions.\n",
        "\n",
        "**Why:**\n",
        "To make the dataset consistent and ready for analysis by:\n",
        "\n",
        "Removing irrelevant phrases like â€œHow to makeâ€ or â€œfor Saudi National Dayâ€.\n",
        "\n",
        "Avoiding over-cleaning that might erase Arabic or culturally important dish names.\n",
        "\n",
        "Standardizing different spellings (e.g., kbsa, kabsah â†’ Kabsa) into one consistent form.\n",
        "\n",
        "**steps:**\n",
        "\n",
        "Load the latest dataset (remove_original_names.csv).\n",
        "\n",
        "Use clean_dish_name() to strip only specific English words, numbers, and phrases â€” preserving Arabic names.\n",
        "\n",
        "Use standardize_dish_name() to unify variations and detect dish types and proteins (e.g., Kabsa Chicken).\n",
        "\n",
        "Display before/after examples of cleaned names.\n",
        "\n",
        "Save the cleaned and standardized dataset as SaudiFoodFile_cc.csv.\n",
        "\n",
        "**Input file:**\n",
        "remove_original_names.csv\n",
        "\n",
        "**Output file:**\n",
        "SaudiFoodFile_cleaned_dishName.csv (final dataset with standardized and culturally accurate dish names)"
      ],
      "metadata": {
        "id": "4-kFHadxOP6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('remove_original_names.csv')\n",
        "\n",
        "# Display initial data info\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Task 1: Clean dish names - remove descriptions, quantities, and non-ingredient information\n",
        "def clean_dish_name(name):\n",
        "    \"\"\"\n",
        "    Remove descriptions, quantities, and non-ingredient information from dish names\n",
        "    while preserving authentic Arabic/Middle Eastern dish names\n",
        "    \"\"\"\n",
        "    # Common patterns to remove (ONLY specific English descriptions)\n",
        "    patterns_to_remove = [\n",
        "        # Occasions and events (English)\n",
        "        r'\\bfor saudi national day\\b',\n",
        "        r'\\bfor hosting\\b',\n",
        "        r'\\bsummer offer\\b',\n",
        "\n",
        "        # Cooking methods and styles (English)\n",
        "        r'\\bhow to make\\b',\n",
        "        r'\\bhow to boil\\b',\n",
        "        r'\\bsaudi style\\b',\n",
        "        r'\\bhijazi style\\b',\n",
        "        r'\\btraditional\\b',\n",
        "        r'\\bauthentic\\b',\n",
        "        r'\\beasy\\b',\n",
        "        r'\\bcopycat recipe\\b',\n",
        "        r'\\bslow-?roast\\b',\n",
        "        r'\\bno bake\\b',\n",
        "        r'\\brussian style\\b',\n",
        "        r'\\brussian\\b',\n",
        "\n",
        "        # Remove ALL parentheses and their content\n",
        "        r'\\([^)]*\\)',\n",
        "\n",
        "        # Very specific quantity descriptions only\n",
        "        r'\\bwhole grain\\b',\n",
        "        r'\\bhalf a piece\\b',\n",
        "        r'\\bhalf piece\\b',\n",
        "        r'\\bone person\\b',\n",
        "\n",
        "        # General English descriptions (be very specific)\n",
        "        r'\\bmethod for\\b',\n",
        "        r'\\baccording to\\b',\n",
        "        r'\\bway\\b',\n",
        "        r'\\brecipe\\b',\n",
        "    ]\n",
        "\n",
        "    cleaned_name = name.strip()\n",
        "\n",
        "    # Remove only very specific patterns\n",
        "    for pattern in patterns_to_remove:\n",
        "        cleaned_name = re.sub(pattern, '', cleaned_name, flags=re.IGNORECASE)\n",
        "\n",
        "    # Clean up extra spaces and punctuation carefully\n",
        "    cleaned_name = re.sub(r'\\s+', ' ', cleaned_name)\n",
        "    cleaned_name = cleaned_name.strip()\n",
        "\n",
        "    return cleaned_name\n",
        "\n",
        "# Task 2: Standardize dish name variations to consistent naming\n",
        "def standardize_dish_name(name):\n",
        "    \"\"\"\n",
        "    Standardize variations of dish names to consistent naming conventions\n",
        "    while preserving the main dish identity\n",
        "    \"\"\"\n",
        "    if not name or name.strip() == '':\n",
        "        return 'Unclassified Dish'\n",
        "\n",
        "    # First, extract the main dish components\n",
        "    name_lower = name.lower()\n",
        "\n",
        "    # Identify the main dish type\n",
        "    main_dish = None\n",
        "    protein = None\n",
        "\n",
        "    # Check for main dishes (preserve Arabic names)\n",
        "    dish_patterns = {\n",
        "        'Kabsa': r'\\bkabsa\\b|\\bkabsah\\b|\\bkbsa\\b',\n",
        "        'Mandi': r'\\bmandi\\b',\n",
        "        'Madhbi': r'\\bmadhbi\\b',\n",
        "        'Madfoon': r'\\bmadfoon\\b',\n",
        "        'Madghut': r'\\bmadghog\\b|\\bmadjoun\\b|\\bmadjoon\\b',\n",
        "        'Shakshuka': r'\\bshakshuka\\b|\\bshaksuka\\b',\n",
        "        'Jareesh': r'\\bjareesh\\b|\\bjarish\\b|\\bgroats\\b',\n",
        "        'Maqluba': r'\\bmaqluba\\b|\\bmakloubeh\\b|\\bmagloba\\b',\n",
        "        'Kleja': r'\\bkleija\\b|\\bkleja\\b|\\bklija\\b',\n",
        "        'Maamoul': r'\\bmaamoul\\b|\\bmamoul\\b',\n",
        "        'Mutabak': r'\\bmutabbaq\\b|\\bmutabak\\b',\n",
        "        'Sambusa': r'\\bsambosa\\b|\\bsambousek\\b|\\bsamosa\\b',\n",
        "        'Basbousa': r'\\bbasbousa\\b|\\bbasbosa\\b',\n",
        "        'Kunafa': r'\\bkunafa\\b|\\bknafeh\\b',\n",
        "        'Mulukhiyah': r'\\bmulukhiyah\\b|\\bmolokhia\\b|\\bmolokhiya\\b',\n",
        "        'Saleek': r'\\bsaleek\\b|\\bsaleeq\\b|\\bsuliq\\b|\\bsulait\\b',\n",
        "        'Freekeh': r'\\bfreekeh\\b|\\bfreekey\\b',\n",
        "        'Mujaddara': r'\\bmujadara\\b|\\bmujaddara\\b',\n",
        "        'Luqaimat': r'\\bluqaimat\\b|\\bluqaymat\\b',\n",
        "        'Harees': r'\\bharees\\b|\\bhareeseh\\b',\n",
        "        'Thareed': r'\\bthareed\\b|\\btharid\\b',\n",
        "    }\n",
        "\n",
        "    for dish, pattern in dish_patterns.items():\n",
        "        if re.search(pattern, name_lower):\n",
        "            main_dish = dish\n",
        "            break\n",
        "\n",
        "    # If no specific dish found, try to identify by main components\n",
        "    if not main_dish:\n",
        "        # Check for proteins\n",
        "        if re.search(r'\\bchicken\\b', name_lower):\n",
        "            protein = 'Chicken'\n",
        "        elif re.search(r'\\blamb\\b', name_lower):\n",
        "            protein = 'Lamb'\n",
        "        elif re.search(r'\\bmeat\\b|\\bbeef\\b', name_lower):\n",
        "            protein = 'Meat'\n",
        "        elif re.search(r'\\bfish\\b', name_lower):\n",
        "            protein = 'Fish'\n",
        "        elif re.search(r'\\bshrimp\\b', name_lower):\n",
        "            protein = 'Shrimp'\n",
        "        elif re.search(r'\\bcamel\\b', name_lower):\n",
        "            protein = 'Camel'\n",
        "\n",
        "    # Build standardized name\n",
        "    if main_dish:\n",
        "        if protein:\n",
        "            standardized_name = f\"{main_dish} {protein}\"\n",
        "        else:\n",
        "            standardized_name = main_dish\n",
        "    elif protein:\n",
        "        # If we only have protein but no specific dish, use the cleaned name\n",
        "        standardized_name = name.title()\n",
        "    else:\n",
        "        # For names without clear dish type, clean but preserve the name\n",
        "        standardized_name = name.title()\n",
        "\n",
        "        # Apply gentle standardization for common variations\n",
        "        variations = {\n",
        "            r'\\bkabsah\\b': 'Kabsa',\n",
        "            r'\\bkbsa\\b': 'Kabsa',\n",
        "            r'\\bmandi\\b': 'Mandi',\n",
        "            r'\\bmutabbaq\\b': 'Mutabak',\n",
        "            r'\\bsambosa\\b': 'Sambusa',\n",
        "            r'\\bmaamoul\\b': 'Maamoul',\n",
        "            r'\\bkunafa\\b': 'Kunafa',\n",
        "            r'\\bbasbousa\\b': 'Basbousa',\n",
        "        }\n",
        "\n",
        "        for pattern, replacement in variations.items():\n",
        "            standardized_name = re.sub(pattern, replacement, standardized_name, flags=re.IGNORECASE)\n",
        "\n",
        "    return standardized_name.strip()\n",
        "\n",
        "# Apply cleaning and standardization\n",
        "print(\"\\nApplying data cleaning...\")\n",
        "\n",
        "# Create cleaned dish names\n",
        "df['cleaned_dish_name'] = df['dish_name'].apply(clean_dish_name)\n",
        "df['standardized_dish_name'] = df['cleaned_dish_name'].apply(standardize_dish_name)\n",
        "\n",
        "# Show before and after examples - focus on problem cases\n",
        "print(\"\\nName cleaning examples (focusing on problem cases):\")\n",
        "problem_cases = [\n",
        "    \"A quarter of mandi chicken\",\n",
        "    \"Quarter goat\",\n",
        "    \"Chicken kabsa with rice\",\n",
        "    \"Meat kabsa and daqoos salad\",\n",
        "    \"Saudi meat kabsa and daqoos salad\",\n",
        "    \"How to make Saudi kleija\",\n",
        "    \"Saudi style chicken kabsa\",\n",
        "    \"Russian style borscht soup\",\n",
        "    \"Chicken Kabsa (curry) with rice\",\n",
        "    \"Plain mandi rice\"\n",
        "]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    if any(case.lower() in row['dish_name'].lower() for case in problem_cases):\n",
        "        print(f\"Original: {row['dish_name']}\")\n",
        "        print(f\"Cleaned: {row['cleaned_dish_name']}\")\n",
        "        print(f\"Standardized: {row['standardized_dish_name']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Show most common dish names after standardization\n",
        "print(\"\\nMost common standardized dish names:\")\n",
        "print(df['standardized_dish_name'].value_counts().head(20))\n",
        "\n",
        "# Save the cleaned data\n",
        "df_cleaned = df.copy()\n",
        "# Keep original name and use standardized as main dish name\n",
        "df_cleaned['dish_name_original'] = df['dish_name']\n",
        "df_cleaned['dish_name'] = df['standardized_dish_name']\n",
        "\n",
        "# Drop temporary columns\n",
        "df_cleaned = df_cleaned.drop(['cleaned_dish_name', 'standardized_dish_name'], axis=1)\n",
        "\n",
        "print(f\"\\nFinal data shape: {df_cleaned.shape}\")\n",
        "print(\"\\nFirst few rows of cleaned data:\")\n",
        "print(df_cleaned[['dish_name_original', 'dish_name']].head(20))\n",
        "\n",
        "# Save to new CSV file\n",
        "output_filename = 'SaudiFoodFile_cleaned_dishName.csv'\n",
        "df_cleaned.to_csv(output_filename, index=False)\n",
        "print(f\"\\nCleaned data saved to: {output_filename}\")\n",
        "\n",
        "# Additional analysis: Show name standardization results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"NAME STANDARDIZATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Show specific problem cases and their resolution\n",
        "print(\"\\nSpecific problem cases and their resolution:\")\n",
        "test_cases = [\n",
        "    \"A quarter of mandi chicken\",\n",
        "    \"Quarter goat\",\n",
        "    \"Chicken kabsa with rice\",\n",
        "    \"Saudi meat kabsa and daqoos salad\",\n",
        "    \"How to make Saudi kleija\",\n",
        "    \"Saudi style chicken kabsa\",\n",
        "    \"Plain mandi rice\",\n",
        "    \"Half grilled chicken with rice\",\n",
        "    \"Russian style borscht soup\",\n",
        "    \"Chicken Kabsa (curry) with rice\",\n",
        "    \"Mandi Chicken (whole grain)\",\n",
        "    \"Grilled chicken (half a piece)\"\n",
        "]\n",
        "\n",
        "for case in test_cases:\n",
        "    cleaned = clean_dish_name(case)\n",
        "    standardized = standardize_dish_name(cleaned)\n",
        "    print(f\"Original: '{case}'\")\n",
        "    print(f\"Cleaned: '{cleaned}'\")\n",
        "    print(f\"Standardized: '{standardized}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Show reduction in unique names\n",
        "original_unique = df['dish_name'].nunique()\n",
        "cleaned_unique = df_cleaned['dish_name'].nunique()\n",
        "print(f\"\\nUnique name reduction: {original_unique} â†’ {cleaned_unique} ({(1-cleaned_unique/original_unique)*100:.1f}% reduction)\")\n",
        "\n",
        "print(\"\\nCleaning complete! Arabic dish names are preserved while English descriptions are removed.\")"
      ],
      "metadata": {
        "id": "Js3mRRk0U-ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 7: Clean and Standardize Ingredients**\n",
        "\n",
        "\n",
        "This step cleans and standardizes all ingredient information before the consistency check.\n",
        "Some dishes originally had messy or incomplete ingredient lists â€” for example:\n",
        "\n",
        "> `\"olive oil / tomatoes - onion, chilli\"` or sometimes just `\"unknown\"`\n",
        "\n",
        "The goal here is to convert everything into clear, structured ingredient lists such as:\n",
        "\n",
        "> `[\"olive oil\", \"tomato\", \"onion\", \"chili\"]`\n",
        "> and to replace unclear or missing entries like `[\"unknown\"]` with the clear label `[\"unknown_ingredients\"]`.\n",
        "\n",
        "**What this step does:**\n",
        "\n",
        "1. **Splits ingredients correctly:**\n",
        "   Separates text using real separators (`|`, `/`, `,`, `;`, Arabic commas, or dashes) without breaking multi-word names.\n",
        "\n",
        "2. **Protects multi-word ingredients:**\n",
        "   Keeps terms like *olive oil* or *tomato paste* together as one ingredient.\n",
        "\n",
        "3. **Removes non-ingredient words:**\n",
        "   Drops extra words such as *add*, *garnish*, *with*, or *hot* that arenâ€™t actual ingredients.\n",
        "\n",
        "4. **Normalizes plurals and spellings:**\n",
        "   Converts plurals (*tomatoes â†’ tomato*) and unifies spellings (*chilli*, *chilies* â†’ *chili*).\n",
        "\n",
        "5. **Handles unknown or missing entries:**\n",
        "   Any ingredient cell that is empty or simply says `\"not found\"` or `\"unknown\"` becomes `[\"unknown\"]` to keep the format consistent.\n",
        "\n",
        "6. **Embedding + Clustering (semantic cleaning):**\n",
        "   group similar ingredient names (like *cardamon* and *cardamom*) into one canonical form.\n",
        "\n",
        "7. **Creates standardized ingredient lists:**\n",
        "   Every dish ends up with a clean list of consistent, machine-readable ingredients.\n",
        "\n",
        "**Outputs:**\n",
        "\n",
        "* `SaudiFoodFile_standardized.csv` â†’ the final cleaned ingredient lists per dish. will be used in the next step.\n",
        "* `ingredient_clusters_report.csv` â†’ groups of similar ingredients and their canonical names (helper)\n",
        "* `ingredient_canonical_map.json` â†’ mapping of each ingredient to its canonical form (helper)\n"
      ],
      "metadata": {
        "id": "AagSB_ruRHf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "#  â€” Ingredient Cleaning\n",
        "# - Split on real separators only (| / \\ , ; ØŒ - with spaces, bullets)\n",
        "# - Protect multi-word ingredient phrases (KEEP_PHRASES)\n",
        "# - Extract ingredients from stray sentences; drop non-ingredient words\n",
        "# - Canonicalize with ALIASES + singularization + fuzzy nudge\n",
        "# - Output only: dish_name | classifications_std_list | image_file | scrape_date\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q pandas sentence-transformers scikit-learn rapidfuzz inflect\n",
        "\n",
        "import re, json, pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from rapidfuzz import process, fuzz\n",
        "import inflect\n",
        "\n",
        "# --- Unknown/placeholder detection ---\n",
        "UNKNOWN_PATTERNS = [\n",
        "    r\"^\\s*not\\s*found\\s*$\",\n",
        "    r\"^\\s*unknown\\s*$\",\n",
        "    r\"^\\s*n/?a\\s*$\",\n",
        "    r\"^\\s*none\\s*$\",\n",
        "    r\"^\\s*null\\s*$\",\n",
        "    r\"^\\s*missing\\s*$\",\n",
        "    r\"^\\s*Ø¨Ø¯ÙˆÙ†\\s*$\",           # Arabic: without\n",
        "    r\"^\\s*ØºÙŠØ±\\s*Ù…ØªÙˆÙØ±\\s*$\",   # Arabic: unavailable\n",
        "]\n",
        "\n",
        "UNKNOWN_REGEXES = [re.compile(p, re.IGNORECASE) for p in UNKNOWN_PATTERNS]\n",
        "\n",
        "def is_unknown_text(s: str) -> bool:\n",
        "    return any(rx.match(s) for rx in UNKNOWN_REGEXES)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "INPUT_CSV  = \"SaudiFoodFile_cleaned_dishName.csv\"\n",
        "OUTPUT_CSV = \"SaudiFoodFile_standardized.csv\"\n",
        "REPORT_CSV = \"ingredient_clusters_report.csv\"\n",
        "MAP_JSON   = \"ingredient_canonical_map.json\"\n",
        "\n",
        "CLASS_COL_CANDIDATES = [\"classifications\", \"classifications_en\", \"classfications\"]\n",
        "MODEL_NAME = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
        "DISTANCE_THRESHOLD = 0.35\n",
        "FUZZY_SCORE_CUTOFF = 92\n",
        "\n",
        "# ----------------------------\n",
        "# Phrase protection & vocabulary\n",
        "# ----------------------------\n",
        "# Multi-word ingredients to KEEP as single tokens\n",
        "KEEP_PHRASES = {\n",
        "    # oils / dairy\n",
        "    \"olive oil\", \"vegetable oil\", \"clarified butter\", \"milk powder\", \"butter milk\", \"heavy cream\",\n",
        "    # stocks / sauces / pastes\n",
        "    \"tomato paste\", \"tomato sauce\", \"soy sauce\", \"pomegranate molasses\", \"date molasses\", \"rose water\", \"orange blossom water\",\n",
        "    # veg & herbs\n",
        "    \"bell pepper\", \"green onion\", \"spring onion\", \"bay leaves\", \"mint leaves\", \"parsley leaves\", \"coriander leaves\",\n",
        "    # spices\n",
        "    \"black lime\", \"mixed spices\", \"spice mix\", \"allspice\", \"black pepper\", \"white pepper\",\n",
        "    # proteins / grains\n",
        "    \"basmati rice\", \"chicken stock\", \"beef stock\", \"vegetable stock\",\n",
        "}\n",
        "\n",
        "# Synonyms/variants â†’ canonical (lowercase)\n",
        "ALIASES = {\n",
        "    # souring / lime\n",
        "    \"dried lime\": \"black lime\", \"dried limes\": \"black lime\", \"omani lime\": \"black lime\",\n",
        "    \"omani limes\": \"black lime\", \"loomi\": \"black lime\", \"black lemon\": \"black lime\",\n",
        "\n",
        "    # oils/fats/dairy\n",
        "    \"veg oil\": \"vegetable oil\", \"olive oils\": \"olive oil\", \"butter milk\": \"buttermilk\",\n",
        "    \"yoghurt\": \"yogurt\", \"labnah\": \"labneh\",\n",
        "\n",
        "    # herbs & veg\n",
        "    \"cilantro\": \"coriander\", \"coriander leaves\": \"coriander\", \"green coriander\": \"coriander\",\n",
        "    \"parsley leaves\": \"parsley\", \"mint leaves\": \"mint\", \"spring onion\": \"green onion\",\n",
        "\n",
        "    # peppers\n",
        "    \"capsicum\": \"bell pepper\", \"green pepper\": \"bell pepper\", \"sweet pepper\": \"bell pepper\",\n",
        "    \"chilli\": \"chili\", \"chilies\": \"chili\", \"chillies\": \"chili\", \"chili pepper\": \"chili\", \"chili peppers\": \"chili\",\n",
        "\n",
        "    # powders & sticks -> base spice\n",
        "    \"turmeric powder\": \"turmeric\", \"ginger powder\": \"ginger\",\n",
        "    \"garlic powder\": \"garlic\", \"onion powder\": \"onion\",\n",
        "    \"cinnamon stick\": \"cinnamon\", \"cinnamon sticks\": \"cinnamon\",\n",
        "    \"cardamon\": \"cardamom\",\n",
        "\n",
        "    # sauces/pastes/syrups\n",
        "    \"tomato purÃ©e\": \"tomato paste\", \"tomato puree\": \"tomato paste\",\n",
        "    \"simple syrup\": \"sugar syrup\",\n",
        "\n",
        "    # pulses/grains\n",
        "    \"garbanzo beans\": \"chickpeas\", \"garbanzo bean\": \"chickpeas\",\n",
        "    \"chick peas\": \"chickpeas\", \"chick pea\": \"chickpeas\",\n",
        "    \"black eyed beans\": \"black-eyed beans\", \"black eyed pea\": \"black-eyed beans\",\n",
        "\n",
        "    # spice mixes / generic\n",
        "    \"spice mix\": \"mixed spices\", \"mix spices\": \"mixed spices\", \"spices mix\": \"mixed spices\"\n",
        "}\n",
        "\n",
        "# Words that are NOT ingredients (filler, verbs, instructions)\n",
        "NON_ING_WORDS = {\n",
        "    \"after\",\"decorate\",\"decoration\",\"decorations\",\"add\",\"with\",\"such\",\"touch\",\"patriotic\",\"cream\",\n",
        "    \"or\",\"and\",\"the\",\"a\",\"an\",\"then\",\"until\",\"when\",\"like\",\"as\",\"to\",\"for\",\"of\",\"into\",\"over\",\n",
        "    \"warm\",\"hot\",\"cold\",\"slice\",\"sliced\",\"diced\",\"chopped\",\"minced\",\"ground\",\"crushed\",\"whole\",\n",
        "    \"fresh\",\"optional\",\"needed\",\"garnish\",\"make\",\"prepare\",\"preparation\",\"cook\",\"cooked\",\"baked\",\n",
        "    \"boiled\",\"fried\",\"seauted\",\"sauteed\",\"browned\",\"mix\",\"mixed\",\"topping\",\"kitchen\",\"precise\",\"instant\"\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "SEP_NORMALIZER = re.compile(r\"[|/\\\\ØŒ;,]+\")      # | / \\ , ; Arabic comma\n",
        "AROUND_HYPHEN  = re.compile(r\"\\s*[-â€“â€”]\\s*\")     # spaced hyphens/dashes as separators\n",
        "BULLETS        = re.compile(r\"[â€¢Â·]+\")\n",
        "\n",
        "# After splitting, strip these inside tokens\n",
        "PUNCT_DROP_INSIDE = re.compile(r\"[-_/\\\\|]+\")\n",
        "NONWORD           = re.compile(r\"[^\\w\\s\\(\\)]\")\n",
        "\n",
        "IRREGULAR = {\"tomatoes\":\"tomato\",\"potatoes\":\"potato\",\"limes\":\"lime\",\"chillies\":\"chili\",\"chilies\":\"chili\",\"cloves\":\"cloves\"}\n",
        "p = inflect.engine()\n",
        "\n",
        "def safe_str(x):\n",
        "    if x is None: return \"\"\n",
        "    s = str(x)\n",
        "    return \"\" if s.strip().lower() in {\"\", \"nan\", \"none\", \"null\"} else s\n",
        "\n",
        "def protect_phrases(text: str) -> str:\n",
        "    \"\"\"Replace spaces inside KEEP_PHRASES with underscores to protect them.\"\"\"\n",
        "    s = text\n",
        "    # longer phrases first to avoid partial overlaps\n",
        "    for ph in sorted(KEEP_PHRASES, key=lambda x: -len(x)):\n",
        "        pattern = r\"\\b\" + re.escape(ph) + r\"\\b\"\n",
        "        s = re.sub(pattern, ph.replace(\" \", \"_\"), s, flags=re.IGNORECASE)\n",
        "    return s\n",
        "\n",
        "def normalize_separators(s: str) -> str:\n",
        "    s = BULLETS.sub(\"|\", s)\n",
        "    s = AROUND_HYPHEN.sub(\"|\", s)           # ' - ' â†’ '|'\n",
        "    s = SEP_NORMALIZER.sub(\"|\", s)          # unify to '|'\n",
        "    s = re.sub(r\"\\|{2,}\", \"|\", s).strip(\"| \")\n",
        "    return s\n",
        "\n",
        "def to_singular(word: str) -> str:\n",
        "    w = word.strip().lower()\n",
        "    if w in IRREGULAR: return IRREGULAR[w]\n",
        "    s = p.singular_noun(w)\n",
        "    return s if isinstance(s, str) and s else w\n",
        "\n",
        "def clean_phrase(s: str) -> str:\n",
        "    s = s.strip().lower()\n",
        "    s = PUNCT_DROP_INSIDE.sub(\" \", s)\n",
        "    s = NONWORD.sub(\" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def drop_non_ingredients(words):\n",
        "    return [w for w in words if w not in NON_ING_WORDS and len(w) > 1]\n",
        "\n",
        "def split_to_tokens(cell) -> list[str]:\n",
        "    \"\"\"\n",
        "    - If the whole cell is a placeholder (not found/unknown/etc.), return []\n",
        "      so it becomes [\"unknown_ingredients\"] later.\n",
        "    - Normalize separators to '|', protect phrases, split on '|'.\n",
        "    - Extract protected phrases, then ingredient-like words only.\n",
        "    \"\"\"\n",
        "    raw = safe_str(cell)\n",
        "    if not raw:\n",
        "        return []\n",
        "    # whole-cell unknowns â†’ empty list (will become [\"unknown_ingredients\"])\n",
        "    if is_unknown_text(raw):\n",
        "        return []\n",
        "\n",
        "    # normalize separators and protect phrases\n",
        "    s = normalize_separators(raw)\n",
        "    s = protect_phrases(s)\n",
        "    parts = [p.strip() for p in s.split(\"|\") if p.strip()]\n",
        "\n",
        "    tokens = []\n",
        "    for p in parts:\n",
        "        # per-part unknowns too\n",
        "        if is_unknown_text(p):\n",
        "            # skip this part entirely\n",
        "            continue\n",
        "\n",
        "        # restore underscores for already isolated protected phrases\n",
        "        if \"_\" in p and p in KEEP_PHRASES or p.replace(\"_\",\" \") in KEEP_PHRASES:\n",
        "            tokens.append(p.replace(\"_\",\" \"))\n",
        "            continue\n",
        "\n",
        "        # 1) collect any protected phrases still inside\n",
        "        found = []\n",
        "        rest  = p\n",
        "        for ph in sorted(KEEP_PHRASES, key=lambda x: -len(x)):\n",
        "            ph_prot = ph.replace(\" \", \"_\")\n",
        "            if ph_prot.lower() in rest.lower():\n",
        "                found.append(ph)\n",
        "                rest = re.sub(re.escape(ph_prot), \" \", rest, flags=re.IGNORECASE)\n",
        "\n",
        "        # 2) remaining words (filtered)\n",
        "        rest = clean_phrase(rest.replace(\"_\",\" \"))\n",
        "        words = drop_non_ingredients(rest.split())\n",
        "        words = [to_singular(w) for w in words]\n",
        "\n",
        "        for ph in found:\n",
        "            tokens.append(ph)\n",
        "        for w in words:\n",
        "            tokens.append(w)\n",
        "\n",
        "    # alias + dedupe\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        t0 = ALIASES.get(t.strip().lower().replace(\"_\",\" \"), t.strip().lower().replace(\"_\",\" \"))\n",
        "        if t0:\n",
        "            out.append(t0)\n",
        "\n",
        "    seen, dedup = set(), []\n",
        "    for x in out:\n",
        "        if x not in seen:\n",
        "            dedup.append(x); seen.add(x)\n",
        "    return dedup\n",
        "\n",
        "    # normalize separators and protect phrases\n",
        "    s = normalize_separators(raw)\n",
        "    s = protect_phrases(s)\n",
        "    parts = [p.strip() for p in s.split(\"|\") if p.strip()]\n",
        "\n",
        "    tokens = []\n",
        "    for p in parts:\n",
        "        # restore underscores for already isolated protected phrases\n",
        "        if \"_\" in p and p in KEEP_PHRASES or p.replace(\"_\",\" \") in KEEP_PHRASES:\n",
        "            tokens.append(p.replace(\"_\",\" \"))\n",
        "            continue\n",
        "\n",
        "        # If it's a sentence: extract any protected phrases inside, then words\n",
        "        # 1) pull out protected phrases inside the piece\n",
        "        found = []\n",
        "        rest  = p\n",
        "        for ph in sorted(KEEP_PHRASES, key=lambda x: -len(x)):\n",
        "            ph_prot = ph.replace(\" \", \"_\")\n",
        "            if ph_prot.lower() in rest.lower():\n",
        "                # collect and remove\n",
        "                found.append(ph)\n",
        "                rest = re.sub(re.escape(ph_prot), \" \", rest, flags=re.IGNORECASE)\n",
        "\n",
        "        # 2) remaining words\n",
        "        rest = clean_phrase(rest.replace(\"_\",\" \"))\n",
        "        words = drop_non_ingredients(rest.split())\n",
        "        # singularize last token of any 1-2 word units (light touch)\n",
        "        words = [to_singular(w) for w in words]\n",
        "\n",
        "        # combine: protected phrases + remaining words\n",
        "        for ph in found:\n",
        "            tokens.append(ph)\n",
        "        for w in words:\n",
        "            tokens.append(w)\n",
        "\n",
        "    # apply aliases & final cleanup\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        t0 = t.strip().lower()\n",
        "        t0 = t0.replace(\"_\",\" \")\n",
        "        if not t0:\n",
        "            continue\n",
        "        t0 = ALIASES.get(t0, t0)\n",
        "        out.append(t0)\n",
        "\n",
        "    # de-dup preserve order\n",
        "    seen, dedup = set(), []\n",
        "    for x in out:\n",
        "        if x not in seen:\n",
        "            dedup.append(x); seen.add(x)\n",
        "\n",
        "    return dedup\n",
        "\n",
        "def normalize_for_clustering(token: str) -> str:\n",
        "    \"\"\"Secondary normalization for clustering stage.\"\"\"\n",
        "    t = token.strip().lower()\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t\n",
        "\n",
        "# ----------------------------\n",
        "# Load data (robust to \"NA\")\n",
        "# ----------------------------\n",
        "df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\", keep_default_na=False, na_filter=False)\n",
        "n_rows = len(df)\n",
        "CLASS_COL = next((c for c in CLASS_COL_CANDIDATES if c in df.columns), None)\n",
        "if not CLASS_COL:\n",
        "    raise ValueError(f\"Could not find classifications column. Found: {list(df.columns)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Tokenize all rows with phrase-aware extractor\n",
        "# ----------------------------\n",
        "raw_lists = df[CLASS_COL].apply(split_to_tokens)\n",
        "\n",
        "# Clean tokens for clustering\n",
        "cleaned_all, bag = [], []\n",
        "for tokens in raw_lists:\n",
        "    cleaned = []\n",
        "    for t in tokens:\n",
        "        nt = normalize_for_clustering(t)\n",
        "        if nt:\n",
        "            cleaned.append(nt)\n",
        "            bag.append(nt)\n",
        "    cleaned_all.append(cleaned)\n",
        "\n",
        "freq = Counter(bag)\n",
        "unique_tokens = list(freq.keys())\n",
        "\n",
        "# If no tokens â†’ fill unknowns but still emit rows\n",
        "if not unique_tokens:\n",
        "    df[\"classifications_std_list\"] = [[\"unknown_ingredients\"] for _ in range(n_rows)]\n",
        "    out = df[[\"dish_name\",\"classifications_std_list\",\"image_file\",\"scrape_date\"]]\n",
        "    out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "    print(f\"âœ… Done (all unknown). Rows: {len(out)}\")\n",
        "    raise SystemExit\n",
        "\n",
        "# ----------------------------\n",
        "# Embed & cluster\n",
        "# ----------------------------\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "emb = model.encode(unique_tokens, show_progress_bar=True, normalize_embeddings=True)\n",
        "\n",
        "try:\n",
        "    clust = AgglomerativeClustering(\n",
        "        n_clusters=None, linkage=\"average\",\n",
        "        metric=\"cosine\", distance_threshold=DISTANCE_THRESHOLD\n",
        "    )\n",
        "except TypeError:\n",
        "    clust = AgglomerativeClustering(\n",
        "        n_clusters=None, linkage=\"average\",\n",
        "        affinity=\"cosine\", distance_threshold=DISTANCE_THRESHOLD\n",
        "    )\n",
        "labels = clust.fit_predict(emb)\n",
        "\n",
        "clusters = defaultdict(list)\n",
        "for tok, lab in zip(unique_tokens, labels):\n",
        "    clusters[lab].append(tok)\n",
        "\n",
        "# canonical per cluster: most frequent (tie -> shortest)\n",
        "cluster_canonical = {lab: sorted(toks, key=lambda t: (-freq[t], len(t)))[0] for lab, toks in clusters.items()}\n",
        "token2canon = {tok: cluster_canonical[lab] for tok, lab in zip(unique_tokens, labels)}\n",
        "\n",
        "# optional fuzzy nudge\n",
        "canonical_vocab = list(set(token2canon.values()))\n",
        "def fuzzy_canon(token: str, cutoff=FUZZY_SCORE_CUTOFF):\n",
        "    best = process.extractOne(token, canonical_vocab, scorer=fuzz.WRatio, score_cutoff=cutoff)\n",
        "    return best[0] if best else token\n",
        "for tok in list(token2canon.keys()):\n",
        "    cand = fuzzy_canon(tok)\n",
        "    if cand != token2canon[tok] and freq.get(cand, 0) >= freq.get(token2canon[tok], 0):\n",
        "        token2canon[tok] = cand\n",
        "\n",
        "# ----------------------------\n",
        "# Apply mapping to EVERY row\n",
        "# ----------------------------\n",
        "std_lists = []\n",
        "for cleaned in cleaned_all:\n",
        "    mapped = [token2canon.get(t, t) for t in cleaned]\n",
        "    # de-dup preserve order\n",
        "    seen, dedup = set(), []\n",
        "    for x in mapped:\n",
        "        if x not in seen:\n",
        "            dedup.append(x); seen.add(x)\n",
        "    if not dedup:\n",
        "        dedup = [\"unknown_ingredients\"]\n",
        "    std_lists.append(dedup)\n",
        "\n",
        "assert len(std_lists) == n_rows\n",
        "df[\"classifications_std_list\"] = std_lists\n",
        "\n",
        "# --- Force unknown_ingredients for placeholder fragments like [\"not\",\"found\"] ---\n",
        "def coalesce_unknown(lst):\n",
        "    # If list is empty, we'll handle later; if it exactly equals [\"not\",\"found\"], force unknown\n",
        "    if not lst:\n",
        "        return [\"unknown_ingredients\"]\n",
        "    lf = [x.strip().lower() for x in lst]\n",
        "    if lf == [\"not\",\"found\"] or lf == [\"unknown\"]:\n",
        "        return [\"unknown_ingredients\"]\n",
        "    # If list contains only non-ingredient placeholders, collapse too\n",
        "    joined = \" \".join(lf)\n",
        "    if is_unknown_text(joined):\n",
        "        return [\"unknown_ingredients\"]\n",
        "    return lst\n",
        "\n",
        "df[\"classifications_std_list\"] = df[\"classifications_std_list\"].apply(coalesce_unknown)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Post-processing consistency fix: collapse generic + specific variants\n",
        "# ---------------------------------------------------------\n",
        "canonical_collapse = {\n",
        "    # generic â†’ preferred\n",
        "    \"oil\": \"olive oil\",\n",
        "    \"vegetable oil\": \"olive oil\",\n",
        "    \"ghee\": \"butter\",\n",
        "    \"yogurt\": \"labneh\",       # example, if you prefer labneh\n",
        "    # add any others you notice\n",
        "}\n",
        "\n",
        "def collapse_variants(lst):\n",
        "    \"\"\"Replace generic tokens with canonical equivalents and de-duplicate.\"\"\"\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for x in lst:\n",
        "        y = canonical_collapse.get(x, x)\n",
        "        if y not in seen:\n",
        "            out.append(y)\n",
        "            seen.add(y)\n",
        "    return out\n",
        "\n",
        "df[\"classifications_std_list\"] = df[\"classifications_std_list\"].apply(collapse_variants)\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Reports & mapping\n",
        "# ----------------------------\n",
        "rows = []\n",
        "for lab, toks in clusters.items():\n",
        "    can = cluster_canonical[lab]\n",
        "    for t in sorted(toks):\n",
        "        rows.append({\"cluster_id\": lab, \"canonical\": can, \"member\": t, \"member_freq\": freq[t]})\n",
        "pd.DataFrame(rows).sort_values([\"canonical\",\"member\"]).to_csv(REPORT_CSV, index=False, encoding=\"utf-8\")\n",
        "with open(MAP_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(token2canon, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ----------------------------\n",
        "# Save ONLY the requested 4 columns\n",
        "# ----------------------------\n",
        "required_cols = [\"dish_name\", \"image_file\", \"scrape_date\"]\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing expected columns: {missing}\")\n",
        "\n",
        "out = df[[\"dish_name\",\"classifications_std_list\",\"image_file\",\"scrape_date\"]]\n",
        "out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Done.\")\n",
        "print(f\"Rows in/out: {n_rows} / {len(out)}\")\n",
        "print(f\"Example rows:\\n{out.head(6).to_string(index=False)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jsj8pb3IVAyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 7.1: Clean and Standardize Ingredients - Final Consistency Pass**\n",
        "\n",
        "After cleaning and standardizing the ingredients, some names were still not completely consistent.\n",
        "For example, a few dishes still used slightly different spellings or duplicate ingredient terms such as:\n",
        "\n",
        "> [\"paper\", \"chilli\", \"olive oils\"] instead of [\"chili\", \"olive oil\"].\n",
        "\n",
        "This step was added to double-check and correct any remaining inconsistencies in spelling, wording, or duplicates.\n",
        "It reviews every ingredient list and makes final adjustments to ensure that all rows follow the same standard format."
      ],
      "metadata": {
        "id": "H86xaWkARf6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Consistency pass over the produced CSV\n",
        "# - Enforce canonical spellings & synonyms (e.g., chilli â†’ chili)\n",
        "# - Your requests: paper â†’ chili, allspices â†’ mixed spices\n",
        "# - Remove non-ingredient words, dedupe, keep only 4 columns\n",
        "# Outputs:\n",
        "#   â€¢ SaudiFoodFile_standardized_consistent.csv\n",
        "#   â€¢ consistency_changes_report.csv (what changed)\n",
        "# ============================================================\n",
        "\n",
        "import ast, re, json, pandas as pd\n",
        "\n",
        "INPUT_CSV  = \"SaudiFoodFile_standardized.csv\"   # <â€” your produced file\n",
        "OUTPUT_CSV = \"SaudiFoodFile_standardized_consistent.csv\"\n",
        "REPORT_CSV = \"consistency_changes_report.csv\"\n",
        "\n",
        "# If your column name differs, adjust here:\n",
        "CLASS_COL = \"classifications_std_list\"\n",
        "REQ_COLS  = [\"dish_name\", CLASS_COL, \"image_file\", \"scrape_date\"]\n",
        "\n",
        "# ---------- canonicalization config ----------\n",
        "# âœ… Add/adjust anything you want here:\n",
        "ALIASES = {\n",
        "    # your explicit requests\n",
        "    \"paper\": \"chili\",              # e.g., OCR/typo -> chili\n",
        "    \"allspices\": \"mixed spices\",\n",
        "    \"all-spices\": \"mixed spices\",\n",
        "    \"all spice\": \"mixed spices\",\n",
        "    \"all-spice\": \"mixed spices\",\n",
        "    \"allspice\": \"mixed spices\",    # if you prefer \"mixed spices\" as canonical\n",
        "\n",
        "    # common spellings / plurals / variants\n",
        "    \"chilli\": \"chili\",\n",
        "    \"chilies\": \"chili\",\n",
        "    \"chillies\": \"chili\",\n",
        "    \"green chilli\": \"chili\",\n",
        "    \"green chili\": \"chili\",\n",
        "    \"red chili\": \"chili\",\n",
        "    \"red chilli\": \"chili\",\n",
        "    \"chili pepper\": \"chili\",\n",
        "    \"chili peppers\": \"chili\",\n",
        "\n",
        "    \"black lemon\": \"black lime\",\n",
        "    \"dried lime\": \"black lime\",\n",
        "    \"dried limes\": \"black lime\",\n",
        "    \"omani lime\": \"black lime\",\n",
        "    \"omani limes\": \"black lime\",\n",
        "    \"loomi\": \"black lime\",\n",
        "\n",
        "    \"veg oil\": \"vegetable oil\",\n",
        "    \"olive oils\": \"olive oil\",\n",
        "\n",
        "    \"cilantro\": \"coriander\",\n",
        "    \"coriander leaves\": \"coriander\",\n",
        "    \"green coriander\": \"coriander\",\n",
        "    \"parsley leaves\": \"parsley\",\n",
        "    \"mint leaves\": \"mint\",\n",
        "    \"spring onion\": \"green onion\",\n",
        "    \"capsicum\": \"bell pepper\",\n",
        "    \"green pepper\": \"bell pepper\",\n",
        "    \"sweet pepper\": \"bell pepper\",\n",
        "\n",
        "    \"turmeric powder\": \"turmeric\",\n",
        "    \"ginger powder\": \"ginger\",\n",
        "    \"garlic powder\": \"garlic\",\n",
        "    \"onion powder\": \"onion\",\n",
        "    \"cinnamon stick\": \"cinnamon\",\n",
        "    \"cinnamon sticks\": \"cinnamon\",\n",
        "    \"cardamon\": \"cardamom\",\n",
        "\n",
        "    \"tomato puree\": \"tomato paste\",\n",
        "    \"tomato purÃ©e\": \"tomato paste\",\n",
        "    \"simple syrup\": \"sugar syrup\",\n",
        "\n",
        "    \"garbanzo bean\": \"chickpeas\",\n",
        "    \"garbanzo beans\": \"chickpeas\",\n",
        "    \"chick pea\": \"chickpeas\",\n",
        "    \"chick peas\": \"chickpeas\",\n",
        "    \"black eyed bean\": \"black-eyed beans\",\n",
        "    \"black eyed beans\": \"black-eyed beans\",\n",
        "}\n",
        "\n",
        "# If BOTH appear, drop the generic in favor of the preferred.\n",
        "# (You can add more pairs here.)\n",
        "COLLAPSE_IF_PRESENT = [\n",
        "    (\"oil\", \"olive oil\"),          # keep \"olive oil\", drop \"oil\"\n",
        "    (\"vegetable oil\", \"olive oil\"),# keep \"olive oil\"\n",
        "    (\"pepper\", \"chili\"),           # if chili is present, drop generic \"pepper\"\n",
        "]\n",
        "\n",
        "# Words to drop if they sneak in (not ingredients)\n",
        "NON_ING_WORDS = {\n",
        "    \"after\",\"decorate\",\"decoration\",\"decorations\",\"add\",\"with\",\"such\",\"touch\",\"patriotic\",\n",
        "    \"or\",\"and\",\"the\",\"a\",\"an\",\"then\",\"until\",\"when\",\"like\",\"as\",\"to\",\"for\",\"of\",\"into\",\"over\",\n",
        "    \"warm\",\"hot\",\"cold\",\"slice\",\"sliced\",\"diced\",\"chopped\",\"minced\",\"ground\",\"crushed\",\"whole\",\n",
        "    \"fresh\",\"optional\",\"needed\",\"garnish\",\"make\",\"prepare\",\"preparation\",\"cook\",\"cooked\",\"baked\",\n",
        "    \"boiled\",\"fried\",\"seauted\",\"sauteed\",\"browned\",\"mix\",\"mixed\",\"topping\",\"kitchen\",\"precise\",\"instant\"\n",
        "}\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def parse_list_cell(x):\n",
        "    \"\"\"Parse list stored as Python-list string, or already-list, or fallback.\"\"\"\n",
        "    if isinstance(x, list):\n",
        "        return [str(t).strip().lower() for t in x if str(t).strip()]\n",
        "    s = str(x).strip()\n",
        "    if not s:\n",
        "        return []\n",
        "    try:\n",
        "        val = ast.literal_eval(s)\n",
        "        if isinstance(val, list):\n",
        "            return [str(t).strip().lower() for t in val if str(t).strip()]\n",
        "    except Exception:\n",
        "        pass\n",
        "    # fallback: split by | or comma\n",
        "    parts = re.split(r\"\\s*\\|\\s*|,+\", s)\n",
        "    return [p.strip().lower() for p in parts if p.strip()]\n",
        "\n",
        "def apply_aliases(tokens):\n",
        "    out = []\n",
        "    for t in tokens:\n",
        "        t0 = t.strip().lower()\n",
        "        if not t0 or t0 in NON_ING_WORDS:\n",
        "            continue\n",
        "        t0 = ALIASES.get(t0, t0)\n",
        "        out.append(t0)\n",
        "    # de-dup preserve order\n",
        "    seen, dedup = set(), []\n",
        "    for x in out:\n",
        "        if x not in seen:\n",
        "            dedup.append(x); seen.add(x)\n",
        "    return dedup\n",
        "\n",
        "def collapse_generics(tokens):\n",
        "    s = set(tokens)\n",
        "    # if preferred present, drop generic\n",
        "    for generic, preferred in COLLAPSE_IF_PRESENT:\n",
        "        if preferred in s and generic in s:\n",
        "            s.discard(generic)\n",
        "    # rebuild original order\n",
        "    out, seen = [], set()\n",
        "    for x in tokens:\n",
        "        if x in s and x not in seen:\n",
        "            out.append(x); seen.add(x)\n",
        "    return out\n",
        "\n",
        "# ---------- load ----------\n",
        "df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
        "missing = [c for c in REQ_COLS if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# ---------- process ----------\n",
        "changes = []\n",
        "new_lists = []\n",
        "for idx, row in df.iterrows():\n",
        "    original = parse_list_cell(row[CLASS_COL])\n",
        "    aliased  = apply_aliases(original)\n",
        "    collapsed = collapse_generics(aliased)\n",
        "\n",
        "    # track changes (only if different)\n",
        "    if original != collapsed:\n",
        "        changes.append({\n",
        "            \"row\": idx,\n",
        "            \"before\": json.dumps(original, ensure_ascii=False),\n",
        "            \"after\":  json.dumps(collapsed, ensure_ascii=False),\n",
        "        })\n",
        "    new_lists.append(collapsed if collapsed else [\"unknown_ingredients\"])\n",
        "\n",
        "df[CLASS_COL] = new_lists\n",
        "\n",
        "# keep only requested 4 columns, same order\n",
        "out = df[[\"dish_name\", CLASS_COL, \"image_file\", \"scrape_date\"]]\n",
        "out.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "pd.DataFrame(changes).to_csv(REPORT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Consistency pass complete.\")\n",
        "print(f\"â€¢ Output CSV: {OUTPUT_CSV}\")\n",
        "print(f\"â€¢ Changes report: {REPORT_CSV}\")\n",
        "print(f\"Rows changed: {len(changes)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I4aq1VIbVHO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Ingredient Encoding Only - Keep All Original Columns ===\n",
        "# Colab-ready: upload â â€¯SaudiFoodFile_standardized_consistent.csvâ€¯â  to /content first.\n",
        "\n",
        "import pandas as pd, numpy as np, ast, re, os\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "SRC = \"./SaudiFoodFile_standardized_consistent (1).csv\"   # <-- path in Colab\n",
        "OUT_DIR = \"/content\"\n",
        "\n",
        "# -------------- HELPERS -----------------\n",
        "def parse_ingredients(val):\n",
        "    \"\"\"Turn a list-like string or pipe-separated string into a clean Python list.\"\"\"\n",
        "    if pd.isna(val):\n",
        "        return []\n",
        "    if isinstance(val, list):\n",
        "        return [str(x).strip() for x in val if str(x).strip()]\n",
        "    s = str(val).strip()\n",
        "    if s.startswith('[') and s.endswith(']'):\n",
        "        try:\n",
        "            parsed = ast.literal_eval(s)\n",
        "            return [str(x).strip() for x in parsed if str(x).strip()]\n",
        "        except Exception:\n",
        "            pass\n",
        "    return [x.strip() for x in s.split('|') if x.strip()]\n",
        "\n",
        "def sanitize_col(name):\n",
        "    \"\"\"Safe ingredient column names: lowercase, underscores, alnum-only.\"\"\"\n",
        "    name = str(name).lower()\n",
        "    name = re.sub(r\"[^\\w]+\", \"_\", name)\n",
        "    name = re.sub(r\"__+\", \"_\", name).strip(\"_\") # Fix: Use \"__+\" to match one or more underscores\n",
        "    return name\n",
        "\n",
        "def save_csv(df, path):\n",
        "    df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------------- LOAD --------------------\n",
        "df = None\n",
        "for enc in [\"utf-8\", \"utf-8-sig\", \"cp1256\"]:\n",
        "    try:\n",
        "        df = pd.read_csv(SRC, encoding=enc)\n",
        "        break\n",
        "    except Exception:\n",
        "        pass\n",
        "if df is None:\n",
        "    raise RuntimeError(\"Could not read the CSV with common encodings. Check path/encoding.\")\n",
        "\n",
        "# Check for required ingredients column\n",
        "if \"classifications_std_list\" not in df.columns:\n",
        "    raise ValueError(\"Missing required column: classifications_std_list\")\n",
        "\n",
        "# -------------- INGREDIENT ENCODING --------------\n",
        "# Parse ingredients\n",
        "df[\"ingredients_list\"] = df[\"classifications_std_list\"].apply(parse_ingredients)\n",
        "\n",
        "# Build multi-hot ingredient columns and vocab\n",
        "all_ings = [ing for lst in df[\"ingredients_list\"] for ing in lst]\n",
        "vocab = sorted(set(all_ings))\n",
        "\n",
        "print(f\"Found {len(vocab)} unique ingredients:\")\n",
        "for i, ing in enumerate(vocab[:10]):  # Show first 10\n",
        "    print(f\"  {i+1}. {ing}\")\n",
        "if len(vocab) > 10:\n",
        "    print(f\"  ... and {len(vocab) - 10} more\")\n",
        "\n",
        "# Create multi-hot encoded columns and add them to the original dataframe\n",
        "for ing in vocab:\n",
        "    col = f\"ing_{sanitize_col(ing)}\"\n",
        "    df[col] = df[\"ingredients_list\"].apply(lambda lst, i=ing: int(i in lst))\n",
        "\n",
        "# -------------- OUTPUTS --------------\n",
        "# Save the complete dataframe with all original columns + new ingredient encoding columns\n",
        "save_csv(df, f\"{OUT_DIR}/dataset_with_ingredient_encoding.csv\")\n",
        "\n",
        "# Optional: Save just the ingredient encoding columns for reference\n",
        "ing_cols = [c for c in df.columns if c.startswith(\"ing_\")]\n",
        "ingredient_only_table = df[ing_cols].copy()\n",
        "save_csv(ingredient_only_table, f\"{OUT_DIR}/ingredient_encoding_columns_only.csv\")\n",
        "\n",
        "# Optional: Save ingredient vocabulary\n",
        "vocab_df = pd.DataFrame({\"ingredient_name\": vocab})\n",
        "save_csv(vocab_df, f\"{OUT_DIR}/ingredient_vocabulary.csv\")\n",
        "\n",
        "# -------------- SUMMARY PRINT --------------\n",
        "print(\"\\nâœ… Ingredient Encoding Complete\")\n",
        "print({\n",
        "    \"total_rows\": len(df),\n",
        "    \"total_ingredients\": len(vocab),\n",
        "    \"ingredient_columns_created\": len(ing_cols),\n",
        "    \"total_columns_in_output\": len(df.columns),\n",
        "    \"output_files\": [\n",
        "        \"dataset_with_ingredient_encoding.csv (all original columns + ingredient encoding)\",\n",
        "        \"ingredient_encoding_columns_only.csv\",\n",
        "        \"ingredient_vocabulary.csv\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Show the structure of the output\n",
        "print(f\"\\nðŸ“Š Output dataset has {len(df.columns)} total columns:\")\n",
        "print(f\"   - Original columns: {len(df.columns) - len(ing_cols)}\")\n",
        "print(f\"   - Ingredient encoding columns: {len(ing_cols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO_spLlIJP5k",
        "outputId": "c12f695c-1602-4dc1-d1bf-5c4ba3f591a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 104 unique ingredients:\n",
            "  1. almond\n",
            "  2. appetizer\n",
            "  3. bay\n",
            "  4. beef\n",
            "  5. bread\n",
            "  6. butter\n",
            "  7. camel\n",
            "  8. cardamom\n",
            "  9. carrot\n",
            "  10. cheese\n",
            "  ... and 94 more\n",
            "\n",
            "âœ… Ingredient Encoding Complete\n",
            "{'total_rows': 278, 'total_ingredients': 104, 'ingredient_columns_created': 104, 'total_columns_in_output': 109, 'output_files': ['dataset_with_ingredient_encoding.csv (all original columns + ingredient encoding)', 'ingredient_encoding_columns_only.csv', 'ingredient_vocabulary.csv']}\n",
            "\n",
            "ðŸ“Š Output dataset has 109 total columns:\n",
            "   - Original columns: 5\n",
            "   - Ingredient encoding columns: 104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3339305461.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col] = df[\"ingredients_list\"].apply(lambda lst, i=ing: int(i in lst))\n",
            "/tmp/ipython-input-3339305461.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col] = df[\"ingredients_list\"].apply(lambda lst, i=ing: int(i in lst))\n",
            "/tmp/ipython-input-3339305461.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col] = df[\"ingredients_list\"].apply(lambda lst, i=ing: int(i in lst))\n",
            "/tmp/ipython-input-3339305461.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col] = df[\"ingredients_list\"].apply(lambda lst, i=ing: int(i in lst))\n",
            "/tmp/ipython-input-3339305461.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col] = df[\"ingredients_list\"].apply(lambda lst, i=ing: int(i in lst))\n",
            "/tmp/ipython-input-3339305461.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[col] = df[\"ingredients_list\"].apply(lambda lst, i=ing: int(i in lst))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  3 â€“ Image Processing:\n",
        "image preprocessing is an essential stage before any data analysis or machine-learning model training.\n",
        "\n",
        "The main goal is to make sure that all images are clean, consistent, and properly formatted for model input.\n",
        "\n",
        "This includes verifying image files, correcting errors, resizing them to a uniform size, normalizing their pixel values, and applying image augmentation to increase dataset diversity."
      ],
      "metadata": {
        "id": "MEMsoygmNrHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 â€“ Setup and Imports:\n",
        "In this step, I set up the environment and import all the required Python libraries such as pandas, numpy, and Pillow.\n",
        "\n",
        "These libraries allow me to load, analyze, and process the images efficiently.\n",
        "I also define configuration variables like image size and output folders, which control how the images will be processed throughout the notebook."
      ],
      "metadata": {
        "id": "uNH-IZdtNyku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Setup & Imports ===\n",
        "!pip install -q pillow numpy pandas tqdm\n",
        "\n",
        "import io, os, zipfile, shutil, csv, sys\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- Config  ----\n",
        "RESIZE_TO = (512, 512)\n",
        "IMAGES_ROOT = \"images\"   # where the unzipped images will live\n",
        "CLEAN_DIR   = \"clean_images\"  # output directory for processed images\n",
        "MISSING_CSV = \"missing_images.csv\"  # log of missing/corrupt files\n",
        "SAVE_NUMPY  = True        # also save normalized arrays as .npy"
      ],
      "metadata": {
        "id": "r5qUsD9LNwR2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 â€“ Upload CSV and ZIP:\n",
        "Here I upload two main files:\n",
        "\n",
        "\n",
        "*   the CSV file, which contains the dataset information including the image_file column (the name of each image),\n",
        "*   the ZIP file that contains all the original images.\n",
        "\n",
        "\n",
        "This step ensures that the data is available in the Colab environment for extraction and further processing."
      ],
      "metadata": {
        "id": "1ST7v0PDN5VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Upload CSV and ZIP ===\n",
        "from google.colab import files\n",
        "uploaded = files.upload()   # Choose: CSV and the images ZIP\n",
        "\n",
        "# Identify CSV and ZIP from uploads\n",
        "csv_path = None\n",
        "zip_path = None\n",
        "for name in uploaded:\n",
        "    if name.lower().endswith(\".csv\"):\n",
        "        csv_path = name\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        zip_path = name\n",
        "\n",
        "assert csv_path is not None, \"Please upload your CSV file.\"\n",
        "assert zip_path is not None, \"Please upload your images .zip file.\"\n",
        "\n",
        "print(\"CSV:\", csv_path)\n",
        "print(\"ZIP:\", zip_path)"
      ],
      "metadata": {
        "id": "-z05Vsh2OJen",
        "outputId": "c4ec899b-54a2-4f55-f2bc-e71c2409ee38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cf4b07aa-7c6a-4521-80ad-ad97d0894c59\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cf4b07aa-7c6a-4521-80ad-ad97d0894c59\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving renamed_images-2.zip to renamed_images-2 (1).zip\n",
            "Saving SaudiFoodFile_standardized_consistent.csv to SaudiFoodFile_standardized_consistent.csv\n",
            "CSV: SaudiFoodFile_standardized_consistent.csv\n",
            "ZIP: renamed_images-2 (1).zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 â€“ Extract Images:\n",
        "In this step, I extract all images from the uploaded ZIP file into a working folder named images/.\n",
        "\n",
        "This allows me to access each image directly from the folder.\n",
        "A quick preview of a few filenames is displayed to confirm that the extraction worked correctly."
      ],
      "metadata": {
        "id": "oGcqpO-lOa_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Extract images ===\n",
        "# Clean old folders if re-running\n",
        "if os.path.exists(IMAGES_ROOT):\n",
        "    shutil.rmtree(IMAGES_ROOT)\n",
        "os.makedirs(IMAGES_ROOT, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall(IMAGES_ROOT)\n",
        "\n",
        "# Quick peek\n",
        "sample_list = []\n",
        "for root, _, files_ in os.walk(IMAGES_ROOT):\n",
        "    for f in files_:\n",
        "        sample_list.append(os.path.join(root, f))\n",
        "        if len(sample_list) >= 5:\n",
        "            break\n",
        "    if len(sample_list) >= 5:\n",
        "        break\n",
        "\n",
        "print(f\"Extracted under: {IMAGES_ROOT}\")\n",
        "print(\"Sample files:\", sample_list[:5])"
      ],
      "metadata": {
        "id": "YGbXKiMzOaS3",
        "outputId": "a38103a6-7045-49d7-a360-1e8501932dd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted under: images\n",
            "Sample files: ['images/__MACOSX/._renamed_images-2', 'images/__MACOSX/renamed_images-2/._img(209).png', 'images/__MACOSX/renamed_images-2/._img(52).jpg', 'images/__MACOSX/renamed_images-2/._img(81).jpg', 'images/__MACOSX/renamed_images-2/._img(86).png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 â€“ Read CSV and Validate Column:\n",
        "Here I load the dataset CSV into a pandas DataFrame and confirm which column holds the image filenames.\n",
        "\n",
        "For this project, the correct column is image_file.\n",
        "\n",
        "This validation ensures that each record in the CSV can be linked to its corresponding image for processing."
      ],
      "metadata": {
        "id": "73cN4ReKOf6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Load CSV and check column ===\n",
        "df = pd.read_csv(csv_path)\n",
        "col = \"image_file\"  # confirmed column name from your dataset\n",
        "print(f\"Using image filename column: '{col}'\")\n",
        "print(df[[col]].head())"
      ],
      "metadata": {
        "id": "AnkfI0AWOgsw",
        "outputId": "361402a9-adb9-4cce-e158-99e60354f916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using image filename column: 'image_file'\n",
            "   image_file\n",
            "0  img(1).jpg\n",
            "1  img(2).jpg\n",
            "2  img(3).jpg\n",
            "3  img(4).jpg\n",
            "4  img(5).jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 â€“ Image Lookup Function:\n",
        "This section defines a helper function, safe_find(), which locates image files inside the extracted folder.\n",
        "\n",
        "It searches for filenames in a case-insensitive way and ensures the correct image is found even if there are minor differences in name formatting.\n"
      ],
      "metadata": {
        "id": "lnjAxFiDOit6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Utility to find images robustly ===\n",
        "# Build a lookup of normalized filenames (lowercase, basename only) -> full path\n",
        "all_files = {}\n",
        "for root, _, files_ in os.walk(IMAGES_ROOT):\n",
        "    for f in files_:\n",
        "        all_files[os.path.basename(f).lower()] = os.path.join(root, f)\n",
        "\n",
        "def safe_find(full_or_base_name: str):\n",
        "    \"\"\"\n",
        "    Try exact basename match (case-insensitive).\n",
        "    If path-like input is given in CSV, take only the basename for matching.\n",
        "    \"\"\"\n",
        "    if not isinstance(full_or_base_name, str):\n",
        "        return None\n",
        "    base = os.path.basename(full_or_base_name).lower().strip()\n",
        "    return all_files.get(base, None)"
      ],
      "metadata": {
        "id": "f54zHFQsOlpa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 â€“ Main Image Processing Loop:\n",
        "This is the main step where the actual processing happens for every image listed in the CSV.\n",
        "\n",
        "The code performs the following actions:\n",
        "\n",
        "* Saves the processed copies to a new folder called clean_images/.\n",
        "\n",
        "\n",
        "*   Verifies if the image exists; logs missing ones.\n",
        "*   Opens each image safely and skips corrupted files.\n",
        "\n",
        "\n",
        "* Converts non-RGB images (like grayscale) to RGB to maintain consistency.\n",
        "\n",
        "*  Resizes all images to 512x512 pixels.\n",
        "\n",
        "\n",
        "*   Saves the processed copies to a new folder called clean_images/.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "At the end, a summary is printed showing how many images were successfully processed, how many were missing, and how many were converted."
      ],
      "metadata": {
        "id": "WBY3GdfMOpzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Main processing loop ===\n",
        "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
        "\n",
        "missing_rows = []\n",
        "ok_count = 0\n",
        "missing_count = 0\n",
        "corrupt_count = 0\n",
        "converted_to_rgb = 0\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
        "    rel = row[col]\n",
        "    path = safe_find(rel)\n",
        "    if path is None or (not os.path.exists(path)):\n",
        "        missing_rows.append({\n",
        "            \"index\": idx,\n",
        "            \"image_file\": rel,\n",
        "            \"issue\": \"missing\"\n",
        "        })\n",
        "        missing_count += 1\n",
        "        continue\n",
        "\n",
        "    # Try to open via PIL\n",
        "    try:\n",
        "        with Image.open(path) as im:\n",
        "            # Convert non-RGB to RGB\n",
        "            if im.mode != \"RGB\":\n",
        "                im = im.convert(\"RGB\")\n",
        "                converted_to_rgb += 1\n",
        "\n",
        "            # Resize\n",
        "            im = im.resize(RESIZE_TO, Image.BILINEAR)\n",
        "\n",
        "            # Save to CLEAN_DIR preserving basename\n",
        "            out_name = os.path.basename(path)\n",
        "            out_path = os.path.join(CLEAN_DIR, out_name)\n",
        "            im.save(out_path, format=\"JPEG\", quality=95)\n",
        "\n",
        "            ok_count += 1\n",
        "\n",
        "    except (UnidentifiedImageError, OSError) as e:\n",
        "        missing_rows.append({\n",
        "            \"index\": idx,\n",
        "            \"image_file\": rel,\n",
        "            \"issue\": f\"corrupt_or_unreadable: {type(e).__name__}\"\n",
        "        })\n",
        "        corrupt_count += 1\n",
        "\n",
        "print(\"Done.\")\n",
        "print(f\"OK processed: {ok_count}\")\n",
        "print(f\"Missing:      {missing_count}\")\n",
        "print(f\"Corrupt:      {corrupt_count}\")\n",
        "print(f\"Converted to RGB (non-RGB originally): {converted_to_rgb}\")"
      ],
      "metadata": {
        "id": "2Jgcv9XsOvRS",
        "outputId": "60cb57ca-aaac-457c-fad9-04bcec5e45f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:12<00:00, 21.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "OK processed: 278\n",
            "Missing:      0\n",
            "Corrupt:      0\n",
            "Converted to RGB (non-RGB originally): 93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7 â€“ Save Log and Summary:\n",
        "This step saves all missing or corrupted image details into a CSV file named missing_images.csv.\n",
        "\n",
        "It also prints a summary of image processing results"
      ],
      "metadata": {
        "id": "XLBOyWe-O185"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 7: Save missing_images.csv and show summary ===\n",
        "log_df = pd.DataFrame(missing_rows, columns=[\"index\", \"image_file\", \"issue\"])\n",
        "log_df.to_csv(MISSING_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "summary = {\n",
        "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "    \"total_rows_in_csv\": int(len(df)),\n",
        "    \"ok_processed\": int(ok_count),\n",
        "    \"missing\": int(missing_count),\n",
        "    \"corrupt\": int(corrupt_count),\n",
        "    \"converted_to_rgb\": int(converted_to_rgb),\n",
        "    \"resize_to\": f\"{RESIZE_TO[0]}x{RESIZE_TO[1]}\",\n",
        "    \"clean_dir\": CLEAN_DIR,\n",
        "    \"log_csv\": MISSING_CSV,\n",
        "}\n",
        "for k,v in summary.items():\n",
        "    print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "id": "SEJMoh4wO0oQ",
        "outputId": "93ba4add-4140-4075-f3c2-2930797151db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timestamp: 2025-10-16T23:01:46\n",
            "total_rows_in_csv: 278\n",
            "ok_processed: 278\n",
            "missing: 0\n",
            "corrupt: 0\n",
            "converted_to_rgb: 93\n",
            "resize_to: 512x512\n",
            "clean_dir: clean_images\n",
            "log_csv: missing_images.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 â€“ Normalize Pixel Values:\n",
        "After the images are cleaned and resized, I normalize all pixel values to be between 0 and 1.\n",
        "\n",
        "This transformation ensures that all images are on the same scale, making it easier and faster for models to learn.\n",
        "\n",
        "The normalized dataset is saved as clean_images_normalized.npy â€” a compact NumPy file that stores all image arrays efficiently and is ready for use in training or analysis."
      ],
      "metadata": {
        "id": "KcQzBmxbO4YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 8: Normalization step ===\n",
        "#This step prepares cleaned images so they can be used in machine-learning or deep-learning models.\n",
        "\n",
        "clean_paths = []\n",
        "for f in os.listdir(CLEAN_DIR):\n",
        "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        clean_paths.append(os.path.join(CLEAN_DIR, f))\n",
        "clean_paths.sort()\n",
        "\n",
        "arrays = []\n",
        "for p in tqdm(clean_paths, desc=\"Normalizing\"):\n",
        "    with Image.open(p) as im:\n",
        "        arr = np.asarray(im, dtype=np.float32) / 255.0  # [0,1]\n",
        "        arrays.append(arr)\n",
        "\n",
        "X = np.stack(arrays, axis=0) if arrays else np.empty((0, RESIZE_TO[1], RESIZE_TO[0], 3))\n",
        "print(\"Normalized array shape:\", X.shape)\n",
        "\n",
        "if SAVE_NUMPY:\n",
        "    np.save(\"clean_images_normalized.npy\", X)\n",
        "    print(\"Saved: clean_images_normalized.npy\")"
      ],
      "metadata": {
        "id": "qG3WF2aNO_8e",
        "outputId": "ce087c1d-b183-40dd-c016-59768437df6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Normalizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:01<00:00, 155.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized array shape: (278, 512, 512, 3)\n",
            "Saved: clean_images_normalized.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step is used to verify that the final processed dataset is correct and consistent before using it in any model training.\n",
        "\n",
        "After normalization, itâ€™s important to make sure all images have been properly prepared and that no hidden issues remain.\n",
        "\n",
        "In this step, we:\n",
        "Check the pixel value range to confirm all values are between 0 and 1, ensuring normalization worked correctly.\n",
        "\n",
        "Check the data type (should be **float32**) because most machine-learning models require floating-point inputs.\n",
        "  \n",
        "Check the image shape and channels to verify that every image has the same size (e.g., **512x512**) and three color channels (**RGB**).\n",
        "\n",
        "\n",
        "These checks guarantee that the dataset is standardized, consistent, and ready for modeling. It helps catch problems early â€” such as unnormalized pixels, grayscale images, or inconsistent image sizes â€” which could otherwise cause errors or poor model performance later."
      ],
      "metadata": {
        "id": "mvbQQkQpPAn9"
      }
    }
  ]
}