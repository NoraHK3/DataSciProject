{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "ALsaudi.ipynb",
      "authorship_tag": "ABX9TyMRkvykkO39lgllfJDcZ44s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoraHK3/DataSciProject/blob/main/ALsaudi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: Install & Base Setup ===\n",
        "!pip -q install playwright==1.46.0 requests nest_asyncio pillow\n",
        "!playwright install chromium\n",
        "\n",
        "import os, re, csv, json, unicodedata, asyncio\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "BASE_URL = \"https://www.alsaudi.sa\"\n",
        "LANG = \"ar\"\n",
        "CATEGORY_IDS = [3, 9, 8, 2, 19, 6, 5]  # your sections\n",
        "UA = (\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "      \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\")\n",
        "\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "def build_category_url(cat_id: int) -> str:\n",
        "    return f\"{BASE_URL}/menu/category/{cat_id}?language={LANG}\"\n",
        "\n",
        "def slugify(text, maxlen=90):\n",
        "    if not text: return \"image\"\n",
        "    t = unicodedata.normalize(\"NFKD\", text)\n",
        "    t = \"\".join(ch for ch in t if not unicodedata.combining(ch))\n",
        "    t = re.sub(r\"[^0-9A-Za-z\\u0600-\\u06FF _.-]+\", \"\", t)\n",
        "    t = re.sub(r\"\\s+\", \"_\", t.strip())\n",
        "    return (t[:maxlen] or \"image\")\n"
      ],
      "metadata": {
        "id": "S11DfOxqa4w8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G9yOse3Ja1wl"
      },
      "outputs": [],
      "source": [
        "# === Cell 2 (REPLACE): Saudi-focused classifier with Arabic-safe overrides ===\n",
        "import re, unicodedata\n",
        "\n",
        "def _norm_ar(s: str) -> str:\n",
        "    if not s: return \"\"\n",
        "    s = unicodedata.normalize(\"NFKD\", s)\n",
        "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
        "    return (s.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
        "             .replace(\"ى\",\"ي\").replace(\"ئ\",\"ي\").replace(\"ؤ\",\"و\")\n",
        "             .replace(\"ة\",\"ه\"))\n",
        "\n",
        "# Section fallback if nothing matches\n",
        "CATEGORY_FALLBACK = {\n",
        "    3: \"Appetizer/Side\",\n",
        "    9: \"Rice\",\n",
        "    8: \"Dessert\",\n",
        "    2: \"Rice - Chicken\",\n",
        "    19: \"Rice - Chicken\",\n",
        "    6: \"Rice\",\n",
        "    5: \"Meat\",\n",
        "}\n",
        "\n",
        "# Core triggers (compact, Saudi-focused)\n",
        "RICE     = r\"(رز|ارز|كبسه|برياني|بخاري|مندي|مظبي|مدفون|مضغوط|قوزي|سليق|منسف|مقلوبه|جريش|قرصان|مطازيز|مرقوق|هريس|كابلي|بشاور)\"\n",
        "CHICKEN  = r\"(دجاج|شوايه|شواية|مسحب|تكا)\"\n",
        "MEAT     = r\"(لحم|غنم|خروف|بقر|تيس|حاشي|جمل|كباب|كفته|كبد|مقلقل)\"\n",
        "SEAFOOD  = r\"(سمك|روبيان|جمبري|هامور)\"\n",
        "BREAD    = r\"(خبز|تميس)\"\n",
        "SALAD    = r\"(سلطه|سلطة|حمص|متبل|بابا ?غنوج|تبوله|تبولة|فتوش)\"\n",
        "SOUP     = r\"(شوربه|شوربة|حساء|مرق|ملوخيه|ملوخيh?|باميه|بامية|عدس)\"\n",
        "DESSERT  = r\"(حلى|حلويات|لقيمات|كنافه|كنافة|ام علي|معصوب|عريكه|رز ?بحليب|مهلبيه|مهلبية|كريم ?كراميل|جيلي|بقلاوه|بقلاوة)\"\n",
        "DRINKS   = r\"(عصير|مشروب|مشروبات|شاي|قهوه|قهوة|لبن|حليب)\"\n",
        "VEG      = r\"(خضار( ?مشكل)?|خيار|طماطم|باذنجان|كوسا)\"\n",
        "CHEESE   = r\"(جبن|جبنه|جبنة|قشطه|قشطة|لبنه|لبنة)\"\n",
        "\n",
        "STOPWORDS = r\"(نفر|فرد|حبه|حبة|نص|نصف|كامل[هة]?|ربع|صغير|كبير|ساده|سادة|وجبه|وجبة|علبه|علبة)\"\n",
        "\n",
        "def classify_food(name, extra=\"\", category_id=None):\n",
        "    text = _norm_ar(f\"{name or ''} {extra or ''}\")\n",
        "    text = re.sub(r\"[ـ،؛!؟:()\\[\\]{}«»\\\"'\\/\\\\\\-\\.]\", \" \", text)\n",
        "    text = re.sub(STOPWORDS, \" \", text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
        "\n",
        "    # ---- Arabic-safe hard overrides (substring checks, no \\b) ----\n",
        "    t = text  # alias\n",
        "    # exact short names / very common items\n",
        "    if \"خضار مشكل\" in t:           return \"Vegetables\"\n",
        "    if \"باميه\" in t:               return \"Soup/Stew\"\n",
        "    if \"ملوخيه\" in t:              return \"Soup/Stew\"\n",
        "    if \"كريم كراميل\" in t:         return \"Dessert\"\n",
        "    if \"جيلي\" in t:                return \"Dessert\"\n",
        "    if \"مهلبيه\" in t:              return \"Dessert\"\n",
        "    # sambosa cheese\n",
        "    if (\"سمبوس\" in t or \"سنبوس\" in t) and \"جبن\" in t:\n",
        "        return \"Starter - Cheese\"\n",
        "\n",
        "    labels = []\n",
        "\n",
        "    # Special rule: any sambosa → Starter (+ fillings below)\n",
        "    if \"سمبوس\" in t or \"سنبوس\" in t:\n",
        "        labels.append(\"Starter\")\n",
        "        if re.search(CHEESE, t): labels.append(\"Cheese\")\n",
        "        if re.search(MEAT,   t): labels.append(\"Meat\")\n",
        "        if re.search(VEG,    t): labels.append(\"Vegetables\")\n",
        "        return \" - \".join(dict.fromkeys(labels))\n",
        "\n",
        "    # Main ingredient groups\n",
        "    if re.search(RICE, t):    labels.append(\"Rice\")\n",
        "    if re.search(CHICKEN, t): labels.append(\"Chicken\")\n",
        "    if re.search(MEAT, t):    labels.append(\"Meat\")\n",
        "    if re.search(SEAFOOD, t): labels.append(\"Fish/Seafood\")\n",
        "    if re.search(BREAD, t):   labels.append(\"Bread\")\n",
        "    if re.search(SALAD, t):   labels.append(\"Appetizer/Side\")\n",
        "    if re.search(SOUP, t):    labels.append(\"Soup/Stew\")\n",
        "    if re.search(DESSERT, t): labels.append(\"Dessert\")\n",
        "    if re.search(DRINKS, t):  labels.append(\"Drinks\")\n",
        "    if re.search(VEG, t):     labels.append(\"Vegetables\")\n",
        "    if re.search(CHEESE, t):  labels.append(\"Cheese\")\n",
        "\n",
        "    # Section fallback if still nothing\n",
        "    if not labels and category_id in CATEGORY_FALLBACK:\n",
        "        labels.append(CATEGORY_FALLBACK[category_id])\n",
        "\n",
        "    return \" - \".join(dict.fromkeys(labels)) if labels else \"Unclassified\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3 (REPLACE): Playwright scraping helpers with better name fallback ===\n",
        "async def download_image_with_context(context, img_url, dish_name):\n",
        "    \"\"\"Download image using Playwright context (keeps headers).\"\"\"\n",
        "    if not img_url:\n",
        "        return \"\"\n",
        "    url = img_url if img_url.startswith(\"http\") else urljoin(BASE_URL, img_url)\n",
        "    try:\n",
        "        resp = await context.request.get(url)\n",
        "        if not resp.ok:\n",
        "            return \"\"\n",
        "        data = await resp.body()\n",
        "        ext = os.path.splitext(urlparse(url).path)[1].lower() or \".jpg\"\n",
        "        if ext not in [\".jpg\", \".jpeg\", \".png\", \".webp\"]:\n",
        "            ext = \".jpg\"\n",
        "        fname = f\"images/{slugify(dish_name or 'item')}{ext}\"\n",
        "        # avoid overwriting duplicate names\n",
        "        base, e = os.path.splitext(fname)\n",
        "        i = 1\n",
        "        while os.path.exists(fname):\n",
        "            fname = f\"{base}_{i}{e}\"\n",
        "            i += 1\n",
        "        with open(fname, \"wb\") as f:\n",
        "            f.write(data)\n",
        "        return fname\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "async def scrape_one_category(context, category_url: str, category_id: int):\n",
        "    \"\"\"Scrape a single category page.\"\"\"\n",
        "    rows = []\n",
        "    page = await context.new_page()\n",
        "    await page.goto(category_url, wait_until=\"domcontentloaded\", timeout=60000)\n",
        "\n",
        "    # Scroll to load lazy items\n",
        "    last = 0\n",
        "    for _ in range(20):\n",
        "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "        await page.wait_for_timeout(800)\n",
        "        h = await page.evaluate(\"document.body.scrollHeight\")\n",
        "        if h == last:\n",
        "            break\n",
        "        last = h\n",
        "\n",
        "    # Find cards\n",
        "    cards = await page.query_selector_all(\"div.menu-list article.card-wrapper\")\n",
        "    print(f\"[{category_url}] Cards found:\", len(cards))\n",
        "\n",
        "    for c in cards:\n",
        "        # Dish name with robust fallbacks\n",
        "        name = \"\"\n",
        "        a = await c.query_selector(\"a.menu-product.card\")\n",
        "        if a:\n",
        "            name = (await a.get_attribute(\"title\")) or \"\"\n",
        "            if not name:\n",
        "                try:\n",
        "                    name = (await a.inner_text()) or \"\"\n",
        "                except:\n",
        "                    name = \"\"\n",
        "            name = name.strip()\n",
        "\n",
        "        # Fallback: image alt\n",
        "        if not name:\n",
        "            img_for_alt = await c.query_selector(\"img\")\n",
        "            if img_for_alt:\n",
        "                alt_txt = await img_for_alt.get_attribute(\"alt\")\n",
        "                if alt_txt:\n",
        "                    name = alt_txt.strip()\n",
        "\n",
        "        # Image URL\n",
        "        img_url = \"\"\n",
        "        img = await c.query_selector(\"img\")\n",
        "        if img:\n",
        "            for attr in [\"src\",\"data-src\",\"data-original\",\"data-lazy\",\"srcset\"]:\n",
        "                v = await img.get_attribute(attr)\n",
        "                if v and v.strip():\n",
        "                    if attr == \"srcset\":\n",
        "                        img_url = v.split(\",\")[0].strip().split(\" \")[0]\n",
        "                    else:\n",
        "                        img_url = v.strip()\n",
        "                    break\n",
        "\n",
        "        # Keep full card text as extra context for classification\n",
        "        try:\n",
        "            extra = (await c.inner_text()) or \"\"\n",
        "        except:\n",
        "            extra = \"\"\n",
        "\n",
        "        rows.append({\n",
        "            \"name\": name,\n",
        "            \"img_url\": img_url,\n",
        "            \"extra\": extra,\n",
        "            \"category_id\": category_id\n",
        "        })\n",
        "\n",
        "    await page.close()\n",
        "    return rows\n",
        "\n",
        "async def scrape_many_categories(category_ids):\n",
        "    \"\"\"Launch browser once, scrape all categories, classify + download.\"\"\"\n",
        "    final = []\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True, args=[\"--no-sandbox\"])\n",
        "        context = await browser.new_context(\n",
        "            locale=\"ar-SA\", user_agent=UA, viewport={\"width\":1366,\"height\":2200}\n",
        "        )\n",
        "\n",
        "        all_rows = []\n",
        "        for cid in category_ids:\n",
        "            url = build_category_url(cid)\n",
        "            try:\n",
        "                rows = await scrape_one_category(context, url, cid)\n",
        "                all_rows.extend(rows)\n",
        "            except Exception as e:\n",
        "                print(f\"Category {cid} failed: {e}\")\n",
        "\n",
        "        # Finalize rows (classify + download)\n",
        "        scrape_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "        for r in all_rows:\n",
        "            cls = classify_food(r[\"name\"], r.get(\"extra\",\"\"), category_id=r[\"category_id\"])\n",
        "            img_path = await download_image_with_context(context, r[\"img_url\"], r[\"name\"] or \"item\")\n",
        "            final.append({\n",
        "                \"name\": r[\"name\"],\n",
        "                \"image_file\": img_path,\n",
        "                \"classification\": cls,\n",
        "                \"scrape_date\": scrape_date\n",
        "            })\n",
        "\n",
        "        await context.close()\n",
        "        await browser.close()\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "ZCy65DwcbCOH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Save outputs ===\n",
        "import csv, json\n",
        "\n",
        "OUT_JSON = \"alsaudi_menu.json\"\n",
        "OUT_CSV  = \"alsaudi_menu.csv\"\n",
        "\n",
        "final = await scrape_many_categories(CATEGORY_IDS)\n",
        "\n",
        "print(f\"Total items scraped: {len(final)}\")\n",
        "missing_img = sum(1 for r in final if not r.get(\"image_file\"))\n",
        "print(f\"Images saved: {len(final)-missing_img} | Missing images: {missing_img}\")\n",
        "\n",
        "# JSON\n",
        "with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(final, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# CSV\n",
        "fieldnames = [\"name\", \"image_file\", \"classification\", \"scrape_date\"]\n",
        "with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "    w.writeheader()\n",
        "    for row in final:\n",
        "        w.writerow({k: row.get(k, \"\") for k in fieldnames})\n",
        "\n",
        "print(f\"Saved → {OUT_JSON} & {OUT_CSV}. Images in ./images/\")\n"
      ],
      "metadata": {
        "id": "aEAyDMMUeJXS",
        "outputId": "9ddcde7b-f4a6-44f3-9df5-2bb280f6ba7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[https://www.alsaudi.sa/menu/category/3?language=ar] Cards found: 9\n",
            "[https://www.alsaudi.sa/menu/category/9?language=ar] Cards found: 12\n",
            "[https://www.alsaudi.sa/menu/category/8?language=ar] Cards found: 7\n",
            "[https://www.alsaudi.sa/menu/category/2?language=ar] Cards found: 17\n",
            "[https://www.alsaudi.sa/menu/category/19?language=ar] Cards found: 7\n",
            "[https://www.alsaudi.sa/menu/category/6?language=ar] Cards found: 10\n",
            "[https://www.alsaudi.sa/menu/category/5?language=ar] Cards found: 5\n",
            "Total items scraped: 67\n",
            "Images saved: 67 | Missing images: 0\n",
            "Saved → alsaudi_menu.json & alsaudi_menu.csv. Images in ./images/\n"
          ]
        }
      ]
    }
  ]
}