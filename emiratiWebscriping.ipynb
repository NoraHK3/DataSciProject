{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMb7VcqRHr/aZcAEQf8umDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoraHK3/DataSciProject/blob/main/emiratiWebscriping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tks2Vf4cTY9L",
        "outputId": "8b376174-976c-4375-aece-dc046d2d345f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1952213176.py:170: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"date_scraped\": datetime.utcnow().date().isoformat(),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[dish] Luqaimat  (https://arabianteahouse.me/luqaimat/)\n",
            "[dish] Kunafa  (https://arabianteahouse.me/kunafa/)\n",
            "[dish] Umm ALI  (https://arabianteahouse.me/umm-ali/)\n",
            "[dish] Chicken Biryani  (https://arabianteahouse.me/chicken-biryani/)\n",
            "[dish] Chicken Machboos  (https://arabianteahouse.me/chicken-machboos/)\n",
            "[dish] Beef Machboos  (https://arabianteahouse.me/beef-machboos/)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[skip] https://arabianteahouse.me/ar-home/dhmontenegro20@gmail.com -> 404 Client Error: Not Found for url: https://arabianteahouse.me/ar-home/dhmontenegro20@gmail.com\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[dish] Liver Hummus  (https://arabianteahouse.me/liver-ummus/)\n",
            "[checkpoint] pages=50 dishes=7\n",
            "[done] visited=50 unique_dishes=7\n"
          ]
        }
      ],
      "source": [
        "# Colab-ready site-wide dish scraper for arabianteahouse.me\n",
        "# Output: dishes_all.json + dishes_all.csv\n",
        "# Fields: dish_name, labels, image_url, date_scraped, source_url\n",
        "# Notes:\n",
        "# - Crawls arabianteahouse.me, detects \"Ingredients\" sections to identify dish pages.\n",
        "# - Throttled & domain-restricted. Adjust LIMITS to cover more pages if needed.\n",
        "\n",
        "import requests, re, time, json, pandas as pd, sys\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "START_URLS = [\n",
        "    \"https://arabianteahouse.me/\",                         # home\n",
        "    \"https://arabianteahouse.me/menu/\",                    # menu hub (if exists)\n",
        "    \"https://arabianteahouse.me/podgorica-branch/\",        # example branch\n",
        "]\n",
        "ALLOWED_DOMAIN = \"arabianteahouse.me\"\n",
        "\n",
        "# ---- LIMITS / TUNABLES ----\n",
        "MAX_PAGES_TO_VISIT = 2500      # raise if you want deeper coverage\n",
        "REQUEST_TIMEOUT = 25\n",
        "CRAWL_DELAY_SEC = 0.6          # be polite\n",
        "SAVE_EVERY = 50                # checkpoint frequency\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# A lightweight vocabulary to normalize \"main\" ingredients -> labels\n",
        "# Extend freely for better recall\n",
        "INGREDIENT_VOCAB = {\n",
        "    # staples\n",
        "    \"rice\",\"basmati\",\"bread\",\"vermicelli\",\"noodles\",\"bulgur\",\"couscous\",\n",
        "    # proteins\n",
        "    \"chicken\",\"meat\",\"beef\",\"lamb\",\"mutton\",\"fish\",\"shrimp\",\"prawn\",\"egg\",\"eggs\",\n",
        "    # veg & legumes\n",
        "    \"tomato\",\"tomatoes\",\"onion\",\"onions\",\"garlic\",\"ginger\",\"potato\",\"potatoes\",\n",
        "    \"chickpeas\",\"lentils\",\"okra\",\"eggplant\",\"zucchini\",\"pepper\",\"green chili\",\n",
        "    # herbs\n",
        "    \"coriander\",\"cilantro\",\"parsley\",\"mint\",\"dill\",\n",
        "    # dairy & sauces\n",
        "    \"yogurt\",\"laban\",\"cream\",\"tomato paste\",\"tomato sauce\",\"tahini\",\"ghee\",\"milk\",\n",
        "    # spices\n",
        "    \"cumin\",\"coriander powder\",\"turmeric\",\"cardamom\",\"cinnamon\",\"cloves\",\n",
        "    \"black pepper\",\"bay leaf\",\"bay leaves\",\"sumac\",\"zaatar\",\"saffron\",\n",
        "    \"paprika\",\"chili\",\"red chili\",\n",
        "    # oils & basics\n",
        "    \"oil\",\"olive oil\",\"vegetable oil\",\"salt\",\"water\",\"vinegar\",\"lemon\",\"lime\"\n",
        "}\n",
        "\n",
        "SKIP_SUBSTRINGS = [\n",
        "    # obvious non-content / utility pages to skip enqueuing\n",
        "    \"/wp-json\", \"/feed\", \"/xmlrpc\", \"/tag/\", \"/author/\", \"/category/\",\n",
        "    \"/privacy\", \"/terms\", \"/contact\", \"/reservation\", \"/book\", \"/cart\",\n",
        "    \"/checkout\", \"/my-account\", \"/login\"\n",
        "]\n",
        "\n",
        "def same_domain(url):\n",
        "    try:\n",
        "        return urlparse(url).netloc.endswith(ALLOWED_DOMAIN)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def get_soup(url):\n",
        "    r = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
        "    r.raise_for_status()\n",
        "    return BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "def looks_like_dish_page(soup):\n",
        "    \"\"\"Heuristic: presence of an 'Ingredients' header with following list items.\"\"\"\n",
        "    hdr = soup.find(lambda t: t and t.name in (\"h2\",\"h3\",\"h4\") and \"ingredients\" in t.get_text(strip=True).lower())\n",
        "    if not hdr:\n",
        "        return False\n",
        "    ul = hdr.find_next([\"ul\",\"ol\"])\n",
        "    return bool(ul and ul.find_all(\"li\"))\n",
        "\n",
        "def extract_ingredients(soup):\n",
        "    hdr = soup.find(lambda t: t and t.name in (\"h2\",\"h3\",\"h4\") and \"ingredients\" in t.get_text(strip=True).lower())\n",
        "    if not hdr:\n",
        "        return []\n",
        "    ul = hdr.find_next([\"ul\",\"ol\"])\n",
        "    if not ul:\n",
        "        return []\n",
        "    out = []\n",
        "    for li in ul.find_all(\"li\"):\n",
        "        txt = re.sub(r\"\\s+\", \" \", li.get_text(\" \", strip=True))\n",
        "        out.append(txt)\n",
        "    return out\n",
        "\n",
        "def normalize_labels(ingredients_list):\n",
        "    labels = set()\n",
        "    for line in ingredients_list:\n",
        "        low = line.lower()\n",
        "        for token in sorted(INGREDIENT_VOCAB, key=lambda x: -len(x)):\n",
        "            if token in low:\n",
        "                normalized = (token\n",
        "                              .replace(\"tomatoes\",\"tomato\")\n",
        "                              .replace(\"onions\",\"onion\")\n",
        "                              .replace(\"potatoes\",\"potato\")\n",
        "                              .replace(\"eggs\",\"egg\")\n",
        "                              .replace(\"bay leaves\",\"bay leaf\")\n",
        "                              .replace(\"prawns\",\"shrimp\"))\n",
        "                labels.add(normalized)\n",
        "    # prefer specific over generic\n",
        "    if \"coriander powder\" in labels and \"coriander\" in labels:\n",
        "        labels.discard(\"coriander\")\n",
        "    return sorted(labels)\n",
        "\n",
        "def extract_title(soup):\n",
        "    for sel in [\"h1\", \"header h1\", \"article h1\", \"title\"]:\n",
        "        h = soup.select_one(sel)\n",
        "        if h:\n",
        "            return h.get_text(strip=True)\n",
        "    return \"\"\n",
        "\n",
        "def extract_image_url(soup):\n",
        "    # Prefer og:image if present\n",
        "    og = soup.find(\"meta\", property=\"og:image\")\n",
        "    if og and og.get(\"content\"):\n",
        "        return og[\"content\"]\n",
        "    # Fallback to first image in main content\n",
        "    content = soup.find(\"div\", class_=re.compile(r\"(entry|post|content)\", re.I)) or soup\n",
        "    img = content.find(\"img\")\n",
        "    if img and img.get(\"src\"):\n",
        "        return img[\"src\"]\n",
        "    # Final fallback: any image\n",
        "    img = soup.find(\"img\")\n",
        "    return img[\"src\"] if img and img.get(\"src\") else \"\"\n",
        "\n",
        "def is_skippable_link(href):\n",
        "    if not href:\n",
        "        return True\n",
        "    if not same_domain(href):\n",
        "        return True\n",
        "    low = href.lower()\n",
        "    if any(s in low for s in SKIP_SUBSTRINGS):\n",
        "        return True\n",
        "    # avoid media files and fragments\n",
        "    if any(low.endswith(ext) for ext in [\".jpg\",\".jpeg\",\".png\",\".gif\",\".webp\",\".pdf\",\".svg\",\".mp4\",\".zip\"]):\n",
        "        return True\n",
        "    if \"#\" in low:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def extract_links(current_url, soup):\n",
        "    links = set()\n",
        "    for a in soup.select(\"a[href]\"):\n",
        "        href = a.get(\"href\", \"\").strip()\n",
        "        if not href:\n",
        "            continue\n",
        "        href = urljoin(current_url, href)\n",
        "        if not is_skippable_link(href):\n",
        "            links.add(href)\n",
        "    return links\n",
        "\n",
        "def scrape_dish(url, soup=None):\n",
        "    if soup is None:\n",
        "        soup = get_soup(url)\n",
        "    title = extract_title(soup)\n",
        "    ingredients = extract_ingredients(soup)\n",
        "    labels = normalize_labels(ingredients)\n",
        "    image_url = extract_image_url(soup)\n",
        "    return {\n",
        "        \"dish_name\": title,\n",
        "        \"labels\": labels,\n",
        "        \"image_url\": image_url,\n",
        "        \"date_scraped\": datetime.utcnow().date().isoformat(),\n",
        "        \"source_url\": url\n",
        "    }\n",
        "\n",
        "def checkpoint(data):\n",
        "    # Save JSON\n",
        "    with open(\"dishes_all.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "    # Save CSV\n",
        "    df = pd.DataFrame(data)\n",
        "    if not df.empty:\n",
        "        df[\"labels\"] = df[\"labels\"].apply(lambda xs: \";\".join(xs))\n",
        "    df.to_csv(\"dishes_all.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "def crawl():\n",
        "    from collections import deque\n",
        "    queue = deque(START_URLS)\n",
        "    visited = set()\n",
        "    results = []\n",
        "    seen_sources = set()  # dedupe by URL\n",
        "    pages_seen = 0\n",
        "\n",
        "    while queue and pages_seen < MAX_PAGES_TO_VISIT:\n",
        "        url = queue.popleft()\n",
        "        if url in visited:\n",
        "            continue\n",
        "        visited.add(url)\n",
        "\n",
        "        try:\n",
        "            soup = get_soup(url)\n",
        "        except Exception as e:\n",
        "            # print errors but keep going\n",
        "            print(f\"[skip] {url} -> {e}\", file=sys.stderr)\n",
        "            continue\n",
        "\n",
        "        pages_seen += 1\n",
        "        # If page is a dish, extract\n",
        "        if looks_like_dish_page(soup):\n",
        "            try:\n",
        "                item = scrape_dish(url, soup)\n",
        "                if item[\"dish_name\"] and item[\"labels\"]:\n",
        "                    if url not in seen_sources:\n",
        "                        results.append(item)\n",
        "                        seen_sources.add(url)\n",
        "                        print(f\"[dish] {item['dish_name']}  ({url})\")\n",
        "            except Exception as e:\n",
        "                print(f\"[dish-error] {url} -> {e}\", file=sys.stderr)\n",
        "\n",
        "        # Enqueue more links\n",
        "        for link in extract_links(url, soup):\n",
        "            if link not in visited:\n",
        "                queue.append(link)\n",
        "\n",
        "        # throttle\n",
        "        time.sleep(CRAWL_DELAY_SEC)\n",
        "\n",
        "        # periodic checkpoint\n",
        "        if pages_seen % SAVE_EVERY == 0:\n",
        "            print(f\"[checkpoint] pages={pages_seen} dishes={len(results)}\")\n",
        "            checkpoint(results)\n",
        "\n",
        "    # final save\n",
        "    checkpoint(results)\n",
        "    print(f\"[done] visited={pages_seen} unique_dishes={len(results)}\")\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    crawl()\n"
      ]
    }
  ]
}