{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpP5q/ff/aV58gGkpKT4Db",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoraHK3/DataSciProject/blob/main/DataSets/Shawayahouse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries (Playwright, requests, nest_asyncio, Pillow)\n",
        "!pip -q install playwright==1.46.0 requests nest_asyncio pillow\n",
        "# Install the Chromium browser for Playwright\n",
        "!playwright install chromium\n",
        "\n",
        "# ===== Imports =====\n",
        "import os, re, csv, json, time, unicodedata, asyncio, requests\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()   # allow nested asyncio loops (needed in Colab)\n",
        "\n",
        "from urllib import robotparser              # for robots.txt checking\n",
        "from playwright.async_api import async_playwright  # Playwright (async)\n",
        "from PIL import Image                       # to process images\n",
        "from io import BytesIO                      # to handle image bytes\n",
        "\n",
        "# ---------- Config ----------\n",
        "BASE_URL = \"https://shawayahouse.my.taker.io\"   # website root\n",
        "MENU_ROOT = f\"{BASE_URL}/menu?language=ar\"      # Arabic menu root page\n",
        "UA = (\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "      \"(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\")  # fake browser agent\n",
        "IMAGES_DIR = \"images\"                           # local folder for images\n",
        "os.makedirs(IMAGES_DIR, exist_ok=True)          # create folder if not exists\n",
        "# ----------------------------\n",
        "\n",
        "# ---------- Utils ----------\n",
        "\n",
        "# Regex to match Arabic diacritics and tatweel (elongation mark)\n",
        "AR_DIACRITICS = re.compile(r\"[\\u064B-\\u0652\\u0670\\u0640]\")\n",
        "\n",
        "def ar_clean(text):\n",
        "    \"\"\"Normalize Arabic text: remove diacritics, tatweel, symbols, extra spaces.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    t = unicodedata.normalize(\"NFKC\", text)\n",
        "    t = AR_DIACRITICS.sub(\"\", t)                    # remove diacritics & tatweel\n",
        "    t = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \" \", t)     # keep only words & Arabic range\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()              # collapse multiple spaces\n",
        "    return t\n",
        "\n",
        "def slugify(text, maxlen=80):\n",
        "    \"\"\"Convert dish name to safe filename (Arabic/English).\"\"\"\n",
        "    if not text: return \"image\"\n",
        "    t = unicodedata.normalize(\"NFKD\", text)\n",
        "    t = \"\".join(ch for ch in t if not unicodedata.combining(ch))  # remove accents\n",
        "    t = re.sub(r\"[^0-9A-Za-z\\u0600-\\u06FF _.-]+\", \"\", t)          # keep safe chars\n",
        "    t = re.sub(r\"\\s+\", \"_\", t.strip())                            # replace spaces with _\n",
        "    return (t[:maxlen] or \"image\")\n",
        "\n",
        "def robots_allows(url, user_agent=\"*\"):\n",
        "    \"\"\"Check robots.txt to see if scraping is allowed.\"\"\"\n",
        "    rp = robotparser.RobotFileParser()\n",
        "    rp.set_url(urljoin(BASE_URL, \"/robots.txt\"))\n",
        "    try:\n",
        "        rp.read()\n",
        "    except:\n",
        "        return True          # if can't read robots, allow by default\n",
        "    return rp.can_fetch(user_agent, url)\n",
        "\n",
        "def ensure_abs(url):\n",
        "    \"\"\"Make sure URL is absolute (prefix with BASE_URL if relative).\"\"\"\n",
        "    return url if bool(urlparse(url).netloc) else urljoin(BASE_URL, url)\n",
        "\n",
        "def download_image(img_url, name_hint, referer):\n",
        "    \"\"\"Download image, detect format, save to IMAGES_DIR, return local file path.\"\"\"\n",
        "    if not img_url: return \"\"\n",
        "    img_url = ensure_abs(img_url)\n",
        "    headers = {\"User-Agent\": UA, \"Referer\": referer}\n",
        "    r = requests.get(img_url, headers=headers, timeout=60, stream=True)\n",
        "    r.raise_for_status()\n",
        "    data = r.content\n",
        "    # detect extension by content\n",
        "    try:\n",
        "        im = Image.open(BytesIO(data))\n",
        "        ext = (im.format or \"JPEG\").lower()\n",
        "        if ext == \"jpeg\": ext = \"jpg\"\n",
        "        if ext not in (\"jpg\",\"png\",\"webp\"): ext = \"jpg\"\n",
        "    except Exception:\n",
        "        ext = \"jpg\"\n",
        "    fn = f\"{slugify(name_hint)}.{ext}\"\n",
        "    out_path = os.path.join(IMAGES_DIR, fn)\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        f.write(data)\n",
        "    return out_path\n",
        "\n",
        "# ---------- Classification ----------\n",
        "# Regex rules (Arabic + English keywords) for dish categories\n",
        "CLASS_MAP = [\n",
        "    (r\"(رز|ارز|كبسه|كبسة|برياني|مندي|mandi|kabsa|rice|biryani|جريش|قرصان|هريس)\", \"Rice\"),\n",
        "    (r\"(سلطه|سلطة|تبوله|تبولة|فتوش|salad|tabbouleh|fattoush|كولسلو)\", \"Salad\"),\n",
        "    (r\"(دجاج|مسحب|شوايه|شواية|chicken|tawook|shawaya|broast)\", \"Chicken\"),\n",
        "    (r\"(لحم|بقري|كباب|برجر|burger|beef)\", \"Meat\"),\n",
        "    (r\"(غنم|ضأن|lamb|mutton|حاشي|camel)\", \"Lamb\"),\n",
        "    (r\"(سمك|هامور|fish|روبيان|shrimp|جمبري)\", \"Fish\"),\n",
        "    (r\"(خبز|صامولي|عربي|pita|tamees|tortilla|saj|bread)\", \"Bread\"),\n",
        "    (r\"(بطاطس|بطاطا|فرايز|fries)\", \"Fries\"),\n",
        "    (r\"(صلصه|صلصة|صوص|ثوم|طحينه|طحينة|مايونيز|كاتشب|sauce|garlic|tahini|ketchup)\", \"Sauce\"),\n",
        "    # Desserts (includes Om Ali, Muhallabia)\n",
        "    (r\"(حلى|حلويات|كنافه|كنافة|لقيمات|بسبوسه|بسبوسة|معمول|dessert|kunafa|basbousa|maamoul|ام_علي|أم_علي|ام علي|مهلبيه|مهلبية)\", \"Dessert\"),\n",
        "    (r\"(مكرونه|مكرونة|باستا|pasta)\", \"Pasta\"),\n",
        "    (r\"(شاورما|shawarma|راب|wrap|سندوتش|سندويتش|ساندوتش|ساندويتش)\", \"Sandwich\"),\n",
        "    # Sides / Vegetables (bashamel, molokhia, mixed veg, moussaka)\n",
        "    (r\"(بشاميل|ملوخيه|ملوخية|خضار مشكل|خضار_مشكل|مصقعه|مصقعة|موزه|موزة)\", \"Sides\"),\n",
        "    # Drinks (water, laban/milk, soft drinks in many spellings)\n",
        "    (r\"(مويه|موية|ماء|مياه|معدنيه|معدنية|مياه معدنية|مياه_معدنيه|لبن|لَبَن|laban|milk|مشروبات غازيه|مشروبات غازية|بيبسي|pepsi|كوكاكولا|coca[- ]?cola|سفن اب|7up|sprite|سبرايت|fanta|فانتا|cola|coke)\", \"Drinks\"),\n",
        "]\n",
        "\n",
        "# Section-based hints (helps classification when category name contains keywords)\n",
        "SECTION_HINTS = [\n",
        "    (r\"(السلطات|سلطات|سلطه|سلطة)\", \"Salad\"),\n",
        "    (r\"(الحلى|حلويات)\", \"Dessert\"),\n",
        "    (r\"(السندوتشات|السندويتشات|الشاورما|ساندويتش|رابس?)\", \"Sandwich\"),\n",
        "    (r\"(الاطباق الرئيسية|الرئيسية|الرز|الارز|المنسف|الكبسه)\", \"Rice\"),\n",
        "    (r\"(المشويات|الشوي|grill|مشوي)\", \"Meat\"),\n",
        "    (r\"(البطاطس|الفرايز)\", \"Fries\"),\n",
        "    (r\"(الصوص|الصلصات)\", \"Sauce\"),\n",
        "]\n",
        "\n",
        "def classify_food(name=\"\", desc=\"\", section_name=\"\"):\n",
        "    \"\"\"Return category label(s) for a dish by matching regex on name/desc/section.\"\"\"\n",
        "    text = ar_clean(f\"{name} {desc}\")\n",
        "    sec  = ar_clean(section_name)\n",
        "    labels = set()\n",
        "    # check section hints first\n",
        "    for pat, lab in SECTION_HINTS:\n",
        "        if re.search(pat, sec, flags=re.IGNORECASE):\n",
        "            labels.add(lab)\n",
        "    # check name + description\n",
        "    for pat, lab in CLASS_MAP:\n",
        "        if re.search(pat, text, flags=re.IGNORECASE):\n",
        "            labels.add(lab)\n",
        "    return \" - \".join(labels) if labels else \"Unclassified\"\n",
        "\n",
        "# ---------- Scraping helpers ----------\n",
        "\n",
        "async def scroll_to_bottom(page, max_rounds=20, pause=800):\n",
        "    \"\"\"Scroll down repeatedly to trigger lazy loading.\"\"\"\n",
        "    last = 0\n",
        "    for _ in range(max_rounds):\n",
        "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
        "        await page.wait_for_timeout(pause)\n",
        "        h = await page.evaluate(\"document.body.scrollHeight\")\n",
        "        if h == last: break\n",
        "        last = h\n",
        "\n",
        "async def get_all_category_links(context):\n",
        "    \"\"\"Extract all category links from the main menu page.\"\"\"\n",
        "    page = await context.new_page()\n",
        "    await page.goto(MENU_ROOT, wait_until=\"domcontentloaded\", timeout=60000)\n",
        "    await scroll_to_bottom(page, max_rounds=10)\n",
        "    anchors = await page.query_selector_all('a[href*=\"/menu/category/\"]')\n",
        "    links = set()\n",
        "    for a in anchors:\n",
        "        href = await a.get_attribute(\"href\")\n",
        "        if href and \"/menu/category/\" in href:\n",
        "            # unify URL format & force Arabic language param\n",
        "            links.add(ensure_abs(href.split(\"?\")[0] + \"?language=ar\"))\n",
        "    await page.close()\n",
        "    return sorted(links)\n",
        "\n",
        "async def scrape_category(context, category_url):\n",
        "    \"\"\"Scrape one category page: extract product names, images, dish URL, classify.\"\"\"\n",
        "    rows = []\n",
        "    page = await context.new_page()\n",
        "    await page.goto(category_url, wait_until=\"domcontentloaded\", timeout=60000)\n",
        "    await scroll_to_bottom(page)\n",
        "    section_name = (await page.title()) or \"\"   # use section title for hints\n",
        "\n",
        "    # product cards (most reliable: links to product pages)\n",
        "    cards = await page.query_selector_all('a[href*=\"/menu/product\"]')\n",
        "    if not cards:\n",
        "        # fallback selectors\n",
        "        cards = await page.query_selector_all(\"a.menu-product, a.card, a[href*='product']\")\n",
        "    imgs = await page.query_selector_all(\"img\")  # fallback: any img with alt/name\n",
        "\n",
        "    # loop through product links\n",
        "    for a in cards:\n",
        "        name = (await a.get_attribute(\"title\")) or (await a.inner_text() or \"\")\n",
        "        name = ar_clean(name)\n",
        "\n",
        "        # NEW: get product URL from the anchor\n",
        "        href = await a.get_attribute(\"href\")\n",
        "        dish_url = ensure_abs(href) if href else category_url  # fallback to category page\n",
        "\n",
        "        img_url = \"\"\n",
        "        img = await a.query_selector(\"img\")\n",
        "        if not img:\n",
        "            # fallback: find img in parent card\n",
        "            img = await a.evaluate_handle(\n",
        "                \"el => el.closest('article,div,li,section') && el.closest('article,div,li,section').querySelector('img')\"\n",
        "            )\n",
        "        if img:\n",
        "            try:\n",
        "                if hasattr(img, \"get_attribute\"):\n",
        "                    for attr in [\"src\",\"data-src\",\"data-original\",\"data-lazy\",\"data-srcset\",\"srcset\"]:\n",
        "                        v = await img.get_attribute(attr)\n",
        "                        if v and v.strip():\n",
        "                            img_url = v.split()[0].strip(); break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if name and img_url:\n",
        "            rows.append({\n",
        "                \"name\": name,\n",
        "                \"img_url\": img_url,\n",
        "                \"url\": dish_url,                 # keep dish URL\n",
        "                \"section\": section_name,\n",
        "                \"classification\": classify_food(name=name, section_name=section_name)\n",
        "            })\n",
        "\n",
        "    # fallback: use <img alt=\"...\"> if nothing found\n",
        "    if not rows and imgs:\n",
        "        for img in imgs:\n",
        "            alt = ar_clean((await img.get_attribute(\"alt\") or \"\"))\n",
        "            if not alt: continue\n",
        "            src = \"\"\n",
        "            for attr in [\"src\",\"data-src\",\"data-original\",\"data-lazy\",\"data-srcset\",\"srcset\"]:\n",
        "                v = await img.get_attribute(attr)\n",
        "                if v and v.strip():\n",
        "                    src = v.split()[0].strip(); break\n",
        "            if alt and src:\n",
        "                rows.append({\n",
        "                    \"name\": alt,\n",
        "                    \"img_url\": src,\n",
        "                    \"url\": category_url,          # fallback to category page\n",
        "                    \"section\": section_name,\n",
        "                    \"classification\": classify_food(name=alt, section_name=section_name)\n",
        "                })\n",
        "\n",
        "    await page.close()\n",
        "    return rows\n",
        "\n",
        "async def run_all():\n",
        "    \"\"\"Run scraping for all categories.\"\"\"\n",
        "    if not robots_allows(MENU_ROOT, user_agent=\"*\"):\n",
        "        print(\"[robots] Not allowed. Stop.\")\n",
        "        return []\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True, args=[\"--no-sandbox\"])\n",
        "        context = await browser.new_context(locale=\"ar-SA\", user_agent=UA)\n",
        "\n",
        "        # 1) collect category links\n",
        "        cats = await get_all_category_links(context)\n",
        "        print(f\"[info] Categories found: {len(cats)}\")\n",
        "        for c in cats: print(\" -\", c)\n",
        "\n",
        "        # 2) scrape each category\n",
        "        all_rows = []\n",
        "        for c in cats:\n",
        "            try:\n",
        "                rows = await scrape_category(context, c)\n",
        "                print(f\"[info] {c} -> {len(rows)} items\")\n",
        "                all_rows.extend(rows)\n",
        "            except Exception as e:\n",
        "                print(\"[warn] category failed:\", c, e)\n",
        "\n",
        "        await context.close()\n",
        "        await browser.close()\n",
        "    return all_rows\n",
        "\n",
        "# ---------- Run ----------\n",
        "rows = asyncio.get_event_loop().run_until_complete(run_all())\n",
        "print(\"Total parsed rows:\", len(rows))\n",
        "print(rows[:5])\n",
        "\n",
        "# ---------- Save to CSV/JSON ----------\n",
        "final = []\n",
        "scrape_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "for r in rows:\n",
        "    img_path = download_image(r[\"img_url\"], r[\"name\"] or \"item\", MENU_ROOT)\n",
        "    final.append({\n",
        "        \"name\": r[\"name\"],\n",
        "        \"image_file\": img_path,\n",
        "        \"classification\": r[\"classification\"],\n",
        "        \"scrape_date\": scrape_date,\n",
        "        \"dish_url\": r.get(\"url\", \"\")   # NEW: include dish URL in outputs\n",
        "    })\n",
        "\n",
        "# Save to CSV (adds dish_url column)\n",
        "with open(\"dishes.csv\",\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
        "    w = csv.DictWriter(f, fieldnames=[\"name\",\"image_file\",\"classification\",\"scrape_date\",\"dish_url\"])\n",
        "    w.writeheader()\n",
        "    w.writerows(final)\n",
        "\n",
        "# Save to JSON\n",
        "with open(\"dishes.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump(final, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved {len(final)} items → dishes.csv & dishes.json  |  Images → {IMAGES_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bocp1r63k1Wc",
        "outputId": "50ea21c0-94e3-4ae1-c16c-4aee84793fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.9/37.9 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m625.7/625.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Chromium 128.0.6613.18 (playwright build v1129)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1129/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G162.8 MiB [] 0% 10.8s\u001b[0K\u001b[1G162.8 MiB [] 0% 29.7s\u001b[0K\u001b[1G162.8 MiB [] 0% 20.2s\u001b[0K\u001b[1G162.8 MiB [] 0% 11.8s\u001b[0K\u001b[1G162.8 MiB [] 0% 8.6s\u001b[0K\u001b[1G162.8 MiB [] 1% 7.6s\u001b[0K\u001b[1G162.8 MiB [] 1% 6.7s\u001b[0K\u001b[1G162.8 MiB [] 1% 6.0s\u001b[0K\u001b[1G162.8 MiB [] 2% 5.6s\u001b[0K\u001b[1G162.8 MiB [] 2% 5.8s\u001b[0K\u001b[1G162.8 MiB [] 3% 5.6s\u001b[0K\u001b[1G162.8 MiB [] 3% 5.9s\u001b[0K\u001b[1G162.8 MiB [] 3% 5.5s\u001b[0K\u001b[1G162.8 MiB [] 4% 5.3s\u001b[0K\u001b[1G162.8 MiB [] 4% 5.2s\u001b[0K\u001b[1G162.8 MiB [] 4% 5.0s\u001b[0K\u001b[1G162.8 MiB [] 5% 5.1s\u001b[0K\u001b[1G162.8 MiB [] 5% 5.0s\u001b[0K\u001b[1G162.8 MiB [] 5% 4.8s\u001b[0K\u001b[1G162.8 MiB [] 6% 4.6s\u001b[0K\u001b[1G162.8 MiB [] 7% 4.6s\u001b[0K\u001b[1G162.8 MiB [] 7% 4.4s\u001b[0K\u001b[1G162.8 MiB [] 8% 4.3s\u001b[0K\u001b[1G162.8 MiB [] 8% 4.2s\u001b[0K\u001b[1G162.8 MiB [] 9% 4.2s\u001b[0K\u001b[1G162.8 MiB [] 9% 4.1s\u001b[0K\u001b[1G162.8 MiB [] 10% 4.0s\u001b[0K\u001b[1G162.8 MiB [] 11% 3.9s\u001b[0K\u001b[1G162.8 MiB [] 11% 3.8s\u001b[0K\u001b[1G162.8 MiB [] 12% 3.7s\u001b[0K\u001b[1G162.8 MiB [] 13% 3.6s\u001b[0K\u001b[1G162.8 MiB [] 14% 3.6s\u001b[0K\u001b[1G162.8 MiB [] 14% 3.7s\u001b[0K\u001b[1G162.8 MiB [] 15% 3.7s\u001b[0K\u001b[1G162.8 MiB [] 15% 3.6s\u001b[0K\u001b[1G162.8 MiB [] 16% 3.6s\u001b[0K\u001b[1G162.8 MiB [] 17% 3.6s\u001b[0K\u001b[1G162.8 MiB [] 17% 3.5s\u001b[0K\u001b[1G162.8 MiB [] 18% 3.4s\u001b[0K\u001b[1G162.8 MiB [] 19% 3.3s\u001b[0K\u001b[1G162.8 MiB [] 20% 3.2s\u001b[0K\u001b[1G162.8 MiB [] 21% 3.1s\u001b[0K\u001b[1G162.8 MiB [] 22% 3.0s\u001b[0K\u001b[1G162.8 MiB [] 23% 2.9s\u001b[0K\u001b[1G162.8 MiB [] 24% 2.8s\u001b[0K\u001b[1G162.8 MiB [] 25% 2.8s\u001b[0K\u001b[1G162.8 MiB [] 26% 2.7s\u001b[0K\u001b[1G162.8 MiB [] 27% 2.7s\u001b[0K\u001b[1G162.8 MiB [] 27% 2.6s\u001b[0K\u001b[1G162.8 MiB [] 28% 2.6s\u001b[0K\u001b[1G162.8 MiB [] 29% 2.5s\u001b[0K\u001b[1G162.8 MiB [] 29% 2.7s\u001b[0K\u001b[1G162.8 MiB [] 29% 3.1s\u001b[0K\u001b[1G162.8 MiB [] 29% 3.3s\u001b[0K\u001b[1G162.8 MiB [] 29% 3.4s\u001b[0K\u001b[1G162.8 MiB [] 29% 3.5s\u001b[0K\u001b[1G162.8 MiB [] 29% 3.8s\u001b[0K\u001b[1G162.8 MiB [] 30% 3.9s\u001b[0K\u001b[1G162.8 MiB [] 30% 4.0s\u001b[0K\u001b[1G162.8 MiB [] 31% 3.9s\u001b[0K\u001b[1G162.8 MiB [] 31% 3.8s\u001b[0K\u001b[1G162.8 MiB [] 32% 3.8s\u001b[0K\u001b[1G162.8 MiB [] 33% 3.7s\u001b[0K\u001b[1G162.8 MiB [] 34% 3.6s\u001b[0K\u001b[1G162.8 MiB [] 34% 3.5s\u001b[0K\u001b[1G162.8 MiB [] 35% 3.4s\u001b[0K\u001b[1G162.8 MiB [] 36% 3.3s\u001b[0K\u001b[1G162.8 MiB [] 37% 3.2s\u001b[0K\u001b[1G162.8 MiB [] 38% 3.1s\u001b[0K\u001b[1G162.8 MiB [] 39% 3.0s\u001b[0K\u001b[1G162.8 MiB [] 40% 3.0s\u001b[0K\u001b[1G162.8 MiB [] 41% 2.9s\u001b[0K\u001b[1G162.8 MiB [] 41% 2.8s\u001b[0K\u001b[1G162.8 MiB [] 42% 2.7s\u001b[0K\u001b[1G162.8 MiB [] 43% 2.7s\u001b[0K\u001b[1G162.8 MiB [] 44% 2.6s\u001b[0K\u001b[1G162.8 MiB [] 45% 2.6s\u001b[0K\u001b[1G162.8 MiB [] 45% 2.5s\u001b[0K\u001b[1G162.8 MiB [] 46% 2.4s\u001b[0K\u001b[1G162.8 MiB [] 47% 2.3s\u001b[0K\u001b[1G162.8 MiB [] 48% 2.3s\u001b[0K\u001b[1G162.8 MiB [] 49% 2.2s\u001b[0K\u001b[1G162.8 MiB [] 50% 2.1s\u001b[0K\u001b[1G162.8 MiB [] 51% 2.1s\u001b[0K\u001b[1G162.8 MiB [] 52% 2.0s\u001b[0K\u001b[1G162.8 MiB [] 53% 2.0s\u001b[0K\u001b[1G162.8 MiB [] 54% 1.9s\u001b[0K\u001b[1G162.8 MiB [] 55% 1.9s\u001b[0K\u001b[1G162.8 MiB [] 55% 1.8s\u001b[0K\u001b[1G162.8 MiB [] 56% 1.8s\u001b[0K\u001b[1G162.8 MiB [] 57% 1.7s\u001b[0K\u001b[1G162.8 MiB [] 58% 1.7s\u001b[0K\u001b[1G162.8 MiB [] 59% 1.6s\u001b[0K\u001b[1G162.8 MiB [] 60% 1.6s\u001b[0K\u001b[1G162.8 MiB [] 61% 1.6s\u001b[0K\u001b[1G162.8 MiB [] 61% 1.5s\u001b[0K\u001b[1G162.8 MiB [] 62% 1.5s\u001b[0K\u001b[1G162.8 MiB [] 63% 1.4s\u001b[0K\u001b[1G162.8 MiB [] 64% 1.4s\u001b[0K\u001b[1G162.8 MiB [] 65% 1.4s\u001b[0K\u001b[1G162.8 MiB [] 66% 1.3s\u001b[0K\u001b[1G162.8 MiB [] 67% 1.3s\u001b[0K\u001b[1G162.8 MiB [] 68% 1.2s\u001b[0K\u001b[1G162.8 MiB [] 69% 1.2s\u001b[0K\u001b[1G162.8 MiB [] 70% 1.1s\u001b[0K\u001b[1G162.8 MiB [] 71% 1.1s\u001b[0K\u001b[1G162.8 MiB [] 72% 1.1s\u001b[0K\u001b[1G162.8 MiB [] 72% 1.0s\u001b[0K\u001b[1G162.8 MiB [] 73% 1.0s\u001b[0K\u001b[1G162.8 MiB [] 74% 1.0s\u001b[0K\u001b[1G162.8 MiB [] 75% 0.9s\u001b[0K\u001b[1G162.8 MiB [] 76% 0.9s\u001b[0K\u001b[1G162.8 MiB [] 77% 0.8s\u001b[0K\u001b[1G162.8 MiB [] 78% 0.8s\u001b[0K\u001b[1G162.8 MiB [] 79% 0.7s\u001b[0K\u001b[1G162.8 MiB [] 80% 0.7s\u001b[0K\u001b[1G162.8 MiB [] 81% 0.7s\u001b[0K\u001b[1G162.8 MiB [] 83% 0.6s\u001b[0K\u001b[1G162.8 MiB [] 84% 0.6s\u001b[0K\u001b[1G162.8 MiB [] 84% 0.5s\u001b[0K\u001b[1G162.8 MiB [] 86% 0.5s\u001b[0K\u001b[1G162.8 MiB [] 87% 0.4s\u001b[0K\u001b[1G162.8 MiB [] 88% 0.4s\u001b[0K\u001b[1G162.8 MiB [] 89% 0.4s\u001b[0K\u001b[1G162.8 MiB [] 90% 0.3s\u001b[0K\u001b[1G162.8 MiB [] 91% 0.3s\u001b[0K\u001b[1G162.8 MiB [] 92% 0.3s\u001b[0K\u001b[1G162.8 MiB [] 93% 0.2s\u001b[0K\u001b[1G162.8 MiB [] 94% 0.2s\u001b[0K\u001b[1G162.8 MiB [] 95% 0.2s\u001b[0K\u001b[1G162.8 MiB [] 96% 0.1s\u001b[0K\u001b[1G162.8 MiB [] 97% 0.1s\u001b[0K\u001b[1G162.8 MiB [] 98% 0.1s\u001b[0K\u001b[1G162.8 MiB [] 99% 0.0s\u001b[0K\u001b[1G162.8 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 128.0.6613.18 (playwright build v1129) downloaded to /root/.cache/ms-playwright/chromium-1129\n",
            "Downloading FFMPEG playwright build v1009\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1009/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.6 MiB [] 1% 3.5s\u001b[0K\u001b[1G2.6 MiB [] 1% 3.4s\u001b[0K\u001b[1G2.6 MiB [] 2% 3.5s\u001b[0K\u001b[1G2.6 MiB [] 3% 3.0s\u001b[0K\u001b[1G2.6 MiB [] 4% 2.7s\u001b[0K\u001b[1G2.6 MiB [] 6% 2.5s\u001b[0K\u001b[1G2.6 MiB [] 6% 3.3s\u001b[0K\u001b[1G2.6 MiB [] 9% 2.5s\u001b[0K\u001b[1G2.6 MiB [] 11% 2.1s\u001b[0K\u001b[1G2.6 MiB [] 13% 1.9s\u001b[0K\u001b[1G2.6 MiB [] 15% 1.7s\u001b[0K\u001b[1G2.6 MiB [] 17% 1.5s\u001b[0K\u001b[1G2.6 MiB [] 20% 1.4s\u001b[0K\u001b[1G2.6 MiB [] 26% 1.0s\u001b[0K\u001b[1G2.6 MiB [] 31% 0.8s\u001b[0K\u001b[1G2.6 MiB [] 35% 0.7s\u001b[0K\u001b[1G2.6 MiB [] 40% 0.6s\u001b[0K\u001b[1G2.6 MiB [] 48% 0.4s\u001b[0K\u001b[1G2.6 MiB [] 59% 0.3s\u001b[0K\u001b[1G2.6 MiB [] 70% 0.2s\u001b[0K\u001b[1G2.6 MiB [] 84% 0.1s\u001b[0K\u001b[1G2.6 MiB [] 98% 0.0s\u001b[0K\u001b[1G2.6 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1009 downloaded to /root/.cache/ms-playwright/ffmpeg-1009\n",
            "[info] Categories found: 10\n",
            " - https://shawayahouse.my.taker.io/menu/category/10?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/11?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/12?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/18?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/20?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/25?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/6?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/7?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/8?language=ar\n",
            " - https://shawayahouse.my.taker.io/menu/category/9?language=ar\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/10?language=ar -> 5 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/11?language=ar -> 3 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/12?language=ar -> 7 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/18?language=ar -> 1 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/20?language=ar -> 1 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/25?language=ar -> 2 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/6?language=ar -> 4 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/7?language=ar -> 4 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/8?language=ar -> 4 items\n",
            "[info] https://shawayahouse.my.taker.io/menu/category/9?language=ar -> 6 items\n",
            "Total parsed rows: 37\n",
            "[{'name': 'كنافة بالقشطة', 'img_url': 'https://images.taker.io/shawaya_house/1689255609_XcV5vl.jpg', 'url': 'https://shawayahouse.my.taker.io/menu/10?language=ar', 'section': 'بيت الشواية - الحلا', 'classification': 'Dessert'}, {'name': 'أم علي', 'img_url': 'https://images.taker.io/shawaya_house/1689256976_5ZSOom.jpg', 'url': 'https://shawayahouse.my.taker.io/menu/35?language=ar', 'section': 'بيت الشواية - الحلا', 'classification': 'Unclassified'}, {'name': 'سلطة فواكة', 'img_url': 'https://images.taker.io/shawaya_house/1689255665_0RUWxp.jpg', 'url': 'https://shawayahouse.my.taker.io/menu/36?language=ar', 'section': 'بيت الشواية - الحلا', 'classification': 'Salad'}, {'name': 'أرز بالحليب بالفرن', 'img_url': 'https://images.taker.io/shawaya_house/1689256946_AoJ1Tr.jpg', 'url': 'https://shawayahouse.my.taker.io/menu/37?language=ar', 'section': 'بيت الشواية - الحلا', 'classification': 'Rice'}, {'name': 'مهلبية', 'img_url': 'https://images.taker.io/shawaya_house/1689256463_pvpocB.jpg', 'url': 'https://shawayahouse.my.taker.io/menu/38?language=ar', 'section': 'بيت الشواية - الحلا', 'classification': 'Dessert'}]\n",
            "Saved 37 items → dishes.csv & dishes.json  |  Images → images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import shutil, os, zipfile\n",
        "from google.colab import files\n",
        "\n",
        "IMAGES_DIR = \"images\"\n",
        "ZIP_NAME = \"images_backup.zip\"\n",
        "\n",
        "# يصنع ملف ZIP من مجلد الصور\n",
        "shutil.make_archive(\"images_backup\", \"zip\", IMAGES_DIR)\n",
        "\n",
        "files.download(ZIP_NAME)\n",
        "if os.path.exists(\"dishes.csv\"): files.download(\"dishes.csv\")\n",
        "if os.path.exists(\"dishes.json\"): files.download(\"dishes.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dBvXx31yrC9b",
        "outputId": "99371c07-a64e-40af-ee18-4b4cdca976b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ff52fa82-10bf-45a7-9eda-9449ff28cf14\", \"images_backup.zip\", 2471969)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cc48b996-b43b-4d7d-8bae-648379ab5d41\", \"dishes.csv\", 3054)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_04367a76-bc0b-4e08-a3ff-9a225029e315\", \"dishes.json\", 6342)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}