{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCSBUCFBWmO7hBfLvfE5aq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoraHK3/DataSciProject/blob/main/Sayyidaty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests   # Library for making HTTP requests\n",
        "from bs4 import BeautifulSoup   # Library for parsing HTML documents\n",
        "import csv    # Library for working with CSV files\n",
        "import json   # Library for working with JSON data\n",
        "from datetime import datetime   # Library for getting the current date and time\n",
        "import time   # Library for handling time-related tasks (like pauses)\n",
        "import re   # Library for regular expressions\n",
        "\n",
        "class SaudiDishScraper:\n",
        "\n",
        "    # Define the scraper class\n",
        "    def __init__(self):\n",
        "        # The constructor initializes the scraper's properties\n",
        "        self.base_url = \"https://kitchen.sayidaty.net\"    # Base URL for the website\n",
        "        # The URL for the Saudi cuisine category, using an f-string for formatting\n",
        "        self.saudi_cuisine_url = f\"{self.base_url}/recipes/index/cuisine/2419\"\n",
        "        self.dishes_data = []\n",
        "\n",
        "    def normalize_ar(self, s: str) -> str:\n",
        "      # A method to normalize Arabic text\n",
        "        if not s:\n",
        "            return ''   # Return an empty string if the input is empty\n",
        "        # Remove Arabic diacritical marks (tashkeel) using a regular expression\n",
        "        s = re.sub(r'[\\u064B-\\u0652\\u0670]', '', s)\n",
        "        # Replace the elongation character '_' with an empty string\n",
        "        s = s.replace('ـ', '')\n",
        "        # Replace all forms of 'alif' (آ، أ، إ) with a standard 'ا'\n",
        "        s = re.sub('[إأآا]', 'ا', s)\n",
        "         # Replace the final alif (ى) with 'ya' (ي)\n",
        "        s = s.replace('ى', 'ي')\n",
        "        # Standardize hamza on 'waw' (ؤ) and 'ya' (ئ)\n",
        "        s = s.replace('ؤ', 'و').replace('ئ', 'ي')\n",
        "        # Replace 'ta marbuta' (ة) with 'ha' (ه)\n",
        "        s = s.replace('ة', 'ه')\n",
        "        return s.strip()    # Return the stripped (no leading/trailing whitespace) string\n",
        "\n",
        "    def tokenize(self, s: str):\n",
        "        # A method to split a string into a list of words\n",
        "        # Replace any non-alphanumeric or non-Arabic character with a space\n",
        "        s = re.sub(r'[^\\w\\u0600-\\u06FF]+', ' ', s)\n",
        "        # Use a list comprehension to split the string into a list of words, filtering out any empty strings\n",
        "        parts = [p for p in s.split() if p]\n",
        "        return parts    # Return the list of words\n",
        "\n",
        "    def get_page_content(self, url):\n",
        "        # A method to fetch the HTML content of a given URL\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }   # Define headers to mimic a web browser\n",
        "        try:\n",
        "            # Send an HTTP GET request to the URL with a 10-second timeout\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            # Raise an HTTPError if the respone status code is an error\n",
        "            response.raise_for_status()\n",
        "            return response.text    # Return the HTML content as a string\n",
        "        except requests.RequestException as e:\n",
        "            # Catch any request-related errors\n",
        "            print(f\"Error fetching {url}: {e}\")   # Print error message\n",
        "            return None   # Return None to indicate failure\n",
        "\n",
        "    def extract_dish_links(self, html_content):\n",
        "        # A method to extract all dish links from an HTML page\n",
        "        soup = BeautifulSoup(html_content, 'html.parser') # Parse the HTML content\n",
        "        dish_links = []   # Initialize an empty list for the links\n",
        "        # Find all 'a' tags with an 'href' attribute attribute containing '/node/'\n",
        "        links = soup.select('a[href*=\"/node/\"]')\n",
        "        for link in links:\n",
        "            href = link.get('href')   # Get the value of the 'href' attribute\n",
        "            if href and '/node/' in href:   # Check if the href exists and contains '/node/'\n",
        "                # Create the full URL by combining the base URL and the href\n",
        "                full_url = href if href.startswith('http') else f\"{self.base_url}{href}\"\n",
        "                dish_links.append(full_url)   # Add the full URL to the list\n",
        "        # Return a list of unique links by converting to a set and back to a list\n",
        "        return list(set(dish_links))\n",
        "\n",
        "    def clean_ingredient_text(self, text):\n",
        "        # A method to clean raw ingreadient text\n",
        "        if not text:\n",
        "            return ''   # Return an empty string if the input is empty\n",
        "        if ':' in text:\n",
        "            # If a colon exists, split the string and take the part before it\n",
        "            text = text.split(':')[0]\n",
        "        # Remove any text within paranthesis using a regular expression\n",
        "        text = re.sub(r'\\(.*?\\)', '', text)\n",
        "        # Normalize the text using the normalize_ar method and strip whitespace\n",
        "        return self.normalize_ar(text).strip()\n",
        "\n",
        "    def extract_dish_details(self, dish_url):\n",
        "        # A method to scrape details from a single dish page\n",
        "        html_content = self.get_page_content(dish_url)  # Fetch the page content\n",
        "        if not html_content:\n",
        "            return    # Return None if the page content could not be fetched\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')   # Parse the HTML\n",
        "        # Find the dish name from the most likely heading tags (h1, h2) or the title tag\n",
        "        title_element = soup.find('h1') or soup.find('h2') or soup.find('title')\n",
        "        # Extract the text of the title element, or set to \"Unknown Dish\" if not found\n",
        "        dish_name = title_element.get_text().strip() if title_element else \"Unknown Dish\"\n",
        "        image_url = None    # Initialize the image URL to None\n",
        "        # Try to find the image URL from the 'og:image' meta tag\n",
        "        meta_image = soup.find('meta', property='og:image')\n",
        "        if meta_image:\n",
        "            image_url = meta_image.get('content', '')   # Get the URL from the content attribute\n",
        "        else:\n",
        "            # If not found, look for a general image tag with a common image extension\n",
        "            img_element = soup.find('img', {'src': re.compile(r'\\.(jpg|jpeg|png|webp)', re.I)})\n",
        "            if img_element:\n",
        "                image_url = img_element.get('src', '')\n",
        "                if image_url and not image_url.startswith('http'):\n",
        "                    image_url = f\"{self.base_url}{image_url}\"  # Make the URL absolute if it's relative\n",
        "        ingredients = []    # Initialize an empty list for ingredients\n",
        "        # Find the div with the class 'ingredients-area'\n",
        "        ingredients_area = soup.find('div', class_='ingredients-area')\n",
        "        if ingredients_area:\n",
        "            # Iterate through the strings within the div, including text separated by <br>\n",
        "            for line in ingredients_area.stripped_strings:\n",
        "                cleaned_line = self.clean_ingredient_text(line) # Clean the ingredient line\n",
        "                if cleaned_line:\n",
        "                    ingredients.append(cleaned_line)  # Add the cleaned ingredient to the list\n",
        "        # Remove duplicate ingredients while preserving order\n",
        "        ingredients = list(dict.fromkeys(ingredients))\n",
        "        # Get the current date and time for the scrape_date\n",
        "        scrape_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        return {\n",
        "            'dish_name': dish_name,\n",
        "            'ingredients': ingredients,\n",
        "            'image_url': image_url,\n",
        "            'dish_url': dish_url,\n",
        "            'scrape_date': scrape_date\n",
        "        }   # Return a dictionary of the extracted data\n",
        "\n",
        "    def scrape_saudi_dishes(self, max_pages=5):\n",
        "        # The main method to start the scraping process\n",
        "        print(\"Starting to scrape Saudi dishes...\")\n",
        "        all_dish_links = []   # A list to store all collected dish links\n",
        "        # Loop through a specified number of pages\n",
        "        for page in range(1, max_pages + 1):\n",
        "            # Construct the URL for the current page\n",
        "            page_url = f\"{self.saudi_cuisine_url}?page={page}\" if page > 1 else self.saudi_cuisine_url\n",
        "            print(f\"Scraping page {page}: {page_url}\")\n",
        "            html_content = self.get_page_content(page_url)  # Fetch the HTML for the page\n",
        "            if not html_content:\n",
        "                print(f\"Failed to retrieve page {page}\")\n",
        "                continue    # Skip to the next page if fetching fails\n",
        "            dish_links = self.extract_dish_links(html_content)  # Extract dish links\n",
        "            all_dish_links.extend(dish_links)   # Add the new links to the list\n",
        "            print(f\"Found {len(dish_links)} dishes on page {page}\")\n",
        "            time.sleep(1)   # Pause for 1 second\n",
        "        # Remove any duplicate links found across pages\n",
        "        all_dish_links = list(set(all_dish_links))\n",
        "        print(f\"Total unique dishes found: {len(all_dish_links)}\")\n",
        "        # Loop through each unique dish URL\n",
        "        for i, dish_url in enumerate(all_dish_links, 1):\n",
        "            print(f\"Scraping dish {i}/{len(all_dish_links)}: {dish_url}\")\n",
        "            dish_data = self.extract_dish_details(dish_url) # Scrape the dish details\n",
        "            if dish_data and dish_data['ingredients']:  # Only add data if ingredients were found\n",
        "                self.dishes_data.append(dish_data)  # Add the data dictionary to the list\n",
        "            time.sleep(1)   # Pause for 1 second\n",
        "        print(f\"Successfully scraped {len(self.dishes_data)} dishes with ingredients\")\n",
        "        return self.dishes_data   # Return the list of scraped dishes\n",
        "\n",
        "    def save_to_csv(self, filename=\"saudi_dishes.csv\"):\n",
        "        # A method to save the scraped data to a CSV file\n",
        "        if not self.dishes_data:\n",
        "            print(\"No data to save\")\n",
        "            return\n",
        "        # Open the file in write mode with UTF-8 encoding\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            # Define the column names for the CSV file\n",
        "            fieldnames = ['dish_name', 'ingredients', 'image_url', 'dish_url', 'scrape_date']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()  # Write the header row\n",
        "            for dish in self.dishes_data:\n",
        "                dish_copy = dish.copy()   # Create a copy to avoid modifying the original data\n",
        "                # Join the list of ingredients into a single string separated by '|'\n",
        "                dish_copy['ingredients'] = '|'.join(dish['ingredients'])\n",
        "                writer.writerow(dish_copy)    # Write the row to the CSV file\n",
        "        print(f\"Data saved to {filename}\")\n",
        "\n",
        "    def save_to_json(self, filename=\"saudi_dishes.json\"):\n",
        "        # A method to save the scraped data to a JSON file\n",
        "        if not self.dishes_data:\n",
        "            print(\"No data to save\")\n",
        "            return\n",
        "        # Open the file in write mode with UTF-8 encoding\n",
        "        with open(filename, 'w', encoding='utf-8') as jsonfile:\n",
        "            # Dump the data list into the JSON file, ensuring non-ASCII characters are preserved\n",
        "            json.dump(self.dishes_data, jsonfile, ensure_ascii=False, indent=2)\n",
        "        print(f\"Data saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This block of code runs only when the script is executed directly\n",
        "    scraper = SaudiDishScraper()  # Create an instance of the scraper class\n",
        "    # Call the main scraping method to start the process\n",
        "    dishes = scraper.scrape_saudi_dishes(max_pages=3)\n",
        "    if dishes:\n",
        "        scraper.save_to_csv()  # Save the data to a CSV file\n",
        "        scraper.save_to_json()  # Save the data to a JSON file\n",
        "        print(\"\\nSample of scraped data:\")\n",
        "        # Print a sample of the first 3 dishes for verification\n",
        "        for i, dish in enumerate(dishes[:3]):\n",
        "            print(f\"\\nDish {i+1}:\")\n",
        "            print(f\"Name: {dish['dish_name']}\")\n",
        "            print(f\"Ingredients: {dish['ingredients']}\")\n",
        "            print(f\"Image URL: {dish['image_url']}\")\n",
        "            print(f\"Scraped on: {dish['scrape_date']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMcpUHujvb3c",
        "outputId": "51105c02-97ac-45c5-b62d-5774d3205245"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to scrape Saudi dishes...\n",
            "Scraping page 1: https://kitchen.sayidaty.net/recipes/index/cuisine/2419\n",
            "Found 18 dishes on page 1\n",
            "Scraping page 2: https://kitchen.sayidaty.net/recipes/index/cuisine/2419?page=2\n",
            "Found 19 dishes on page 2\n",
            "Scraping page 3: https://kitchen.sayidaty.net/recipes/index/cuisine/2419?page=3\n",
            "Found 19 dishes on page 3\n",
            "Total unique dishes found: 55\n",
            "Scraping dish 1/55: https://kitchen.sayidaty.net/node/33617/السمبوسة-السعودية-بالجبنة/المقبلات/وصفات-رمضانية\n",
            "Scraping dish 2/55: https://kitchen.sayidaty.net/node/33512/اللقيمات-السعودية-التقليدية/حلويات/وصفات-رمضانية\n",
            "Scraping dish 3/55: https://kitchen.sayidaty.net/node/34837/المندي-السعودي-في-المنزل/وصفات-طبخ/وصفات\n",
            "Scraping dish 4/55: https://kitchen.sayidaty.net/node/33629/السليق-السعودي-بالدجاج-والسمن/وصفات-طبخ/وصفات-رمضانية\n",
            "Scraping dish 5/55: https://kitchen.sayidaty.net/node/36039/القشد-التقليدي-لضيافة-اليوم-الوطني-السعودي/حلويات/وصفات\n",
            "Scraping dish 6/55: https://kitchen.sayidaty.net/node/36053/خبز-الخمير/وصفات-الخبز/وصفات\n",
            "Scraping dish 7/55: https://kitchen.sayidaty.net/node/33639/الحنيذ-السعودي-باللحم/وصفات-طبخ/وصفات-رمضانية\n",
            "Scraping dish 8/55: https://kitchen.sayidaty.net/node/33630/دجاج-الزربيان-مع-الأرز-المتبل/وصفات-طبخ/وصفات-رمضانية\n",
            "Scraping dish 9/55: https://kitchen.sayidaty.net/node/34671/كعك-العيد-السعودي/حلويات/وصفات\n",
            "Scraping dish 10/55: https://kitchen.sayidaty.net/node/34065/الكبسة-السعودية-بالدجاج-مع-البهارات-الأصلية/وصفات-طبخ/وصفات\n",
            "Scraping dish 11/55: https://kitchen.sayidaty.net/node/34368/فول-على-الطريقة-الحجازية/وصفات-فطور/وصفات\n",
            "Error fetching https://kitchen.sayidaty.net/node/34368/فول-على-الطريقة-الحجازية/وصفات-فطور/وصفات: HTTPSConnectionPool(host='kitchen.sayidaty.net', port=443): Read timed out. (read timeout=10)\n",
            "Scraping dish 12/55: https://kitchen.sayidaty.net/node/34734/طريقة-عمل-الكليجة-السعودية/حلويات/وصفات\n",
            "Scraping dish 13/55: https://kitchen.sayidaty.net/node/35528/لقيمات-محشية-بالقشطة/حلويات/وصفات\n",
            "Scraping dish 14/55: https://kitchen.sayidaty.net/node/35065/كبسة-حاشي/وصفات-طبخ/وصفات\n",
            "Scraping dish 15/55: https://kitchen.sayidaty.net/node/34178/اللقيمات-الذهبية-الهشة-بدون-بيض/حلويات/وصفات\n",
            "Scraping dish 16/55: https://kitchen.sayidaty.net/node/33901/كبسة-اللحم-بالأرز-البسمتي/وصفات-طبخ/وصفات\n",
            "Scraping dish 17/55: https://kitchen.sayidaty.net/node/36040/المفطح-السعودي-باللحم/وصفات-طبخ/وصفات\n",
            "Scraping dish 18/55: https://kitchen.sayidaty.net/node/35168/كباب-ميرو-السعودي/وصفات-طبخ/وصفات\n",
            "Scraping dish 19/55: https://kitchen.sayidaty.net/node/36092/الكليجة-السعودية/حلويات/وصفات\n",
            "Scraping dish 20/55: https://kitchen.sayidaty.net/node/36037/الجريش-بالدجاج/وصفات-طبخ/وصفات\n",
            "Scraping dish 21/55: https://kitchen.sayidaty.net/node/35597/البسبوسة-الحجازية/حلويات/وصفات\n",
            "Scraping dish 22/55: https://kitchen.sayidaty.net/node/34841/ثريد-اللحم-بالخضار-السعودي/وصفات-طبخ/وصفات\n",
            "Scraping dish 23/55: https://kitchen.sayidaty.net/node/35624/كبسة-الدجاج-بالزعفران/وصفات-طبخ/وصفات\n",
            "Scraping dish 24/55: https://kitchen.sayidaty.net/node/35556/ساندوتش-مقلقل-دجاج/وصفات-طبخ/وصفات\n",
            "Scraping dish 25/55: https://kitchen.sayidaty.net/node/34672/معمول-حجازي-على-أصوله/حلويات/وصفات\n",
            "Scraping dish 26/55: https://kitchen.sayidaty.net/node/35796/طريقة-عمل-كبسة-لحم/وصفات-طبخ/وصفات\n",
            "Scraping dish 27/55: https://kitchen.sayidaty.net/node/33961/قهوة-لوز-بالفيديو/مشروبات-وعصائر/وصفات-الفيديو\n",
            "Scraping dish 28/55: https://kitchen.sayidaty.net/node/35472/خبز-المقشوش-السعودي-بالعسل/وصفات-الخبز/وصفات\n",
            "Scraping dish 29/55: https://kitchen.sayidaty.net/node/34531/طريقة-عمل-الكبسة-السعودية-باللحمة/وصفات-طبخ/وصفات\n",
            "Scraping dish 30/55: https://kitchen.sayidaty.net/node/33880/حلى-السقدانة-الحجازي-الصحي-للرجيم/حلويات/وصفات\n",
            "Scraping dish 31/55: https://kitchen.sayidaty.net/node/33757/خبز-التميس-الأصلي/وصفات-رمضانية/وصفات\n",
            "Scraping dish 32/55: https://kitchen.sayidaty.net/node/34692/المقلقل-السعودي-الوصفة-الأصلية/وصفات-طبخ/وصفات\n",
            "Scraping dish 33/55: https://kitchen.sayidaty.net/node/36074/طريقة-عمل-الكليجا-السعودية-لضيافة-اليوم-الوطني-السعودي/حلويات/وصفات\n",
            "Scraping dish 34/55: https://kitchen.sayidaty.net/node/33987/خبز-التميس-على-الطريقة-الأصلية/وصفات-الخبز/وصفات\n",
            "Scraping dish 35/55: https://kitchen.sayidaty.net/node/33971/طريقة-عمل-عيش-أبو-اللحم/وصفات-طبخ/وصفات\n",
            "Scraping dish 36/55: https://kitchen.sayidaty.net/node/34718/القهوة-السعودية-لضيافة-العيد/مشروبات-وعصائر/وصفات\n",
            "Scraping dish 37/55: https://kitchen.sayidaty.net/node/33537/الكبسة-وأسياخ-الدجاج-المشوية-بتتبيلة-الروب-من-ساديا-بالفيديو/وصفات-طبخ/وصفات-الفيديو/وصفات-رمضانية\n",
            "Scraping dish 38/55: https://kitchen.sayidaty.net/node/33561/الرز-الحساوي-لإفطار-رمضان/وصفات-طبخ/وصفات-رمضانية\n",
            "Scraping dish 39/55: https://kitchen.sayidaty.net/node/34122/طريقة-الكبسة-الحارة-بدون-دجاج/وصفات-طبخ/وصفات\n",
            "Scraping dish 40/55: https://kitchen.sayidaty.net/node/34886/طريقة-عمل-السليق-بالدجاج-بقدر-الضغط/وصفات-طبخ/وصفات\n",
            "Scraping dish 41/55: https://kitchen.sayidaty.net/node/36077/طريقة-عمل-شوربة-اللحم-السعودي/أكلات-اللحوم/وصفات\n",
            "Scraping dish 42/55: https://kitchen.sayidaty.net/node/33969/طريقة-عمل-المطبق-السعودي/معجنات/وصفات\n",
            "Scraping dish 43/55: https://kitchen.sayidaty.net/node/33511/الكبسة-السعودية-بالدجاج-مع-صلصة-الدقوس/وصفات-طبخ/وصفات-رمضانية\n",
            "Scraping dish 44/55: https://kitchen.sayidaty.net/node/35718/الجريش-الأبيض-بالدجاج/وصفات-طبخ/وصفات\n",
            "Scraping dish 45/55: https://kitchen.sayidaty.net/node/34853/كبسة-اللحمة-السعودية-وسلطة-الدقوس/وصفات-طبخ/وصفات\n",
            "Scraping dish 46/55: https://kitchen.sayidaty.net/node/35541/شوربة-البورش-على-الطريقة-الروسية/الشوربة/وصفات\n",
            "Scraping dish 47/55: https://kitchen.sayidaty.net/node/36093/تميس-السمن/وصفات-الخبز/وصفات\n",
            "Scraping dish 48/55: https://kitchen.sayidaty.net/node/33554/الجريش-الأحمر/وصفات-طبخ/وصفات-رمضانية\n",
            "Scraping dish 49/55: https://kitchen.sayidaty.net/node/36051/المقشوش-السعودي/حلويات/وصفات\n",
            "Scraping dish 50/55: https://kitchen.sayidaty.net/node/36064/شاي-الجمر/مشروبات-وعصائر/وصفات\n",
            "Scraping dish 51/55: https://kitchen.sayidaty.net/node/36035/أرز-الكبسة-السعودي-دون-لحم-أو-دجاج/وصفات-طبخ/وصفات\n",
            "Scraping dish 52/55: https://kitchen.sayidaty.net/node/35403/كبسة-الدجاج-على-الطريقة-السعودية/وصفات-طبخ/وصفات\n",
            "Scraping dish 53/55: https://kitchen.sayidaty.net/node/36089/الأرز-الحساوي-باللحم/وصفات-طبخ/وصفات\n",
            "Scraping dish 54/55: https://kitchen.sayidaty.net/node/36041/القهوة-السعودية-الأصلية-لاحتفال-اليوم-الوطني/مشروبات-وعصائر/وصفات\n",
            "Scraping dish 55/55: https://kitchen.sayidaty.net/node/36081/الشكشوكة-السعودية-بالبيض/وصفات-فطور/وصفات\n",
            "Successfully scraped 54 dishes with ingredients\n",
            "Data saved to saudi_dishes.csv\n",
            "Data saved to saudi_dishes.json\n",
            "\n",
            "Sample of scraped data:\n",
            "\n",
            "Dish 1:\n",
            "Name: السمبوسة السعودية بالجبنة\n",
            "Ingredients: ['- لتحضير الحشوه', 'جبن موزاريلا', 'بقدونس', 'النعناع', '- دقيق', '- الزيت النباتي', '- الماء', '- خميره', '- ملح']\n",
            "Image URL: https://kitchen.sayidaty.net/uploads/small/76/76f4ab5d57aa4791daed182edf478168_w750_h500.jpg\n",
            "Scraped on: 2025-09-20 16:31:45\n",
            "\n",
            "Dish 2:\n",
            "Name: اللقيمات السعودية التقليدية\n",
            "Ingredients: ['- القطر', 'سكر', 'الماء', 'عصير الليمون', 'ماء الورد', '- الدقيق الابيض', '- نشاء الذره', '- سكر', '- الخميره الفوريه', '- ملح', '- ماء دافي', '- الزيت', '- مكسرات']\n",
            "Image URL: https://kitchen.sayidaty.net/uploads/small/d5/d5b1703facc4cc6dad79cf5b7185da38_w750_h500.jpeg\n",
            "Scraped on: 2025-09-20 16:31:49\n",
            "\n",
            "Dish 3:\n",
            "Name: المندي السعودي في المنزل\n",
            "Ingredients: ['- الدجاج', '- البصل', '- الثوم', '- الجزر', '- طماطم', '- ملح', '- فلفل اسود', '- هيل', '- ورق غار', '- القرفه', '- القرنفل', '- الكمون', '- ارز بسمتي', '- ثوم بودره', '- الزنجبيل', '- سمن', '- لومي', '- ملون الطعام البرتقالي', '- الزيت النباتي']\n",
            "Image URL: https://kitchen.sayidaty.net/uploads/small/c9/c9b61c11e50cca8a2d39d916404eca8c_w750_h500.jpg\n",
            "Scraped on: 2025-09-20 16:31:52\n"
          ]
        }
      ]
    }
  ]
}