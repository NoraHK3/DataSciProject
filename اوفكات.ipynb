{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMV/CQ8e0EOACA5yyWq/P0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoraHK3/DataSciProject/blob/main/%D8%A7%D9%88%D9%81%D9%83%D8%A7%D8%AA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install Google Chrome + Python deps (Colab) ---\n",
        "!wget -q -O /tmp/chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!apt -qy install /tmp/chrome.deb\n",
        "!pip -q install selenium webdriver-manager pillow pandas beautifulsoup4 lxml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJvz2ZeykPYp",
        "outputId": "08ece667-a54d-4983-b1df-f7c88251fb8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  google-chrome-stable libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 10.9 MB/132 MB of archives.\n",
            "After this operation, 447 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Get:3 /tmp/chrome.deb google-chrome-stable amd64 140.0.7339.185-1 [121 MB]\n",
            "Fetched 10.9 MB in 2s (6,380 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 126435 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "Preparing to unpack /tmp/chrome.deb ...\n",
            "Unpacking google-chrome-stable (140.0.7339.185-1) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up google-chrome-stable (140.0.7339.185-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "def setup_driver():\n",
        "    opts = Options()\n",
        "    opts.add_argument(\"--headless=new\")\n",
        "    opts.add_argument(\"--no-sandbox\")\n",
        "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
        "    opts.add_argument(\"--window-size=1280,2000\")\n",
        "    service = Service(ChromeDriverManager().install())  # matches Chrome version\n",
        "    return webdriver.Chrome(service=service, options=opts)\n"
      ],
      "metadata": {
        "id": "Zar37wRplJIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEajWDp_i6Vq",
        "outputId": "48018cc3-8927-4c88-ffd7-d4d72c0b0095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ~38 candidate dishes (before filtering).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3409023866.py:199: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  today = datetime.utcnow().date().isoformat()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 38 items → ovqat_menu.json & ovqat_menu.csv. Images in ovqat_images/\n"
          ]
        }
      ],
      "source": [
        "# OVQAT (ovqat.tryorder.net) – menu scraper (Colab-ready)\n",
        "# - Renders the JS site with Selenium\n",
        "# - Extracts dish name, downloads image, classifies (rice/salad/chicken/...),\n",
        "#   stores Base64 image in JSON and local filename in CSV, with scrape date.\n",
        "\n",
        "!pip -q install selenium webdriver-manager pillow pandas beautifulsoup4 lxml\n",
        "\n",
        "import base64, io, os, re, time, json, pandas as pd, requests\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "START_URL = \"https://ovqat.tryorder.net/menu\"\n",
        "OUT_JSON = \"ovqat_menu.json\"\n",
        "OUT_CSV  = \"ovqat_menu.csv\"\n",
        "IMG_DIR  = \"ovqat_images\"\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "# --- Classification keywords (Arabic + English) ---\n",
        "# If a dish has rice + salad + chicken (by name/desc), we'll output: \"rice;salad;chicken\"\n",
        "CLASS_MAP = {\n",
        "    \"rice\":    [r\"\\b(rice|biryani|kabsa|bukhari|riz)\\b\", r\"رز\", r\"كبسة\", r\"بخاري\", r\"مندي\", r\"مظبي\"],\n",
        "    \"salad\":   [r\"\\b(salad|fattoush|tabbouleh)\\b\", r\"سلطة\", r\"فتوش\", r\"تبولة\"],\n",
        "    \"chicken\": [r\"\\b(chicken|chkn)\\b\", r\"دجاج\", r\"فراخ\"],\n",
        "    \"beef\":    [r\"\\b(beef|meat|veal)\\b\", r\"لحم\", r\"لحم بقري\"],\n",
        "    \"lamb\":    [r\"\\b(lamb|mutton)\\b\", r\"غنم\", r\"ضأن\", r\"لحم غنم\"],\n",
        "    \"fish\":    [r\"\\b(fish|hamour|hammour|salmon)\\b\", r\"سمك\", r\"هامور\"],\n",
        "    \"shrimp\":  [r\"\\b(shrimp|prawn)\\b\", r\"روبيان\", r\"جمبري\"],\n",
        "    \"bread\":   [r\"\\b(bread|flatbread|khubz|tamees)\\b\", r\"خبز\", r\"تميس\"],\n",
        "    \"soup\":    [r\"\\b(soup|shorba)\\b\", r\"شوربة\"],\n",
        "    \"pasta\":   [r\"\\b(pasta|spaghetti|macaroni)\\b\", r\"مكرونة\"],\n",
        "    \"sauce\":   [r\"\\b(sauce|gravy|dip)\\b\", r\"صوص\", r\"صلصة\"],\n",
        "    \"dessert\": [r\"\\b(dessert|sweet|kunafa|qatayef|luqaimat|basbousa)\\b\", r\"حلى\", r\"حلويات\", r\"كنافة\", r\"لقيمات\", r\"بسبوسة\"],\n",
        "}\n",
        "\n",
        "def classify_food(text):\n",
        "    found = []\n",
        "    low = text.lower()\n",
        "    for label, patterns in CLASS_MAP.items():\n",
        "        for pat in patterns:\n",
        "            if re.search(pat, low):\n",
        "                found.append(label)\n",
        "                break\n",
        "    return sorted(set(found))\n",
        "\n",
        "def setup_driver():\n",
        "    opts = Options()\n",
        "    opts.add_argument(\"--headless=new\")\n",
        "    opts.add_argument(\"--no-sandbox\")\n",
        "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
        "    opts.add_argument(\"--window-size=1280,2000\")\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=opts)\n",
        "    return driver\n",
        "\n",
        "def fully_load_menu(driver, url):\n",
        "    driver.get(url)\n",
        "    # Wait for any menu section or item to appear\n",
        "    # We'll try several possible selectors common in menu apps\n",
        "    possible = [\n",
        "        # buttons/cards\n",
        "        \"button.menu-item\", \".menu-item\", \".card.menu-item\",\n",
        "        # generic product cards\n",
        "        \"[class*='menu'] [class*='item']\", \"[data-testid*='item']\",\n",
        "        # headings inside sections\n",
        "        \"section h2\", \"h2[class*='text-'], h3[class*='text-']\"\n",
        "    ]\n",
        "    WebDriverWait(driver, 20).until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, \",\".join(possible)))\n",
        "    )\n",
        "    # try to scroll to load lazy content\n",
        "    last = 0\n",
        "    for _ in range(15):\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(0.8)\n",
        "        new = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new == last:\n",
        "            break\n",
        "        last = new\n",
        "    html = driver.page_source\n",
        "    return html\n",
        "\n",
        "def parse_items_from_html(html, base_url):\n",
        "    \"\"\"\n",
        "    Heuristic parser:\n",
        "    - Find likely item blocks by locating images + nearby titles.\n",
        "    - Works across many Tailwind/JS menu UIs.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # Strategy:\n",
        "    # 1) Find all images that look like dish images (have alt/title or are inside buttons/cards)\n",
        "    # 2) For each, try to find the closest name element (alt, aria-label, nearby h3/h4, button text)\n",
        "    items = []\n",
        "    candidates = []\n",
        "\n",
        "    # All images\n",
        "    for img in soup.select(\"img\"):\n",
        "        src = img.get(\"src\") or img.get(\"data-src\") or \"\"\n",
        "        if not src:\n",
        "            continue\n",
        "        # Skip logos/icons\n",
        "        if any(x in src.lower() for x in [\"logo\", \"icon\", \"sprite\", \"placeholder\", \"avatar\"]):\n",
        "            continue\n",
        "        # wrap candidates with context\n",
        "        parent = img.find_parent([\"button\",\"a\",\"div\",\"article\",\"section\",\"li\"]) or img.parent\n",
        "        candidates.append((img, parent, src))\n",
        "\n",
        "    def nearest_text(node):\n",
        "        # prefer alt/title/aria-label\n",
        "        alt = (node.get(\"alt\") or node.get(\"title\") or \"\").strip()\n",
        "        if alt:\n",
        "            return alt\n",
        "        # nearby heading\n",
        "        for tag in [\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"p\",\"span\",\"strong\",\"button\"]:\n",
        "            sib = node.find_next(tag)\n",
        "            if sib and sib.get_text(strip=True):\n",
        "                return sib.get_text(strip=True)\n",
        "        # parent text\n",
        "        p = node.parent\n",
        "        if p and p.get_text(strip=True):\n",
        "            return p.get_text(strip=True)\n",
        "        return \"\"\n",
        "\n",
        "    seen = set()\n",
        "    for img, parent, src in candidates:\n",
        "        name = (img.get(\"alt\") or \"\").strip()\n",
        "        if not name:\n",
        "            name = nearest_text(parent or img) or nearest_text(img)\n",
        "        name = re.sub(r\"\\s+\", \" \", name).strip()\n",
        "\n",
        "        # Keep only likely dish-like names (Arabic or English words, not very short)\n",
        "        if not name or len(name) < 2:\n",
        "            continue\n",
        "\n",
        "        # Absolute URL for image\n",
        "        src_abs = urljoin(base_url, src)\n",
        "\n",
        "        key = (name, src_abs)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "\n",
        "        # Try to find a short description near the image to help classification\n",
        "        desc = \"\"\n",
        "        for tag in [\"p\",\"span\",\"div\"]:\n",
        "            n = (parent or img).find_next(tag)\n",
        "            if n and len(n.get_text(strip=True)) > 10:\n",
        "                desc = n.get_text(\" \", strip=True)\n",
        "                break\n",
        "\n",
        "        text_for_cls = f\"{name} {desc}\"\n",
        "        items.append({\n",
        "            \"dish_name\": name,\n",
        "            \"image_url\": src_abs,\n",
        "            \"desc\": desc\n",
        "        })\n",
        "\n",
        "    # Light de-dup based on name if many images repeat the same dish\n",
        "    by_name = {}\n",
        "    for it in items:\n",
        "        nm = it[\"dish_name\"]\n",
        "        if nm not in by_name:\n",
        "            by_name[nm] = it\n",
        "    return list(by_name.values())\n",
        "\n",
        "def fetch_image_to_file(url, fname):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        # Basic image validation + convert to JPEG/PNG if needed\n",
        "        img = Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
        "        img.save(fname, format=\"JPEG\", quality=90)\n",
        "        with open(fname, \"rb\") as f:\n",
        "            b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "        return b64, True, None\n",
        "    except Exception as e:\n",
        "        return \"\", False, str(e)\n",
        "\n",
        "def main():\n",
        "    driver = setup_driver()\n",
        "    try:\n",
        "        html = fully_load_menu(driver, START_URL)\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    raw_items = parse_items_from_html(html, START_URL)\n",
        "    print(f\"Found ~{len(raw_items)} candidate dishes (before filtering).\")\n",
        "\n",
        "    rows = []\n",
        "    today = datetime.utcnow().date().isoformat()\n",
        "    for idx, it in enumerate(raw_items, 1):\n",
        "        name = it[\"dish_name\"]\n",
        "        img_url = it[\"image_url\"]\n",
        "        desc = it.get(\"desc\",\"\")\n",
        "        cls_tags = classify_food(f\"{name} {desc}\")\n",
        "        # Keep food items only (optional filter: must match at least one class)\n",
        "        if not cls_tags:\n",
        "            # you can skip non-food lines; for now, keep them but with empty classification\n",
        "            pass\n",
        "\n",
        "        # download image\n",
        "        safe_base = re.sub(r\"[^-\\w_\\.]+\", \"_\", name.lower())[:60] or f\"item_{idx}\"\n",
        "        img_path = os.path.join(IMG_DIR, f\"{safe_base}.jpg\")\n",
        "        image_b64, ok, err = fetch_image_to_file(img_url, img_path)\n",
        "        if not ok:\n",
        "            print(f\"[img-fail] {name} -> {err}\")\n",
        "\n",
        "        row = {\n",
        "            \"dish_name\": name,\n",
        "            \"food_classification\": \";\".join(cls_tags),\n",
        "            \"image_file\": img_path,         # path for CSV use\n",
        "            \"image_b64\": image_b64,         # actual image bytes (Base64) for JSON\n",
        "            \"date_scraped\": today,\n",
        "            \"source_url\": START_URL\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "    # Save JSON (with embedded images)\n",
        "    with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(rows, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Save CSV (filepaths are practical here)\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"Saved {len(rows)} items → {OUT_JSON} & {OUT_CSV}. Images in {IMG_DIR}/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}